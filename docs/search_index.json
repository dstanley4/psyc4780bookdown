[["index.html", "Welcome! Course:", " Welcome! David J. Stanley 2023-09-23 Course: These notes are a resource for students in my PSYC 4780 class. The notes are a subset of material that I am creating for a public domain statistics book to be released sometime in the distant future. "],["about-the-author.html", "About the Author", " About the Author David J. Stanley is an Associate Professor of Industrial and Organizational Psychology at the University of Guelph in Canada. He obtained his PhD from Western University in London, Ontario. David has published articles in Advances in Methods and Practices in Psychological Science, Organizational Research Methods, Journal of Applied Psychology, Perspectives in Psychological Science, Journal of Business and Psychology, Journal of Vocational Behaviour, Journal of Personality and Social Psychology, Behavior Research Methods, Industrial and Organizational Psychology, and Emotion among other journals. David also created the apaTables R package. "],["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 A focus on workflow 1.2 R works with plug-ins 1.3 Software Choice 1.4 Run RStudio not R 1.5 RStudio configuration, First run only 1.6 Begin with an RStudio Project 1.7 Exploring the R Studio Interface 1.8 Writing your first script 1.9 Loading your data 1.10 Checking out your data 1.11 Run vs. Source with Echo vs. Source 1.12 Trying Source with Echo 1.13 A few key points about 1.14 Revisiting read_csv() 1.15 That’s it!", " Chapter 1 Introduction to R Welcome! In this guide, we will teach you about statistics using the statistical software R with the interface provided by R Studio. The purpose of this chapter to is provide you with a set of activities that get you up-and-running in R quickly so get a sense of how it works. In later chapters we will revisit these same topics in more detail. 1.1 A focus on workflow An important part of this guide is training you in a workflow that will avoid many problems than can occur when using R. 1.2 R works with plug-ins R is a statistical language with many plug-ins called packages that you will use for analyses. You can think of R as being like your smartphone. To do things with your phone you need an App (R equivalent: a package) from the App Store (R equivalent: CRAN). Apps need to be downloaded (R equivalent: install.packages) before you can use them. To use the app you need Open it (R equivalent: library command). These similarities are illustrated in Table 1.1 below. TABLE 1.1: R packages are similar to smart phone apps (Kim, 2018) Smart Phone Terminology R Terminology App package App Store CRAN Download App from App Store install.packages(“apaTables” , dependencies = TRUE) Open App library(“apaTables”) 1.3 Software Choice The software for this course is free and runs on Mac or PC/Windows. If you do not have a Mac or PC but instead are working with a tablet - contact the instructor during office hours. 1.3.1 Mac On a Mac you need to download and install R and RStudio. The RStudio software is a “front end” for using R. Although you need to download and install both - you will only ever load RStudio. 1.3.1.1 Mac Download R Go to this link to download the latest version of R. On the screen that appears select “Download R for (Mac) OS X”. On the next screen you will see something like the image below. However, the number (4.0.3) will be different you. Download this file. Run the file you just downloaded to install R. Remember you will never run R - you just need it installed so you can use RStudio. Now install RStudio (see below) 1.3.1.2 Mac Download RStudio After installing R you will install RStudio. This is the software you will actually use in this course. Go to this link to download the latest version of RStudio. On the page that appears scroll down until you see the following: Look at the line for Mac users and download the file. Here it is: RStudio-1.3.1093.dmg your numbers will be different - but this is the file you need. Download it. Open the file you downloaded. You should see something like the screenshot below. Drag the RStudio icon to the Applications folder to install it. In the future, when you want to run RStudio you should do so from the Applications folder. You can access the Applications folder using the Menu shown below. This will open the applications folder. In the Applications folder there is now an RStudio icon. Click this icon to run RStudio each time you use it. You can also drag the RStudio icon in the Applications folder to the Dock for easy access if you prefer. 1.3.2 PC 1.3.2.1 PC Download R Go to this link to download the latest version of R. On the screen that appears select “Download R for Windows”. On the next screen you will see something like the image below. Select “base”. On the next screen you will see something like the image below. As before the specific file numbers will differ for you. Select “Download R 4.0.3 for Windows” and download it. Run the file you just downloaded to install R. Remember you will never run R - you just need it installed so you can use RStudio. Now install RStudio 1.3.2.2 PC Download RStudio After installing R you will install RStudio. This is the software you will actually use in this course. Go to this link to download the latest version of RStudio. On the page that appears scroll down until you see the following: Look at the line for “Windows 10/8/7” users and download the file. Here it is: RStudio-1.3.1093.exe your numbers will be different - but this is the file you need. Download it. Open the file you downloaded and run it to install RStudio. In the future you will only use RStudio to access use R. 1.4 Run RStudio not R As noted above, in the future you will run RStudio to use R. That is, you click the icon below to use RStudio moving forward. On macOs make sure it is the icon in the Applications folder. On Windows make sure it is the icon in the Program Files folder. 1.5 RStudio configuration, First run only Prior using RStudio it is important it to configure it properly to avoid problems getting your code to work. Therefore, the very first time you run RStudio you should configure it correctly. As illustrated below, go to the Tools menu and select Global Options… Next, ensure General is selected on the left side. Then ensure that “Restore .RData into workspace at startup” is not checked. I repeat, NOT CHECKED. Also, make sure that “Save workspace to .RData on exit” is set to NEVER. Then click the OK button at the bottom of the window. 1.6 Begin with an RStudio Project It can be hard to keep track of files you use and create when using RStudio. Therefore, you should always use an RStudio Project to keep track of things. This is basically just a Folder on your computer that RStudio associates with a particular task. Later in this introduction to RStudio we will use the file data_okcupid.csv. Right now we will set up a Project to make it easier to use that file. Create a folder on your computer called “psyc6060_project_1”. Be sure you know where you create this folder so you can find it again easily. Download the data_okcupid.csv file. NOTE:You must get this file from your Downloads folder. For many people (on macOS) the file may Open in Numbers - then they are tempted to save it from Numbers. This will not work. Just close Numbers. Then retrieve the file from you Downloads folder. Place the data_okcupid.csv file into the psyc6060_project_1 folder. Load RStudio. Go to the File menu and select “New Project…” You will see the window below. Select “Existing Directory”. You will see the window below. Click the “Browse” button and then locate the folder you created. Once this is done click the “Create Project” button. You’re ready to begin. Follow this process of creating an RStudio project for each class project/activity. 1.7 Exploring the R Studio Interface Once you have opened (or created) a Project folder, you are presented with the R Studio interface. There are a few key elements to the user interface that are illustrated in Figure 1.1 In the lower right of the screen you can see the a panel with several tabs (i.e., Files, Plots, Packages, etc) that I will refer to as the Files pane. You look in this pane to see all the files associated with your project. On the left side of the screen is the Console which is an interactive pane where you type and obtain results in real time. I’ve placed two large grey blocks on the screen with text to more clearly identify the Console and Files panes. Not shown in this figure is the Script panel where we can store our commands for later reuse. FIGURE 1.1: R Studio interface 1.7.1 Console panel When you first start R, the Console panel is on the left side of the screen. Sometimes there are two panels on the left side (one above the other); if so, the Console panel is the lower one (and labeled accordingly). We can use R a bit like a calculator. Try typing the following into the Console window: 8 + 6 + 7 + 5. You can see that R immediately produced the result on a line preceded by two hashtags (##). 8 + 6 + 7 + 5 ## [1] 26 We can also put the result into a variable to store it. Later we can use the print command to see that result. In the example below we add the numbers 3, 0, and 9 and store the result in the variable my_sum. The text “&lt;-” indicate you are putting what is on the right side of the arrow into the variable on the left side of the arrow. You can think of a variable as cup into which you can put different things. In this case, imagine a real-world cup with my_sum written on the outside and inside the cup we have stored the sum of 3, 0, and 9 (i.e., 12). my_sum &lt;- 3 + 0 + 9 We can inspect the contents of the my_sum variable (i.e., my_sum cup) with the print command: print(my_sum) ## [1] 12 Variable are very useful in R. We will use them to store a single number, an entire data set, the results of an analysis, or anything else. 1.7.2 Script Panel Although you can use R with just with the Console panel, it’s a better idea to use scripts via the Script panel - not visible yet. Scripts are just text files with the commands you use stored in them. You can run a script (as you will see below) using the Run or Source buttons located in the top right of the Script panel. Scripts are valuable because if you need to run an analysis a second time you don’t have to type the command in a second time. You can run the script again and again without retyping your commands. More importantly though, the script provides a record of your analyses. A common problem in science is that after an article is published, the authors can’t reproduce the numbers in the paper. You can read more about the important problem in a surprising article in the journal Molecular Brain. In this article an editor reports how a request for the data underlying articles resulted in the wrong data for 40 out of 41 papers. Long story short – keep track of the data and scripts you use for your paper. In a later chapter, it’s generally poor practice to manipulate or modify or analyze your data using any menu driven software because this approach does not provide a record of what you have done. 1.8 Writing your first script 1.8.1 Create the script file Create a script in your R Studio project by using the menu File &gt; New File &gt; R Script. Save the file with an appropriate name using the File menu. The file will be saved in your Project folder. A common, and good, convention for naming is to start all script names with the word “script” and separate words with an underscore. You might save this first script file with the name “script_my_first_one.R”. The advantage of beginning all script files with the word script is that when you look at your list of files alphabetically, all the script files will cluster together. Likewise, it’s a good idea to save all data files such that they begin with “data_”. This way all the data files will cluster together in your directory view as well. You can see there is already a data file with this convention called “data_okcupid.csv”. You can see as discussed previously, we are trying to instill an effective workflow as you learn R. Using a good naming convention (that is consistent with what others use) is part of the workflow. When you write your scripts it’s a good idea to follow the tidyverse style guide for script names, variable name, file names, and more. 1.8.2 Add a comment to your script In the previous section you created your first script. We begin by adding a comment to the script. A comment is something that will be read by humans rather than the computer/R. You make comments for other people that will read your code and need to understand what you have done. However, realize that you are also making comments for your future self as illustrated in an XKCD cartoon. A good way to start every script is with a comment that includes the date of your script (or even better when you installed your packages, more on this later). Like smartphone apps, packages are updated regularly. Sometimes after a package is updated it will no longer work with an older script. Fortunately, the checkpoint package lets users role back the clock and use older versions of packages. Adding a comment with the date of your script will help future users (including you) to use your script with the same version of the package used when you wrote the script. Dating your script is an important part of an effective and reproducible workflow. # Code written on: YYYY/MM/DD # By: John Smith Moving forward, I suggest you use comments to make your own personal notes in your own code as your write it. Note that in the above comment I used the internationally accepted date format order Year/Month/Day created by the International Organization for Standardization (ISO). Some people use the mnemonic You’re My Dream to remember the Year Month Day order. Wikipedia provides more information about this International Date Format (ISO 8601). An XKCD cartoon highlights the ISO date format: 1.8.3 Background about the tidyverse There are generally two broad ways of using R, the older way and the newer way. Using R the older way is referred to as using base R. A more modern approach to using R is the tidyverse. The tidyverse represents a collection of packages the work together to give R a modern workflow. These packages do many things to help the data analyst (loading data, rearranging data, graphing, etc.). We will use the tidyverse approach to R in this guide. A noted the tidyverse is a collection of packages. Each package adds new commands to R. The number of packages and correspondingly the number of new commands added to R by the tidyverse is large. Below is a list of the tidyverse packages: ## [1] &quot;broom&quot; &quot;conflicted&quot; &quot;cli&quot; ## [4] &quot;dbplyr&quot; &quot;dplyr&quot; &quot;dtplyr&quot; ## [7] &quot;forcats&quot; &quot;ggplot2&quot; &quot;googledrive&quot; ## [10] &quot;googlesheets4&quot; &quot;haven&quot; &quot;hms&quot; ## [13] &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; ## [16] &quot;magrittr&quot; &quot;modelr&quot; &quot;pillar&quot; ## [19] &quot;purrr&quot; &quot;ragg&quot; &quot;readr&quot; ## [22] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; ## [25] &quot;rstudioapi&quot; &quot;rvest&quot; &quot;stringr&quot; ## [28] &quot;tibble&quot; &quot;tidyr&quot; &quot;xml2&quot; ## [31] &quot;tidyverse&quot; Before you can use a package it needs to be installed – this is the same as downloading an app from the App Store. Normally, you can install a single packages with the install.packages command. Previously, you needed run an install.package command for every package in the tidyverse as illustrated below (though we no longer use this approach). # The old way of installing the tidyverse packages # Like downloading apps from the app store install.packages(&quot;broom&quot;, dep = TRUE) install.packages(&quot;cli&quot;, dep = TRUE) install.packages(&quot;ggplot&quot;, dep = TRUE) # etc Fortunately, the tidyverse packages can now by installed with a single install.packages command. Specifically, the install.packages command below will install all of the packages listed above. install.packages(&quot;tidyverse&quot;, dep = TRUE) 1.8.3.1 Warnings regarding install.packages() Prior to running install.packages() ALWAYS use Session &gt; Restart R. Failure to do so could corrupt the R installation on your computer. The install.packages() command should NEVER be put in a script. ALWAYS run install.packages() from the Console not your script. 1.8.4 Add library(tidyverse) to your script The tidyverse is now installed, so we need to activate it. We do that with the library command. Put the library line below at the top of your script file (below your comment): # Code written on: YYYY/MM/DD # By: John Smith library(tidyverse) 1.8.5 Activate tidyverse auto-complete for your script Select the library(tidyverse) text with your mouse/track-pad so that it is highlighted. Then click the Run button in the upper right of the Script panel. Doing this “runs” the selected text. After you click the Run button you should see text like the following the Console panel: ## ── Attaching core tidyverse packages ────────────────── ## ✔ dplyr 1.1.3 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors When you use library(tidyverse) to activate the tidyverse you activate the most commonly used subset of the tidyverse packages. In the output you see checkmarks beside names of the tidyverse packages you have activated. By activating these packages you have added new commands to R that you will use. Sometimes these packages replace older versions of commands in R. The “Conflicts” section in the output shows you where the packages you activated replaced older R commands with newer R commands. You can activate the other tidyverse package by running a library command for each package – if needed. No need to do so now. Most importantly, running the library(tidyverse) prior to entering the rest of your script allows R Studio to present auto-complete options when typing your text. Remember to start each script with the library(tidyverse) command and then Run it so you get the autocomplete options for the rest of the commands your enter. 1.9 Loading your data 1.9.1 Use read_csv (not read.csv) to open files. If you inspect the Files pane on the right of the screen you see the data_okcupid.csv data file in our project directory. We will load this data with the commands below. If you followed the steps above, you should have auto-complete for the tidyverse commands you type for now in – in the current R session. Enter the command below into your script. As your start to type read_csv you will likely be presented with an auto-complete option. You can use the arrow keys to move up and down the list of options to select the one you want - then press tab to select it. Once your command looks like the one below select the text and click on the “Run” button. okcupid_profiles &lt;- read_csv(file = &quot;data_okcupid.csv&quot;) ## Rows: 59946 Columns: 6 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): diet, pets, sex, status ## dbl (2): age, height ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The output indicates that you have loaded a data file and the type of data in each column. The sex column is of type col_character which indicates it contains text/letters. Most of the columns are of the type character. The age and height columns contain numbers are correspondingly indicated to be the type col_double. The label col_double indicates that a column of numbers represented in R with high precision. There are other ways of representing numbers in R but this is the type we will see/use most often. 1.10 Checking out your data There many ways of viewing the actual data you loaded. A few of these are illustrated now. 1.10.1 view(): See a spreadsheet view of your data You can inspect your data in a spreadsheet view by using the view command. Do NOT add this command to your script file – EVER. Adding it to the script can cause substantial problems. Type this command in the Console. view(okcupid_profiles) 1.10.2 print(): See you data in the Console You can inspect the first few rows of your data with the print() command. It is OK to add a print command to your script. Try the print() command below in the Console: print(okcupid_profiles) ## # A tibble: 59,946 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs a… m single ## 2 35 mostly other 70 likes dogs a… m single ## 3 38 anything 68 has cats m avail… ## 4 23 vegetarian 71 likes cats m single ## 5 29 &lt;NA&gt; 66 likes dogs a… m single ## 6 29 mostly anything 67 likes cats m single ## 7 32 strictly anything 65 likes dogs a… f single ## 8 31 mostly anything 65 likes dogs a… f single ## 9 24 strictly anything 67 likes dogs a… f single ## 10 37 mostly anything 65 likes dogs a… m single ## # ℹ 59,936 more rows 1.10.3 head(): Check out the first few rows of data You can inspect the first few rows of your data with the head() command. Try the command below in the Console: head(okcupid_profiles) ## # A tibble: 6 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs an… m single ## 2 35 mostly other 70 likes dogs an… m single ## 3 38 anything 68 has cats m avail… ## 4 23 vegetarian 71 likes cats m single ## 5 29 &lt;NA&gt; 66 likes dogs an… m single ## 6 29 mostly anything 67 likes cats m single You can be even more specific and indicate you only want the first three row of your data with the head() command. Try the command below in the Console: head(okcupid_profiles, 3) ## # A tibble: 3 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs an… m single ## 2 35 mostly other 70 likes dogs an… m single ## 3 38 anything 68 has cats m avail… 1.10.4 tail(): Check out the last few rows of data You can inspect the last few rows of your data with the tail() command. Try the command below in the Console: tail(okcupid_profiles) ## # A tibble: 6 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 &lt;NA&gt; 62 likes dogs f single ## 2 59 &lt;NA&gt; 62 has dogs f single ## 3 24 mostly anything 72 likes dogs and … m single ## 4 42 mostly anything 71 &lt;NA&gt; m single ## 5 27 mostly anything 73 likes dogs and … m single ## 6 39 &lt;NA&gt; 68 likes dogs and … m single You can be even more specific and indicate you only want the last three row of your data with the tail() command. Try the command below in the Console: tail(okcupid_profiles, 3) ## # A tibble: 3 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 42 mostly anything 71 &lt;NA&gt; m single ## 2 27 mostly anything 73 likes dogs and … m single ## 3 39 &lt;NA&gt; 68 likes dogs and … m single 1.10.5 summary(): Quick summaries You can a short summary of your data with the summary() command. Note that we will use the summary() command in many places in the guide. The output of the summary() command changes depending on what you give it - that is put inside the brackets. You can give the summary() command many things such as data, the results of a regression analysis, etc. Try the command below in the Console. You will see that summary() give the mean and median for each of the numeric variables (age and height). summary(okcupid_profiles) ## age diet height ## Min. : 18.0 Length:59946 Min. : 1.0 ## 1st Qu.: 26.0 Class :character 1st Qu.:66.0 ## Median : 30.0 Mode :character Median :68.0 ## Mean : 32.3 Mean :68.3 ## 3rd Qu.: 37.0 3rd Qu.:71.0 ## Max. :110.0 Max. :95.0 ## NA&#39;s :3 ## pets sex status ## Length:59946 Length:59946 Length:59946 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## 1.11 Run vs. Source with Echo vs. Source There are different ways of running commands in R. So far you have used two of these. You can enter them into the Console as we have done already. Or you can put them in your script select the text and click the Run button. There are four ways of running commands in your script. You can: Console: Enter commands directly Script: Select the command(s) and press the Run button. Script: Source (Without Echo) Script: Source With Echo Two of these approaches involve using the Source button, see Figure 1.2. You bring up the options for the Source button, illustrated in this figure, by clicking on the small arrow to the right of the word Source. FIGURE 1.2: Source button options 1.11.1 Run select text The Run button will run the text you highlight and present the relevant output. You have used this command a fair amount already. I strongly suggest you ONLY use the Run button when testing a command to make sure it works or to debug a script. Or to run library(tidyverse) as you start working on your script so that you get the autocomplete options. In general, you should always try to execute your R Scripts using the Source with Echo command (preceded by a Restart, see below). This ensures your script will work beginning to end for you in the future and for others that attempt to use it. Using the Run button in an ad lib basis can create output that is not reproducible. 1.11.2 Source (without Echo) Source (without Echo) is not designed for the typical analysis workflow. It is mostly helpful when you run simulations. When you run Source (without Echo) much of the output you would wish to read is suppressed. In general, avoid this option. If you use it, you often won’t see what you want to see in the output. 1.11.3 Source with Echo The Source with Echo command runs all of the contents of a script and presents the output in the R console. This is the approach you should use to running your scripts in most cases. Prior to running Source with Echo (or just Source), it’s always a good idea to restart R. This makes sure you clear the computer memory of any errors from any previous runs. So you should do the following EVERY time you run your script. Use the menu item: Session &gt; Restart R Click the down arrow beside the Source button, and click on Source With Echo This will clear potentially problematic previous stats, run the script commands, and display the output in the Console. Moving forward we will use this approach for running scripts. Once you have used Source with Echo once, you can just click the Source button and it will use Source with Echo automatically (without the need to use the pull down option for selecting Source with Echo). Using Restart R before you run a script, or R code in general, is a critical workflow tip. 1.12 Trying Source with Echo Put the head(), tail(), and summary() command we used previously into your script. Then save your script using using the File &gt; Save menu. You script should appear as below. # Code written on: YYYY/MM/DD # By: John Smith library(tidyverse) okcupid_profiles &lt;- read_csv(file = &quot;data_okcupid.csv&quot;) head(okcupid_profiles) tail(okcupid_profiles) summary(okcupid_profiles) Now do the following: Use the menu item: Session &gt; Restart R Click the down arrow beside the Source button, and click on Source With Echo You should see the output below: # Code written on: YYYY/MM/DD # By: John Smith library(tidyverse) okcupid_profiles &lt;- read_csv(file = &quot;data_okcupid.csv&quot;) ## Rows: 59946 Columns: 6 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): diet, pets, sex, status ## dbl (2): age, height ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(okcupid_profiles) ## # A tibble: 6 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs an… m single ## 2 35 mostly other 70 likes dogs an… m single ## 3 38 anything 68 has cats m avail… ## 4 23 vegetarian 71 likes cats m single ## 5 29 &lt;NA&gt; 66 likes dogs an… m single ## 6 29 mostly anything 67 likes cats m single tail(okcupid_profiles) ## # A tibble: 6 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 &lt;NA&gt; 62 likes dogs f single ## 2 59 &lt;NA&gt; 62 has dogs f single ## 3 24 mostly anything 72 likes dogs and … m single ## 4 42 mostly anything 71 &lt;NA&gt; m single ## 5 27 mostly anything 73 likes dogs and … m single ## 6 39 &lt;NA&gt; 68 likes dogs and … m single summary(okcupid_profiles) ## age diet height ## Min. : 18.0 Length:59946 Min. : 1.0 ## 1st Qu.: 26.0 Class :character 1st Qu.:66.0 ## Median : 30.0 Mode :character Median :68.0 ## Mean : 32.3 Mean :68.3 ## 3rd Qu.: 37.0 3rd Qu.:71.0 ## Max. :110.0 Max. :95.0 ## NA&#39;s :3 ## pets sex status ## Length:59946 Length:59946 Length:59946 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## Congratulations you just ran your first script! 1.13 A few key points about Sometimes you will need to send a command additional information. Moreover, that information often needs to be grouped together into a vector or a list before you can send it to the command. We’ll learn more about doing so in the future but here is a quick over view of vectors and lists to provide a foundation for future chapters. 1.13.0.1 Vector of numbers We can create a vector of only numbers using the “c” function - which you can think of as being short for “combine” (or concatenate). In the commands below we create a vector of a few even numbers called “even_numbers”. even_numbers &lt;- c(2, 4, 6, 8, 10) print(even_numbers) ## [1] 2 4 6 8 10 We can obtain the second number in the vector using the following notation: print(even_numbers[2]) ## [1] 4 1.13.0.2 Vector of characters We can also create vectors using only characters. Note that I use SHIFT RETURN after each comma to move to the next line. favourite_things &lt;- c(&quot;copper kettles&quot;, &quot;woolen mittens&quot;, &quot;brown paper packages&quot;) print(favourite_things) ## [1] &quot;copper kettles&quot; &quot;woolen mittens&quot; ## [3] &quot;brown paper packages&quot; As before, can obtain the second item in the vector using the following notation: print(favourite_things[2]) ## [1] &quot;woolen mittens&quot; 1.13.1 Lists Lists are similar to vectors in that you can create them and access items by their numeric position. Vectors must be all characters or all numbers. Lists can be a mix of characters or numbers. Most importantly items in lists can be accessed by their label. Note that I use SHIFT RETURN after each comma to move to the next line in the code below. my_list &lt;- list(last_name = &quot;Smith&quot;, first_name = &quot;John&quot;, office_number = 1913) print(my_list) ## $last_name ## [1] &quot;Smith&quot; ## ## $first_name ## [1] &quot;John&quot; ## ## $office_number ## [1] 1913 You can access an item in a list using double brackets: print(my_list[2]) ## $first_name ## [1] &quot;John&quot; You can access an item in a list by its label/name using the dollar sign: print(my_list$last_name) ## [1] &quot;Smith&quot; print(my_list$office_number) ## [1] 1913 1.14 Revisiting read_csv() In the text above we learned about vectors of characters. This information can be used handle missing values in the read_csv() command. Consider a data set where for many participants we did not get a response to a question. We might have that represented in various ways within the data file. The value for that question might simply be omitted (i.e., represented by nothing – ““), represented by NA for not available, or maybe simply be something like -777. When we load data with read_csv() we can indicate to the computer how missing values are represented. This ensures the computer sees something like -777 as a missing value rather than an actual value. We do so in the read_csv() command below using the na argument (not available). Note: use SHIFT-RETURN or SHIFT-ENTER to move the next line within the read_csv() command. Failure to indicate the missing value codes within read_csv() will result in incorrect answers in most scenarios with missing data. okcupid_profiles &lt;- read_csv(file = &quot;data_okcupid.csv&quot;, na = c(&quot;&quot;, &quot;NA&quot;, &quot;-777&quot;)) 1.15 That’s it! Congratulations! You’ve reached the end of the introduction to R. Take a break, have a cookie, and read some more about R tomorrow! "],["handling-data-with-the-tidyverse.html", "Chapter 2 Handling Data with the Tidyverse 2.1 Required 2.2 Objective 2.3 Using the Console 2.4 Tidyverse help with the Introverse 2.5 Basic tidyverse commands 2.6 Advanced tidyverse commands 2.7 Using help 2.8 Base R vs tidyverse", " Chapter 2 Handling Data with the Tidyverse A key component of doing statistics in the modern world is managing/wrangling or cleaning data to make it ready for analysis. Indeed, some estimate that those who do data science spend 80% of their time engaged in cleaning data. Although this specific percentage may not be accurate it does accurately reflect the fact that data scientists do spend a substantial portion of their time preparing data for analysis. Many complain about this fact but as Leigh Dodds points out “I would argue that spending time working with data. To transform, explore and understand it better is absolutely what data scientists should be doing. This is the medium they are working in.” In this chapter we focus on teaching you foundational skills for data preparation. The skills taught in this chapter largely revolve around using the tidyverse packages to manage data that will eventually help with data cleaning in the Workflow and Qualtrics chapters. There are some additional resources you may want to consult as you learn these tidyverse skills: R for Data Science book. This is an excellent free online book! But it does reflect an exploratory analysis mindset compared to the confirmatory mindset we typically use in Psychological science. You may find a cheatsheet helpful. Specifically, the Data Transformation cheatsheet. tidyverse website 2.1 Required The data files below are used in this chapter. Right click to save each file. Required Data data_okcupid.csv data_experiment.csv The following CRAN packages must be installed: Required CRAN Packages tidyverse remotes The following GitHub package must be installed: Required GitHub Packages sjspielman/introverse After the remotes package is installed, it can be used to install the introverse package from GitHub (an alternative “App Store”). If you get asked if you would like to install more recent versions of packages on your machine press “1” and Return to indicate “All” (if you are asked, you may not be). Note that if you do encounter installation problems with the introverse below - don’t worry. The introverse is just additional documentation. remotes::install_github(&quot;sjspielman/introverse&quot;) 2.2 Objective The objective of this chapter is to familiarize you with some key commands in the tidyverse. These commands are used in isolation of each other for the most part. In the next chapter we will use these commands in a more coordinated way as we load a data set and move it from raw data to data that is ready for analysis (i.e., analytic data). You can start this project by Starting the class assignment on R Studio Cloud that corresponds to the chapter name. 2.3 Using the Console All of the commands in this chapter should be typed into the Console within R. If you see a command split over multiple lines, use SHIFT-RETURN (macOS) or SHIFT-ENTER (Windows) to move the next line that is part of the same command. 2.4 Tidyverse help with the Introverse The introverse package is just additional documentation that is helpful for beginners - until you learn to read the standard R documentation. The first command we will use in this chapter is the read_csv() command. You get the standard R help for this command using the code below in the R Console: library(tidyverse) # this actives the tidyverse commands ?read_csv This gives you help that looks like this: But you can obtain the better introverse documentation by typing the code below in the R Console: library(introverse) # this actives the intoverse help get_help(&quot;read_csv&quot;) This gives you help that looks like this: Most importantly, when you scroll down on the introverse help pages - it gives you code examples that are easy to follow. If at any point during this course you want more information about how a command works use the introverse get_help() command. The get_help() command is especially valuable because it tells you which package (of the many tidyverse packages) a particular command comes from. For example, you can see the read_csv() command is from the readr package when you inspect the introverse help above. 2.5 Basic tidyverse commands If you inspect the Files tab on the lower-right panel in R Studio you will see the file data_okcupid.csv. The code below loads that file. Recall you should type this code into the R Console. library(tidyverse) okcupid_profiles &lt;- read_csv(&quot;data_okcupid.csv&quot;) You can see the first few rows of the data using the print() command. Each row presents a person whereas each column represents a variable. If you have a large number of columns you will only see the first several columns with this approach to viewing your data. print(okcupid_profiles) ## # A tibble: 59,946 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs a… m single ## 2 35 mostly other 70 likes dogs a… m single ## 3 38 anything 68 has cats m avail… ## 4 23 vegetarian 71 likes cats m single ## 5 29 &lt;NA&gt; 66 likes dogs a… m single ## 6 29 mostly anything 67 likes cats m single ## 7 32 strictly anything 65 likes dogs a… f single ## 8 31 mostly anything 65 likes dogs a… f single ## 9 24 strictly anything 67 likes dogs a… f single ## 10 37 mostly anything 65 likes dogs a… m single ## # ℹ 59,936 more rows But it’s also helpful just to see a list of the columns in the data with the glimpse() command: glimpse(okcupid_profiles) ## Rows: 59,946 ## Columns: 6 ## $ age &lt;dbl&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 37, 35,… ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anyth… ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65, 70,… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs an… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;, … The glimpse() command is useful because it quickly allows you to see all of the columns. Moreover, it allows you to see the type for each column. Types were briefly discussed in the last chapter. Notice in the output beside each column name that some columns are labeled “dbl” which is short for double – a type of numeric column. Other columns are labeled “chr” which is short for character - meaning the columns contain characters. These designations will become important in the next chapter as we prepare data for analysis. 2.5.1 decimals Often the tidyverse doesn’t show the desired number of digits - this means too few digits after each decimal. You can set the desired number of total digits (before/after the decimal) with the options(pillar.sigfig = 3) command. Three is the default - but probably too low for our purposes. We’ll begin with this value of 3 but change it later to 6 in the summarise() section. #default value for digits options(pillar.sigfig = 3) #suggested value you set in your script at the top options(pillar.sigfig = 6) 2.5.2 select() The select() command allows you to obtain a subset of the columns in your data. The commands below can be used to obtain the age and height columns. You can read the command as: take the okcupid_profiles data and then select the age and height columns. The “%&gt;%” symbol can be read as “and then”. You can see that this code prints out the data with just the age and height columns. Remember, use SHIFT-ENTER or SHIFT-RETURN to move to the next line in the block of code. okcupid_profiles %&gt;% select(age, height) ## # A tibble: 59,946 × 2 ## age height ## &lt;dbl&gt; &lt;dbl&gt; ## 1 22 75 ## 2 35 70 ## 3 38 68 ## 4 23 71 ## 5 29 66 ## 6 29 67 ## 7 32 65 ## 8 31 65 ## 9 24 67 ## 10 37 65 ## # ℹ 59,936 more rows Of course, it’s usually of little help to just print the subset of the data. It’s better to store it in a new data. In the command below we store the resulting data in a new data set called new_data. new_data &lt;- okcupid_profiles %&gt;% select(age, height) The glimpse() command shows us that only the age and height columns are in new_data. glimpse(new_data) ## Rows: 59,946 ## Columns: 2 ## $ age &lt;dbl&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 37, 35,… ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65, 70,… In the above example we indicated the columns we wanted to retain from the okcupid_profiles data using the select() command. However, we can also indicate the columns we want to drop from okcupid_profiles using a minus sign (-) in front of the columns we specify in the select() command. new_data &lt;- okcupid_profiles %&gt;% select(-age, -height) The glimpse() command shows us that we kept all the columns except the age and height columns when we created new_data. glimpse(new_data) ## Rows: 59,946 ## Columns: 4 ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anyth… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs an… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;, … 2.5.3 summarise() The summarise() command can be used to generate descriptive statistics for a specified column. You can easily calculate column descriptive statistics using the corresponding commands for mean(), sd(), min(), max(), among others. In the example below we calculate the mean for the age column. In the code below, mean(age, na.rm = TRUE), indicates to R that it should calculate the mean of the age column. The na.rm indicates how missing values should be handled. The na stands for not available; in R missing values are classified as Not Available or NA. The rm stands for remove. Consequently, na.rm is asking: “Should we remove missing values when calculating the mean?” The TRUE indicates that yes, missing values should be removed when calculating the mean. The result of this calculation is placed into a variable labelled age_mean, though we could have used any label we wanted instead of age_mean. We see that the mean of the age column is, with rounding, 32.3. okcupid_profiles %&gt;% summarise(age_mean = mean(age, na.rm = TRUE)) ## # A tibble: 1 × 1 ## age_mean ## &lt;dbl&gt; ## 1 32.3 More than one calculation can occur in the same summarise() command. You can easily add the calculation for the standared deviation with the sd() command. okcupid_profiles %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## # A tibble: 1 × 2 ## age_mean age_sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 32.3 9.45 Often this process does too much rounding. We can get more exact results changing the number of decimals with the options(pillar.sigfig = 6) command before our analysis. See the difference below. Now we see more digits. I encourage you to sue this command at the top of your script. The default is options(pillar.sigfig = 3) I suggest you use options(pillar.sigfig = 6) or higher. #NOTICE: We have increased the number of digits in the output options(pillar.sigfig = 6) okcupid_profiles %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## # A tibble: 1 × 2 ## age_mean age_sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 32.3403 9.45278 2.5.4 filter() The filter() command allows you to obtain a subset of the rows in your data. In the example below we create a new data set with just the males from the original data. Notice the structure of the original data below in the glimpse() output. There is a column called sex that uses m and f to indicate male and female, respectively. Also notice that there are 59946 rows in the okcupid_profiles data. glimpse(okcupid_profiles) ## Rows: 59,946 ## Columns: 6 ## $ age &lt;dbl&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 37, 35,… ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anyth… ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65, 70,… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs an… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;, … We use the filter command to select a subset of the rows based on the content of any column. In this case the sex column is used to obtain a subset of the rows; the rows with the value “m” are obtained. Notice the double equals sign is used to indicate “equal to”. The reason a double equals sign is used here (instead of a single equals sign) is to distinguish it from the use of the single equals sign in the summarise command above. In the summarise command above, the single equal sign was used to indicate “assign to”. That is, assign to age_mean the mean of the column age after it is calculated. A single equals sign indicates “assign to” whereas a double equals sign indicates “is equal to”. okcupid_males &lt;- okcupid_profiles %&gt;% filter(sex == &quot;m&quot;) We use glimpse() to inspect these all male data. Notice that only the letter m is in the sex column - indicating only males are in the data set. Also notice that there are 35829 rows in the okcupid_males data - fewer people because males are a subset of the total number of rows. glimpse(okcupid_males) ## Rows: 35,829 ## Columns: 6 ## $ age &lt;dbl&gt; 22, 35, 38, 23, 29, 29, 37, 35, 28, 24, 33,… ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anyth… ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 70, 72, 72, 70,… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs an… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;, … The filter command can be combined with the summarise command to get the descriptive statistics for males without the hassle of creating new data. This is again done using the %&gt;% “and then” operator. okcupid_profiles %&gt;% filter(sex == &quot;m&quot;) %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## # A tibble: 1 × 2 ## age_mean age_sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 32.0186 9.03288 We see that for the 35829 males the mean age is 32.0 and the standard deviation is 9.0. Likewise, we can obtain the descriptive statistics for females with only a slight modification, changing m to f in the filter command: okcupid_profiles %&gt;% filter(sex == &quot;f&quot;) %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## # A tibble: 1 × 2 ## age_mean age_sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 32.8182 10.0254 We see that for the 24117 females the mean age is 32.8 and the standard deviation is 10.0. 2.5.5 group_by() The process we used with the filter command would quickly become onerous if we had many subgroups for a column. Consequently, it’s often better to use the group() command to calculate descriptive statistics for the levels (e.g., male/female) of a variable. By telling the computer to group_by() sex the summarise command is run separately for every level of sex (i.e., m and f). okcupid_profiles %&gt;% group_by(sex) %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## # A tibble: 2 × 3 ## sex age_mean age_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 f 32.8182 10.0254 ## 2 m 32.0186 9.03288 Fortunately, it’s possible to use more than one grouping variable with the group_by() command. In the code below we group by sex and status (i.e., dating status). okcupid_profiles %&gt;% group_by(sex, status) %&gt;% summarise(age_mean = mean(age, na.rm = TRUE), age_sd = sd(age, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;sex&#39;. You can ## override using the `.groups` argument. ## # A tibble: 10 × 4 ## # Groups: sex [2] ## sex status age_mean age_sd ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 f available 32.2378 8.53559 ## 2 f married 33.7111 8.13493 ## 3 f seeing someone 28.0748 6.44038 ## 4 f single 33.0440 10.1549 ## 5 f unknown 27.75 5.90903 ## 6 m available 34.7543 9.40181 ## 7 m married 38.68 10.0785 ## 8 m seeing someone 30.8115 7.06094 ## 9 m single 31.9214 9.03813 ## 10 m unknown 40.6667 8.86942 The resulting output provide for age the mean and standard deviation for every combination of sex and dating status. The first five rows provide output for females at every level of dating status whereas the subsequent five rows provide output for males at every level of dating status. 2.5.6 mutate() The mutate() command can be used to calculate a new column in a data. In the example below we calculate a new column called age_centered which is the new version of the age_column where the mean of the column has been removed from every value. This is merely an example of the many different types of calculations we can perform to create a new column using mutate(). okcupid_profiles &lt;- okcupid_profiles %&gt;% mutate(age_centered = age - mean(age, na.rm = TRUE)) Notice that the glimpse() command reveals that after we use the mutate() command there is a new column called age_centered. glimpse(okcupid_profiles) ## Rows: 59,946 ## Columns: 7 ## $ age &lt;dbl&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 3… ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, … ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 6… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes d… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;sin… ## $ age_centered &lt;dbl&gt; -10.3403, 2.6597, 5.6597, -9.3403, -3… 2.6 Advanced tidyverse commands In this advanced selection we revisit the commands from the basic tidyverse section but use more complicated code to either select or apply an action to more than one column at a time. We will indicate the columns that we want to select or apply an action to using: starts_with(), ends_with(), contains(), matches(), or where(). The first four of these are used to indicate columns based on column names. In contrast, the last command, where(), is used to indicate the columns based on the column type (numeric, character, factor, etc.). We will review all five commands for indicating the columns we want in the select() selection below. Following that we will, for brevity, typically use only one of the five commands when illustrating how they work with summarise() and mutate(). We begin by loading a new data. library(tidyverse) data_exp &lt;- read_csv(&quot;data_experiment.csv&quot;) ## Rows: 6 Columns: 6 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sex ## dbl (5): id, t1_vomit, t1_aggression, t2_vomit, t2_aggre... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The glimpse() command reveals that this is a small data set where every row represents one rat. The sex of the rat is recorded as well as, for each of two time points, a rating of vomiting and aggression. glimpse(data_exp) ## Rows: 6 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, … ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 2.6.1 select() 2.6.1.1 select() using column name 2.6.1.1.1 starts_with() starts_with() allows us to select columns based on how the column name begins. Here we put the columns that begin with “t1” into a new data called data_time1. data_time1 &lt;- data_exp %&gt;% select(starts_with(&quot;t1&quot;)) The glimpse command shows us the new data only contains the columns that begin with “t1” glimpse(data_time1) ## Rows: 6 ## Columns: 2 ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 2.6.1.1.2 ends_with() ends_with() allows us to select columns based on how the column name ends. Here we put the columns that end with “aggression” into a new data set called data_aggression. data_aggression &lt;- data_exp %&gt;% select(ends_with(&quot;aggression&quot;)) glimpse(data_aggression) ## Rows: 6 ## Columns: 2 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 2.6.1.1.3 contains() contains() allows us to select columns based on the contents of the column name. Here we put the columns that have “_” in the name into a new data set called new_data. new_data &lt;- data_exp %&gt;% select(contains(&quot;_&quot;)) glimpse(new_data) ## Rows: 6 ## Columns: 4 ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 2.6.1.1.4 matches() It’s also possible to use regex (i.e., a regular expression) to select columns. Regex is a powerful way to specify search/matching requirements for text - in this case the text of column names. An explanation of regex is beyond the scope of this chapter. Nonetheless the example below uses regex to select any column with an underscore in the column name followed by any character. The result is the same as the above for the contains() command. However, the matches() command is more flexible than the contains() command and can take into account substantially more complicated situations. data_matched &lt;- data_exp %&gt;% select(matches(&quot;(_.)&quot;)) You can see the columns selected using regex: glimpse(data_matched) ## Rows: 6 ## Columns: 4 ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 You can learn about regex at RegexOne and test your regex specification at Regex101. Ideally though, as we discuss in the next chapter, you can use naming conventions that are sufficiently thoughtful that you don’t need regex, or only rarely. The reason for this is that regex can be challenging to use. As Twitter user @ThatJenPerson noted “Regex is like tequila: use it to try to solve a problem and now you have two problems.” Nonetheless, at one or two points in the future we will use regex to solve a problem (but not tequila). 2.6.1.2 select() using column type In many cases we will want to select or perform an action on a column based on whether the column is a numeric, character, or factor column (indicated in glimpse output as dbl, chr, and fct, respectively). We will learn more about factors later in this chapter. Each of these column types can be selected by using is.numeric, is.character, or is.factor, respectively, in combination with the where() command. We can select numeric columns using where() and is.numeric: data_numeric_columns &lt;- data_exp %&gt;% select(where(is.numeric)) You can see the new data contains only the numeric columns: glimpse(data_numeric_columns) ## Rows: 6 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 We can select character (i.e., chr) columns using where() and is.character: data_character_columns &lt;- data_exp %&gt;% select(where(is.character)) You can see the new data contains only the character columns: glimpse(data_character_columns) ## Rows: 6 ## Columns: 1 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;f… In a future chapter you will see how we can select factors using where(is.factor). 2.6.2 summarise() The summarise() command can summarise multiple columns when combined with starts_with(), ends_with(), contains(), matches(), and where(). However, to use these powerful tools for indicating columns with the summarise command we need the help of the across() command (i.e., across multiple columns). If we want to obtain the mean of all the columns that start with “t1” we use the commands below. The across command requires that we indicate the columns we want via the .cols argument and the command/function we want to run on those columns via the .fns argument. In the example below, we also add na.rm = TRUE at the end; this is something we send to the mean command to let it know how we want to handle missing data. data_exp %&gt;% summarise(across(.cols = starts_with(&quot;t1&quot;), .fns = mean, na.rm = TRUE)) ## Warning: There was 1 warning in `summarise()`. ## ℹ In argument: `across(.cols = starts_with(&quot;t1&quot;), .fns ## = mean, na.rm = TRUE)`. ## Caused by warning: ## ! The `...` argument of `across()` is deprecated as of ## dplyr 1.1.0. ## Supply arguments directly to `.fns` through an ## anonymous function instead. ## ## # Previously ## across(a:b, mean, na.rm = TRUE) ## ## # Now ## across(a:b, \\(x) mean(x, na.rm = TRUE)) ## # A tibble: 1 × 2 ## t1_vomit t1_aggression ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.83333 5.5 If you want to get more sophisticated, you can also add this .names argument below which tells R to call label each output mean by the column name followed by “_mean”. data_exp %&gt;% summarise(across(.cols = starts_with(&quot;t1&quot;), .fns = mean, na.rm = TRUE, .names = &quot;{col}_mean&quot;)) ## # A tibble: 1 × 2 ## t1_vomit_mean t1_aggression_mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.83333 5.5 Often you want to calculate more than one statistic for each column. For example, you might want the mean, standard deviation, min, and max. These statistics can be calculated via the mean, sd, min, and max commands, respectively. However, you need to create a list with the statistics you desire. Below we create a list of the descriptive statistics we desire called desired_statistics, but you can use any name you want. This list only needs to be specified once, but we will repeat it in the examples below for clarity. desired_descriptives &lt;- list( mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE) ) Once you have created the list of descriptive statistics you want you can run the command below to obtain those statistics. However, as you will see the output is too wide to be helpful. data_exp %&gt;% summarise(across(.cols = starts_with(&quot;t1&quot;), .fns = desired_descriptives)) ## # A tibble: 1 × 4 ## t1_vomit_mean t1_vomit_sd t1_aggression_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.83333 1.16905 5.5 ## # ℹ 1 more variable: t1_aggression_sd &lt;dbl&gt; Consequently, we add the t() command (i.e., transpose command) to the end of the summarise request to get a more readable list of statistics: desired_descriptives &lt;- list( mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE) ) data_exp %&gt;% summarise(across(.cols = starts_with(&quot;t1&quot;), .fns = desired_descriptives)) %&gt;% t() ## [,1] ## t1_vomit_mean 1.833 ## t1_vomit_sd 1.169 ## t1_aggression_mean 5.500 ## t1_aggression_sd 1.871 Note that in the across command above we could also have used: ends_with(), contains(), matches(), or where(). 2.6.3 mutate() The mutate() command can also be applied to multiple columns using the across() command. However, sometimes we need to embed our calculation in a custom function. Below is a custom function called make_centered. This custom function takes the values in a column and subtracts the column mean from each value in the column. This is the same task we did previous using the mutate() command in the basic tidyverse section. make_centered &lt;- function(values) { values_out &lt;- values - mean(values, na.rm = TRUE) return(values_out) } The glimpse() command shows us all the column names. Also notice the values in the agresssion columns are integers. glimpse(data_exp) ## Rows: 6 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, … ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; 5, 6, 4, 7, 3, 8 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; 6, 7, 6, 7, 5, 8 We combine the mutate() command, with the across() command, and our custom make_centered() command below. The command “centers” or subtracts the mean from any column that ends with “aggression”. data_exp &lt;- data_exp %&gt;% mutate(across(.cols = ends_with(&quot;aggression&quot;), .fns = make_centered)) You can see via the glimpse() output that the contents of all the columns that end with “aggression” have changed. Every value in each of these columns has had the column mean subtracted from it. glimpse(data_exp) ## Rows: 6 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, … ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; -0.5, 0.5, -1.5, 1.5, -2.5, 2.5 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; -0.5, 0.5, -0.5, 0.5, -1.5, 1.5 Note that in the across() command above, we could also have used: starts_with(), contains(), matches(), or where(). 2.6.3.1 mutate() within rows Researchers often want to average within rows and across columns to create a new column. That is, for each participant (i.e., rat in the current data) we might want to calculate a vomit score that is the average of the two time points (that we will call vomit_avg). To average within rows (and across columns) we use the rowwise() command to inform R of our intent. However, after we do the necessary calculations we have to shut off the rowwise() calculation state by using the ungroup() command. As well, when we are averaging within rows we have to use c_across() instead of across(). The commands below create a new column called vomit_avg which is the average of the vomit ratings across both times. As before, we also include na.rm = TRUE so the computer drops missing values (if present) when calculating the mean. data_exp &lt;- data_exp %&gt;% rowwise() %&gt;% mutate(vomit_avg = mean( c_across(cols = ends_with(&quot;vomit&quot;)), na.rm = TRUE)) %&gt;% ungroup() You can see the new column we created with the glimpse() command: glimpse(data_exp) ## Rows: 6 ## Columns: 7 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, … ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; -0.5, 0.5, -1.5, 1.5, -2.5, 2.5 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; -0.5, 0.5, -0.5, 0.5, -1.5, 1.5 ## $ vomit_avg &lt;dbl&gt; 2.5, 1.5, 0.5, 2.5, 1.5, 1.5 The print() command could make it easier to see that the new column is the average of the other two, but if we use the print() command below it wouldn’t work. Why? There are too many columns in the data set, so only the first few columns are shown. print(data_exp) To see that the new column, vomit_avg, is the average of the other vomit columns we use the select command before print(). This prints only the relvant columns. When this is done, it’s easy to see how the values in the vomit_avg column are the mean of the other two columns. data_exp %&gt;% select(contains(&quot;vomit&quot;)) %&gt;% print() ## # A tibble: 6 × 3 ## t1_vomit t2_vomit vomit_avg ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 2 2.5 ## 2 2 1 1.5 ## 3 0 1 0.5 ## 4 3 2 2.5 ## 5 2 1 1.5 ## 6 1 2 1.5 2.6.3.2 mutate() within rows using column names Sometimes it can be difficult to use one of the advanced select commands to obtain the columns you need to average across. The advanced commands like ends_with() and starts_with() can sometimes include columns you don’t want. The command below is equivalent to the one above, however, we explicitly name the variables we want to average across. data_exp &lt;- data_exp %&gt;% rowwise() %&gt;% mutate(vomit_avg = mean( c_across(cols = c(t1_vomit, t2_vomit)), na.rm = TRUE)) %&gt;% ungroup() You can use print() to confirm we get the same result: data_exp %&gt;% select(contains(&quot;vomit&quot;)) %&gt;% print() ## # A tibble: 6 × 3 ## t1_vomit t2_vomit vomit_avg ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 2 2.5 ## 2 2 1 1.5 ## 3 0 1 0.5 ## 4 3 2 2.5 ## 5 2 1 1.5 ## 6 1 2 1.5 2.6.3.3 mutate() for factors It is critical that you indicate to R that categorical variables are in fact categorical variables. In R, categorical variables are referred to as factors. For humans, a factor like sex has three possible levels: female, male, intersex. An inspection of the glimpse() command output reveals that the sex column has the type character - as indicated by “chr”. Also notice, as you inspect this output, that we use words (e.g., female) to indicate the sex in the column rather than a number to represent a female participant (e.g., 2). This is the preferred, but less common, approach to entering data. glimpse(data_exp) ## Rows: 6 ## Columns: 7 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, … ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; -0.5, 0.5, -1.5, 1.5, -2.5, 2.5 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; -0.5, 0.5, -0.5, 0.5, -1.5, 1.5 ## $ vomit_avg &lt;dbl&gt; 2.5, 1.5, 0.5, 2.5, 1.5, 1.5 We need to convert the sex column to a factor in order for R to handle it appropriately in analyses. Failure to indicate the column is a factor column could result in R conducting all the analyses and presenting incorrect results. Consequently, it is critical that we covert the column to a factor. Fortunately, that is easily done using the as_factor() command (there is also an as.factor command if as_factor won’t work for some reason). We convert the sex column to a factor with this code: data_exp &lt;- data_exp %&gt;% mutate(sex = as_factor(sex)) You can confirm this worked with the glimpse() command: glimpse(data_exp) ## Rows: 6 ## Columns: 7 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, fe… ## $ t1_vomit &lt;dbl&gt; 3, 2, 0, 3, 2, 1 ## $ t1_aggression &lt;dbl&gt; -0.5, 0.5, -1.5, 1.5, -2.5, 2.5 ## $ t2_vomit &lt;dbl&gt; 2, 1, 1, 2, 1, 2 ## $ t2_aggression &lt;dbl&gt; -0.5, 0.5, -0.5, 0.5, -1.5, 1.5 ## $ vomit_avg &lt;dbl&gt; 2.5, 1.5, 0.5, 2.5, 1.5, 1.5 If you entered your data using words for each level of sex (e.g., male, female) you’re done at this point. However, if you used numbers to represent each level of sex in your data, there is one more step. Imagine your data was entered in a poorly advised manner, such that 1 was used to indicate male, 2 was used to indicate female, and 3 was used to indicate intersex. If this was the case, you need to indicate to R what each of those values represent. We do that with the code below. data_exp &lt;- data_exp %&gt;% mutate(sex = fct_recode(sex, male = &quot;1&quot;, female = &quot;2&quot;, intersex = &quot;3&quot;)) 2.7 Using help In order to become efficient at analyzing data using R, you will need to become adapt at reading and understanding the help files associated with each command. After you have activated a package using the library command (e.g., library(tidyverse)) you can access the help page for every command in that package. To access the help page simply type a question mark followed by the command you want to know how to use (no space between them). The code below brings up the help page for the select() command. Notice that we put the library() command first - just a reminder that this needs to be done prior to using help for that package. Try the commands below in the Console: library(tidyverse) ?select Examine the page that appears on the Help tab in the panel in the lower right of your screen. Read through the help file comparing what you read there to what we have learned about the select command. Notice how the help file tells you about the argument that you sent into the select() command, and also what the select() command returns when it receives those commands. Pay particular attention to the examples near the bottom of the help page. At the very bottom of the help page you will see [Package dplyr version 1.0.0 Index]. This tells you the select() command is from the dplyr package (part of the tidyverse). Notice that the word “Index” is underlined. Click on the word Index. You will be presented with list of other commands in the dplyr package. As you become more experienced with R help pages, this is how you will learn to use new commands. Examine the help pages for the commands below by typing a question mark into the Console followed by the command name. Note that for filter and starts_with you will be presented with a menu instead of help page. This typically occurs because the command is in more than one package. If this does occur, read through the options you are presented with to try and figure out which one you wanted. Typically, you want the first option. If you’re not sure, try one. If it’s not what you want, use the back arrow in the Help panel to go back and pick another one. mutate filter starts_with 2.8 Base R vs tidyverse All of the commands used to this point in the chapter have been the tidyverse approach to using R. That is the approach we will normally use. However, it’s important to note that there is another way of using R, called base R. Sometimes students have problems with their code when they mix and match these approaches using a bit of both. We will be using the tidyverse approach to using R but on the internet you will often see sample code that uses the older base R approach. A bit of background knowledge is helpful for understanding why we do things one way (e.g., read_csv with the tidyverse) instead of another (e.g., read.csv with base R). 2.8.1 Tibbles vs. data frames When you load data into R, it is typically represented in one of two formats inside the computer - depending on the command you used. The original format for representing a data set in R is the data frame. You will see this term used frequently when you read about R. When you load data using read.csv(), your data is loaded into a data frame in the computer. That is, your data is represented in the memory of the computer in a particular format and structure called a data frame. This contrasts with the newer tidyverse approach to representing data in the computer called a tibble - which is just a newer more advanced version of the data frame. 2.8.2 read.csv() and data frames When you read data into R using the command read.csv() (with a period) you load the data into a data frame (base R). my_dataframe &lt;- read.csv(file = &quot;data_okcupid.csv&quot;) Notice that when you print a data frame it does not show you the number of rows or columns above the data like our example did with the okcupid_profiles data. Likewise, it does not show you the type of data in each column (e.g., dbl, fct, chr). It also presents all of your data rather than just the first few rows (as the tibble does). As a result, in the output below, we show only the first 10 rows of the output - because all the rows are printed in your Console with a data frame (too much to show here). print(my_dataframe) ## age diet height pets ## 1 22 strictly anything 75 likes dogs and likes cats ## 2 35 mostly other 70 likes dogs and likes cats ## 3 38 anything 68 has cats ## 4 23 vegetarian 71 likes cats ## 5 29 &lt;NA&gt; 66 likes dogs and likes cats ## 6 29 mostly anything 67 likes cats ## 7 32 strictly anything 65 likes dogs and likes cats ## 8 31 mostly anything 65 likes dogs and likes cats ## 9 24 strictly anything 67 likes dogs and likes cats ## 10 37 mostly anything 65 likes dogs and likes cats ## sex status ## 1 m single ## 2 m single ## 3 m available ## 4 m single ## 5 m single ## 6 m single ## 7 f single ## 8 f single ## 9 f single ## 10 m single 2.8.3 read_csv() and tibbles When you read data into R using the command read_csv() (with an underscore) you load the data into a tibble (tidyverse). my_tibble &lt;- read_csv(file = &quot;data_okcupid.csv&quot;) ## Rows: 59946 Columns: 6 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): diet, pets, sex, status ## dbl (2): age, height ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The tibble is modern version of the data frame. Notice that when you print a tibble it DOES show you the number of rows and columns. As well, it shows you the type of data in each column. Importantly, the tibble only provides the first few rows of output so it doesn’t fill your screen. print(my_tibble) ## # A tibble: 59,946 × 6 ## age diet height pets sex status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 strictly anything 75 likes dogs a… m single ## 2 35 mostly other 70 likes dogs a… m single ## 3 38 anything 68 has cats m avail… ## 4 23 vegetarian 71 likes cats m single ## 5 29 &lt;NA&gt; 66 likes dogs a… m single ## 6 29 mostly anything 67 likes cats m single ## 7 32 strictly anything 65 likes dogs a… f single ## 8 31 mostly anything 65 likes dogs a… f single ## 9 24 strictly anything 67 likes dogs a… f single ## 10 37 mostly anything 65 likes dogs a… m single ## # ℹ 59,936 more rows In short, you should always use tibbles (i.e., use read_csv() not read.csv()). The differences between data frames and tibbles run deeper than the superficial output provided here. On some rare occasions an old package or command may not work with a tibble so you need to make it a data frame. You can do so with the commands below. We will flag these rare occurances to you when they occur. # Create a data frame from a tibble new_dataframe &lt;- as.data.frame(my_tibble) "],["graphing.html", "Chapter 3 Graphing 3.1 Required 3.2 Data 3.3 Graph basics 3.4 Graphing efficiently 3.5 Aesthetics 3.6 APA style 3.7 Axes 3.8 Axis values 3.9 Custom colours 3.10 Emoji 3.11 Accessible Colors 3.12 Saving 3.13 Custom themes", " Chapter 3 Graphing A great part of learning R is learning how to use ggplot2 which is part of the tidyverse. One way to learn ggplot2 is via the free ggplot book - I encourage you to check it out. You might also check out this ggplot2 cheatsheet. As well, you might consult this blog to learn more about how themes and colors are handled in ggplot2. In this chapter we merely provide a brief overview of how ggplot2 works. You can also consult this free book on data visualization techniques. 3.1 Required The data files below are used in this chapter. Right click to save each file. Required Data data_movies.csv The following packages CRAN must be installed: Required CRAN Packages tidyverse RColorBrewer remotes The following GitHub packages must be installed: Required GitHub Packages dill/emoGG After the remotes package is installed, it can be used to install a package from GitHub: remotes::install_github(&quot;dill/emoGG&quot;) 3.2 Data To learn about making graphs using the tidyverse we use movie ratings and box office data obtained at the time of writing. Movie ratings were obtained from the IMDB and RottenTomatoes. Box office data (in millions of dollars) was obtained from Box Office Mojo. If you enjoy learning about movies these are all excellent sites. We begin by loading data_movies.csv using read_csv(), not read.csv(): movie_data &lt;- read_csv(&quot;data_movies.csv&quot;) Next we inspect movie_data using the print() command. We see that each row of the data set corresponds to a superhero movie. print(movie_data) ## # A tibble: 8 × 7 ## title short_title year imdb tomatoes_aud boxoffice ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Iron Man Iron 2008 7.9 96 585 ## 2 Thor Ragna… Thor 2017 7.9 93 854 ## 3 Avengers I… AV3 2018 8.5 91 2048 ## 4 Avengers E… AV4 2019 8.7 91 2744 ## 5 Man of Ste… Sup 2013 7.1 75 668 ## 6 Batman v S… BvS 2015 6.5 63 873 ## 7 Justice Le… JL 2017 6.5 72 657 ## 8 Wonder Wom… WW 2017 7.5 88 821 ## # ℹ 1 more variable: studio &lt;chr&gt; Next we use glimpse() to see the columns. glimpse(movie_data) ## Rows: 8 ## Columns: 7 ## $ title &lt;chr&gt; &quot;Iron Man&quot;, &quot;Thor Ragnarok&quot;, &quot;Avenger… ## $ short_title &lt;chr&gt; &quot;Iron&quot;, &quot;Thor&quot;, &quot;AV3&quot;, &quot;AV4&quot;, &quot;Sup&quot;, … ## $ year &lt;dbl&gt; 2008, 2017, 2018, 2019, 2013, 2015, 2… ## $ imdb &lt;dbl&gt; 7.9, 7.9, 8.5, 8.7, 7.1, 6.5, 6.5, 7.5 ## $ tomatoes_aud &lt;dbl&gt; 96, 93, 91, 91, 75, 63, 72, 88 ## $ boxoffice &lt;dbl&gt; 585, 854, 2048, 2744, 668, 873, 657, … ## $ studio &lt;chr&gt; &quot;Marvel&quot;, &quot;Marvel&quot;, &quot;Marvel&quot;, &quot;Marvel… The title and short_title columns provide the full title and short title for each movie. Additionally, the IMDB rating, the Rotten Tomatoes Audience rating, and the Box Office Mojo revenue numbers are provided in the imdb, tomatoes_aud, and boxoffice columns, respectively. Finally, the last column, studio, indicates the studio that made the movie (Marvel or DC). It is extremely important for graphing and analyses that you tell R which columns are composed of categorical variables. We do that using the as_factor command. The as_factor command turns a column into a categorical column. We use the mutate command to replace the original column with the column that has been defined as a categorical variables using as_factor. movie_data &lt;- movie_data %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) We can confirm the column type has changed by using the glimpse() command again and examing the column types: glimpse(movie_data) ## Rows: 8 ## Columns: 7 ## $ title &lt;fct&gt; Iron Man, Thor Ragnarok, Avengers Inf… ## $ short_title &lt;fct&gt; Iron, Thor, AV3, AV4, Sup, BvS, JL, WW ## $ year &lt;dbl&gt; 2008, 2017, 2018, 2019, 2013, 2015, 2… ## $ imdb &lt;dbl&gt; 7.9, 7.9, 8.5, 8.7, 7.1, 6.5, 6.5, 7.5 ## $ tomatoes_aud &lt;dbl&gt; 96, 93, 91, 91, 75, 63, 72, 88 ## $ boxoffice &lt;dbl&gt; 585, 854, 2048, 2744, 668, 873, 657, … ## $ studio &lt;fct&gt; Marvel, Marvel, Marvel, Marvel, DC, D… 3.3 Graph basics In this section we teach you how to make a graph from first principles to form a foundation for understanding how the tidyverse graphing command ggplot() works. Note, however, that the approach used for creating a graph in this section is for teaching purposes only. Later we will make graphs in a typical, and more efficient, manner. We start a graph using the ggplot() command. The ggplot() command creates an empty template for the graph. After creating the template we have to add content (like bars) to the graph using the geom_col() command. We can also add text using the geom_text() command. Commands that plot information on the graph, such as geom_col(), need to know what data set to use to create the graph. We specify the data set to use via the data argument. For example, we use “data = movie_data” to tell a command which data set to use. Additionally, graphing commands, such as geom_col(), must know the columns/variables to use within that data set when plotting the graph. Specifically, commands need to know which variable/column will vary over the x- and y-axes. We can indicate these columns via the mapping argument. For example, we use “mapping = aes(x = short_title, y = boxoffice)” to tell ggplot() that we should use the column short_title along the x-axis and the column boxoffice when determining heights on the y-axis. This information is nested within the aes() command which is short for aesthetic. You are telling ggplot() about the aesthetics for the graph (i.e., which columns to use for the x- and y-axes) using the aes() command. There are a larger number of aesthetics that you can specify within the aes() command (e.g., color, fill, linetype, etc.). In the examples that follow we tell each command (geom_col, geom_text) which data set and columns to use via the data and mapping arguments. Use geom_col() to put each boxoffice column value into a bar. my_graph &lt;- ggplot() + geom_col(data = movie_data, mapping = aes(x = short_title, y = boxoffice)) print(my_graph) Next, we want to put the boxoffice revenue above each bar so it easier to interpret. In R terms, we are putting a label above each bar. We want the contents for the labels to come from the boxoffice column. Therefore, we add the geom_text() command below: my_graph &lt;- ggplot() + geom_col(data = movie_data, mapping = aes(x = short_title, y = boxoffice)) + geom_text(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice)) print(my_graph) Unfortunately, when we position the text at the exact height of each column it overlaps with the column making it difficult to read. We fix this on the next page using nudge_y. We can nudge each label higher on the y-axis using the nudge_y command. In the above code, we nudge it up 150 units. Since nudge_y uses the values on the y-axis we are nudging the labels up by 150 million on the y-axis. my_graph &lt;- ggplot() + geom_col(data = movie_data, mapping = aes(x = short_title, y = boxoffice)) + geom_text(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice), nudge_y = 150) print(my_graph) 3.4 Graphing efficiently You may have noticed that creating the graphs the way we did above required repeating the data and mapping assignments within each command (e.g., geom_text, geom_col). Fortunately, we can use a shortcut and specify the data and mapping only once in the ggplot() command. Once we do that, the contents of the mapping argument are invisibly copied into each subsequent command (e.g., geom_col, geom_text). In this way, we only have to specify the data and the mapping once. Examine the code below and compare it to the code above. Notice how in the geom_col() command we don’t have anything specified – the data and mapping from the ggplot command are used. Likewise, notice how in the geom_text() command we only specify the arguments we need that are different from those in the ggplot command. In this case, that means simply adding the nudge_y = 150 to the geom_text command. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice)) + geom_col() + geom_text(nudge_y = 150) print(my_graph) 3.5 Aesthetics Exactly how does that aesthetic, aes(), command work? What happens when we put the data and mapping in the ggplot() command instead of the specific commands such as geom_col()? When we put data and the mapping arguments in the ggplot() command we set those attributes for the entire graph. To understand this, you need to know that ggplot uses an internal data set that we will call the “black box” data set (i.e., inside the black box of ggplot). To create a graph ggplot maps/copies columns from your data set (e.g., movie_data) to an internal data set. This internal data set is then used to create the graph. Figure 3.1 below illustrates what is happening “inside the black box” when you create the graph using the code above. FIGURE 3.1: Internal data structure for ggplot 3.5.1 Fill color You might want to influence the color of the bars in the graph such that the bars for Marvel and DC movies have different colors. That’s easy to do with the aes() command. We simply tell aes() that the fill color of any object in the graph should be determined by the studio column. Simply adding “fill = studio” to the aes() command changes the colors of the bars – and any other object on the graph for which fill would be relevant. The internal workings are illustrated in Figure 3.2. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio )) + geom_col() + geom_text(nudge_y = 150) print(my_graph) FIGURE 3.2: Adding a fill column to the internal data 3.5.2 Overriding aes() Just because you specify something in the ggplot() command doesn’t mean that you are “stuck with it” for all your subsequent commands. Recall how at the start of this exercise we specified the data and the mapping for each geom_col() and geom_text() individually. We can still do that. Now we want to add the Rotten Tomatoes Audience score for each movie above the box office revenue. But if we use geom_text(), like we did before, it will plot the same boxoffice information because of “label = boxoffice” in the aes() specification within ggplot(). We want the new geom_text() command to plot different text; that is, we want it to use “label = tomatoes_aud”. Fortunately, if we simply put “mapping = aes(label = tomatoes_aud)” within the new geom_text() command we get the desired information on the graph. The mapping/aes arguments within geom_text() override the mapping/aes arguments within ggplot(). Or more accurately, the new geom_text() command creates its own version of the internal data set in which the label column is filled with information from tomatoes_aud. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud)) print(my_graph) Notice that we have the same problem as before with the text being difficult to read because it overlaps with the bar. We add “nudge_y = 400” to move the text higher than the boxoffice text/label. Don’t forget the units used by nudge_y are the units on the y-axis. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) print(my_graph) 3.6 APA style Use theme_classic() to make the graph apear in APA style. We use theme_classic(12) to make the graph APA style with a 12-point font: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + theme_classic(12) print(my_graph) 3.7 Axes 3.7.1 Range We use the coord_cartesian() command to adjust range of axes. In the code below we use coord_cartesian() to make the y-axis range from 0 to 3500. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + theme_classic(12) print(my_graph) Note that if you had a continuous variable on the x-axis (we do not in this example), you could set the range of both the x- and y-axes like this: coord_cartesian(ylim = c(0, 3500), xlim = c(0, 3500)) 3.7.2 Ticks We use the scale_y_continuous() command to adjust the ticks on the y-axis. We set the ticks on the y-axis to range from 0 to 3500 in 500 tick increments using the scale_y_continuous command below via the breaks argument: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + theme_classic(12) print(my_graph) Note that if you had a continuous variable on the x-axis (we do not in this example), you could set the ticks of the x-axis like the code below using scale_x_continuous: scale_x_continuous(breaks = seq(0, 3500, by = 500)) 3.7.3 Labels Labels are an extremely important part of any graph. This fact is the focus of the xkcd cartoon below: We use the labs() command to set the labels for the x- and y-axes: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) print(my_graph) 3.8 Axis values 3.8.1 Text What if we want to use the full movie title rather than the short version on the x-axis of the graph? That is, you want to change the values along the x-axis. Two methods are presented below. 3.8.1.1 Method 1: Recoding axis values Our data file contains a column with the long/full version of the movie names. But many times you won’t have the additional/longer labels available in this manner. In this situation, you use the scale_x_discrete() command to change the values along the x-axis. The values along the x-axis come from the short_title column which is a factor. The levels of that factor correspond to the short titles for the movies (Iron, Thor, etc.). We need to relabel the levels of the short_title factor to get a graph with full titles. We relabel the levels of the short_title factor using the scale_x_discrete() command. The graph code with scale_x_discrete() command is below - notice the problem we have with the labels overlapping though. On the next page, we’ll use an easier approach though - since we have an extra column with the full titles. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = short_title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + scale_x_discrete(labels=c(&quot;Iron&quot; = &quot;Iron Man&quot;, &quot;Thor&quot; = &quot;Thor Ragnarkok&quot;, &quot;AV3&quot; = &quot;Avengers Infinity War&quot;, &quot;AV4&quot; = &quot;Avengers Endgame&quot;, &quot;Sup&quot; = &quot;Man of Steel&quot;, &quot;BvS&quot; = &quot;Batman v Superman&quot;, &quot;JL&quot; = &quot;Justice League&quot;, &quot;WW&quot; = &quot;Wonder Woman&quot;)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) print(my_graph) 3.8.1.2 Method 2: Longer label columns With our movie data we don’t need to use scale_x_discrete() because we have a column in the data with the full titles. Consequently, to use full length titles we just have to change the mappingfor x from short_title to title. Notice that we still have the problem with overlapping labels on the x-axis! my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) print(my_graph) 3.8.2 Angle Use theme() to adjust the angle of x-axis labels. Notice, however, that the longer titles are vertically centered on each point on the x-axis. In the next section we fix this problem. Important: The theme command must come after the theme_classic command. Otherwise, theme_classic will undo the work done by the theme_command if it appears after the theme command. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60)) print(my_graph) 3.8.3 Alignment Use theme() and the hjust argument to adjust the alignment of the x-axis labels. To make the titles look correct on the x-axis we need them at an angle, but we also need them right justified against the x-axis. We do that with the the hjust argument (1 means right justify). See the code below: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) print(my_graph) 3.8.4 Order 3.8.4.1 Increasing order We can make the movie bars go left to right in lowest to highest box office receipt order by changing the factor order prior to creating the graph. We do so with the mutate() and fct_reorder() commands. The default order is ascending values even though we don’t specify it. movie_data &lt;- movie_data %&gt;% mutate(title = fct_reorder(title, boxoffice)) my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) print(my_graph) 3.8.4.2 Decreasing order We can make the movie bars go left to right in highest to lowest box office receipt order by changing the factor order prior to creating the graph. We use the same code as before but add the desc() command (i.e.,descending) around boxoffice in the fct_reorder() call: movie_data &lt;- movie_data %&gt;% mutate(title = fct_reorder(title, desc(boxoffice))) my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) print(my_graph) 3.8.4.3 Custom order We can make the movie bars go left to right in a custom order by changing the factor order prior to creating the graph. Because we want a custom order of the factor levels we use fct_relevel(), instead of the the fct_reorder() command from the previous two examples. Below I use this approach to manually order movies highest to lowest boxoffice within movie studio (Marvel or DC). movie_data &lt;- movie_data %&gt;% mutate(title = fct_relevel(title, &quot;Avengers Endgame&quot;, &quot;Avengers Infinity War&quot;, &quot;Thor Ragnarok&quot;, &quot;Iron Man&quot;, &quot;Batman v Superman&quot;, &quot;Wonder Woman&quot;, &quot;Man of Steel&quot;, &quot;Justice League&quot;)) my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) 3.8.5 Legend order After inspecting the graph on the previous page, you might think that Marvel should be above DC in the legend. You can do that by reordering the studio factor: movie_data &lt;- movie_data %&gt;% mutate(studio = fct_relevel(studio, &quot;Marvel&quot;, &quot;DC&quot;)) my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) 3.9 Custom colours 3.9.1 R palette You might look at the previous graph and think “Marvel should be red and DC should be blue since those are the colours of their respective logos”. You can do that with the code below. Note that you specify the colours in the order the names appear in the legend (top to bottom). R colour names/pictures can be found here: http://sape.inf.usi.ch/quick-reference/ggplot2/colour my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;)) 3.9.2 Hex colours If you are a really big geek (like me) you might look at the previous graph and think “Those aren’t the proper colours for the Marvel and DC - lame!” So… you do some internet research and determine that you can specify colours using hexidecimal numbers. More specifically, you find Marvel red is #ed1d24 and DC blue is #0476F2 using hex colour codes. You can use those precise colours via the scale_fill_manual() command below. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 400) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;#ed1d24&quot;, &quot;#0476F2&quot;)) 3.10 Emoji Make the graph more fun with the emoGG package. You might like to make the graph more fun by putting tomatoes on the graph to indicate what the extra numbers mean. We can do that with the emoGG package. The installation instructions for this package are at the start of this chapter; note, that it is installed via GitHub rather than the CRAN. Course R Studio Cloud users - the installation has already been done. After installation you need to activate the emoGG package: library(emoGG) Visit this link to check out the codes for emoji: https://apps.timwhitlock.info/emoji/tables/unicode If you scroll down to section 5 Uncategorized on this page you will find the code for a tomato is 1f345. Note that the code below will only work with an internet connection. The command geom_emoji() needs internet access to retrieve the emoji graphic requested. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 600, colour = &quot;red&quot;) + geom_emoji(mapping = aes(y = boxoffice + 400), emoji=&quot;1f345&quot;) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;#ed1d24&quot;, &quot;#0476F2&quot;)) 3.11 Accessible Colors The current version of the graph is much improved from where we started at the begining of the chapter. One notable improvement was the use of the “proper” colors for the Marvel and DC studios. Although using these colors was aethetically pleasing, a major consideration is ensuring your graphs are accessible to a wide audience. Color blindness is an issue that affects approximately five percent of the population. A nuanced discussion of the different types of color blindness is beyond the scope of this chapter. We can, however, take a momement to think about every graph as being composed of two parts that work together to create the colors overall image. We can think of there being a lightness (i.e., light vs. dark) component and a hue component (e.g., magenta, yellow, etc.). Together these two components work together to create colors that we see. This distinction between lightness and hue is relevant to all images not just graphs (see Margulis 2005). FIGURE 3.3: Hue removed from current graph Figure 3.3A presents our current graph whereas Figure 3.3B presents the same graph with the hue component removed. You can see that when the hue component is removed that the distinction between the Marvel and DC bars is also removed. The particular colors used to represent Marvel/DC differ in terms of hue but not lightness. Consequently, when we remove the hue component we are left with a graph, 3.3B, that does not differentiate between the two studios. In order for a color graph to be accessible to people with color blindness we need to pick colors that vary in terms of lightness as well as hue. We can do that with the help of the RColorBrewer package. 3.11.1 RColorBrewer 3.11.1.1 Picking a palette The RColorBrewer package can be used to generate color palettes for graphs that are accessible to people with color blindness. That is, it creates sets of colors, called palettes, for which the colors vary in terms of both lightness and hue. You can see the color-blind accessible palettes, along with their respective names, by using the command below. This code produces a wide range of color-blind accessible palettes with a large number of colors in each palette - as illustrated in Figure 3.4. library(RColorBrewer) display.brewer.all(n = NULL, colorblindFriendly = TRUE) FIGURE 3.4: Wide range color palette (color-blind accessible) In Figure 3.4, above, we showed palettes with a large number of colors. But if you have a smaller number of possible colors in your graph - you want a palette with fewer colors (to ensure maximum contrast between those colors). You can, for example, obtain palettes with only three colors using the code below. library(RColorBrewer) display.brewer.all(n = 3, colorblindFriendly = TRUE) The above code generates the three-color palettes presented in Figure 3.5. FIGURE 3.5: Narrow range color palette (color-blind accessible) 3.11.1.2 Using a palette We can use a color palette by specifying its name within scale_fill_brewer(). We want to use the “Paired” palette in Figure 3.5 so we use the code: scale_fill_brewer(palette = “Paired”), as illustrated below: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 600, colour = &quot;red&quot;) + geom_emoji(mapping = aes(y = boxoffice + 400), emoji=&quot;1f345&quot;) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_brewer(palette = &quot;Paired&quot;) 3.11.1.3 Palette subsets Our graph had only two colors but the “Paired” palette had three colors. When ggplot made the graph it automatically used the first two colors of the three color palette. What if you wanted the second two colors in the palette? We can do that but we have to revert to a manual color process like we used before via the scale_fill_manual(). But we want to do so using the colors from the “Paired” palette. We can obtain the color codes for the “Paired” palette with the code below: brewer.pal(n = 3, name = &quot;Paired&quot;) ## [1] &quot;#A6CEE3&quot; &quot;#1F78B4&quot; &quot;#B2DF8A&quot; The numbers in the output correspond to values representing the three colors of the “Paired” palette, see Figure 3.6. If we want the last two colors in the palette for our graph that means we want the colors: #1F78B4 and #B2DF8A. FIGURE 3.6: Paired palette colors with hex numbers We can put the two colors we want (#1F78B4 and #B2DF8A) on the graph by using the scale_fill_manual() command - instead of the scale_fill_brewer() command. We can see this in the code below: my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 600, colour = &quot;red&quot;) + geom_emoji(mapping = aes(y = boxoffice + 400), emoji=&quot;1f345&quot;) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;#1F78B4&quot;, &quot;#B2DF8A&quot;)) Now that we have used a color-blind accessible palette in the graph we can look at the graph without the hue component. In Figure 3.7B, below, notice that the bars representing the two studios are easily distinguishable based on lightness alone. That is, the lightness graph in Figure 3.7B demonstrates the new color-blind accessible colors vary in term of both hue and lightness - not just hue. Therefore, when hue is removed the two colors are distinguishable. This makes the graph accessible to color-blind individuals. FIGURE 3.7: Color information (i.e., hue) removed from new color-blind accessible graph 3.11.2 Avoid color When you only have two colors on a graph another option for creating an accessible graph is to remove the colors entirely and make a black and white graph. In the code below we use scale_fill_manual() with the values “#ffffff” (white) and ““#000000” (black). The intent was to create bars that are white for Marvel and black for DC. Unfortunately, because the graph has a white background you can see this resulted in the Marvel bars disappearing. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col() + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 600, colour = &quot;red&quot;) + geom_emoji(mapping = aes(y = boxoffice + 400), emoji=&quot;1f345&quot;) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;#ffffff&quot;, &quot;#000000&quot;)) How do we get around the Marvel bars disappearing when we use white? Color vs. fill. The ggplot packages makes a distinction between fill and color. The term fill is used to refer to the inside color of bars. In contrast, color is used to the refer to the lines that outline the shape of each bar. In the previous code we used fill to change the colors of the bars but we did not use the “color” argument in a command. In the code below, we modify the geom_col() command to use the “color” argument. Specifically, we change geom_col() to geom_col(color = “black”). This change adds a black outline around all the bars – including the ones with a white fill. You can see the result is a much improved graph. This black/white graph is accessible to everyone. my_graph &lt;- ggplot(data = movie_data, mapping = aes(x = title, y = boxoffice, label = boxoffice, fill = studio)) + geom_col(color = &quot;black&quot;) + geom_text(nudge_y = 150) + geom_text(mapping = aes(label = tomatoes_aud), nudge_y = 600, colour = &quot;red&quot;) + geom_emoji(mapping = aes(y = boxoffice + 400), emoji=&quot;1f345&quot;) + coord_cartesian(ylim = c(0, 3500)) + scale_y_continuous(breaks = seq(0, 3500, by = 500)) + labs(x = &quot;Movie&quot;, y = &quot;Box office (millions)&quot;, fill = &quot;Studio&quot;) + theme_classic(12) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_fill_manual(values = c(&quot;#ffffff&quot;, &quot;#000000&quot;)) 3.12 Saving If you have a Mac it is easy to drag and drop a PDF file into MS Word - so making a PDF file is the best bet for saving your graph. You can do so with the code below which creates a 6 inch by 6 inch graph. 3.12.1 MAC If you are able to use PDFs in your workflow that’s often the best option for saving. PDFs are mathematical in nature and therefore can be printed at any size at high quality. With a MAC you can just drag and drop the PDF file into your MSWord document. ggsave(plot = my_graph, filename = &quot;emoji_graph.pdf&quot;, width = 6, height = 6) 3.12.2 PC or MAC If you have a PC it’s hard to put a PDF into MSWord. Therefore, save the graph as a .jpg file. You do so with the code below. This creates a picture type file at a resolution (dpi = dots per inch) that is sufficiently high for quality printing. With a PC you need to use the INSERT menu and insert the graph as a picture in MSWord. With a MAC you can just drag and drop the .jpg file into your MSWord document. ggsave(plot = my_graph, filename = &quot;emoji_graph.jpg&quot;, width = 6, height = 6, dpi = &quot;print&quot;) 3.13 Custom themes Did you want to modify a theme more extensively? Here is a cheat sheet for doing so created by Clara Granell. The PDF of the cheatsheet is available at this link. You can see the sheet here: References "],["populations.html", "Chapter 4 Populations 4.1 Notation 4.2 Population vs samples 4.3 A small population 4.4 Visualizing populations 4.5 Comparisons: Same \\(\\sigma\\) 4.6 Comparisons: Different \\(\\sigma\\) 4.7 Comparisons: Repeated Measures 4.8 Comparison Benchmarks 4.9 Population relations 4.10 Relation benchmarks 4.11 Key points 4.12 Minor Points 4.13 Self Assessment", " Chapter 4 Populations 4.1 Notation In this chapter we will use summation notation. If you are not familiar with summation notation, we present a brief overview here. Consider a scenario where we have the IQ data for three participants We use the N symbol to represent the number of participants. Because we have three participants N = 3. The data for these participants is illustrated in Figure 4.1. Notice how each person in the data set can be represented by the variable X: the first person by \\(X_1\\), the second by \\(X_2\\), and the third by \\(X_3\\). Often we refer to individuals in a data set by using the variable X accompanied by a subscript (e.g., 1, 2, 3, etc.). FIGURE 4.1: Data for understanding summation notation Referring to participants using the variable X and subscript is valuable because it can be used in conjunction with the sigma (i.e., \\(\\Sigma\\)) symbol for summation. Consider the example below in which we use the summation notation to indicate that we want to add all the X values (representing IQ) for the participants. We use a lower case \\(i\\) to represent all possible subscript values. The notation, \\(i\\) = 1, below the \\(\\Sigma\\) symbol indicates that we should start with participant 1. The notation, N, above the \\(\\Sigma\\) symbol indicates that we should iterate \\(i\\) up to the value indicated by N; in this case 3, because there are three participants. \\[ \\begin{aligned} \\sum_{i=1}^{N} X_i &amp;= X_1 + X_2 + X_3\\\\ &amp;= 110 + 120 + 100 \\\\ &amp;= 330 \\end{aligned} \\] Sometimes, to simplify the notation, the numbers above and below the \\(\\Sigma\\) symbol are omitted. Likewise, the \\(i\\) subscript is omitted. There is a general understanding that when these components of the notation are omitted the version of the notation above is implied. \\[ \\begin{aligned} \\sum{X} &amp;= X_1 + X_2 + X_3\\\\ &amp;= 110 + 120 + 100\\\\ &amp;= 330 \\end{aligned} \\] Calculating a mean. The full version of the notation can be used to indicate how an average/mean is calculated. \\[ \\begin{aligned} \\bar{X} &amp;= \\frac{\\sum_{i=1}^{N} X_i}{N} \\\\ &amp;= \\frac{X_1 + X_2 + X_3}{3}\\\\ &amp;= \\frac{110 + 120 + 100}{3}\\\\ &amp;= \\frac{330}{3}\\\\ &amp;= 110\\\\ \\end{aligned} \\] Likewise, the concise version of the notation can be used to indicate how an average/mean is calculated. \\[ \\begin{aligned} \\bar{X} &amp;= \\frac{\\sum{X}}{N} \\\\ &amp;= \\frac{X_1 + X_2 + X_3}{3}\\\\ &amp;= \\frac{110 + 120 + 100}{3}\\\\ &amp;= \\frac{330}{3}\\\\ &amp;= 110\\\\ \\end{aligned} \\] Calculating squared differences. A common task in statistics is to calculate 1) the squared difference between each person and the mean, and 2) add up those squared differences. This calculation is easily expressed with the full version of the notation. \\[ \\begin{aligned} \\sum_{i=1}^{N}{(X_i - \\bar{X})^2} &amp;= (X_1-\\bar{X})^2 + (X_2-\\bar{X})^2 + (X_3-\\bar{X})^2\\\\ &amp;= (110-110)^2 + (120-110)^2 + (100-110)^2\\\\ &amp;= (0)^2 + (10)^2 + (-10)^2 \\\\ &amp;= 0 + 100 + 100 \\\\ &amp;= 200 \\end{aligned} \\] Likewise, the sum of the squared differences from the mean can be expressed using the concise version of the notation. \\[ \\begin{aligned} \\sum{(X - \\bar{X})^2} &amp;= (X_1-\\bar{X})^2 + (X_2-\\bar{X})^2 + (X_3-\\bar{X})^2\\\\ &amp;= (110-110)^2 + (120-110)^2 + (100-110)^2\\\\ &amp;= (0)^2 + (10)^2 + (-10)^2 \\\\ &amp;= 0 + 100 + 100 \\\\ &amp;= 200 \\end{aligned} \\] 4.2 Population vs samples As we move closer to conducting our own research it is critical to make a distinction between populations and samples. A population is the complete set of people/animals about which we want to make conclusions. A sample is a randomly selected subset of the population. In most scenarios it is impractical to work with an entire population and, for practical reasons, we study a subset of the population called a sample. Researchers, and consumers of research, typically have little interest in making conclusions at the sample level. In general, we care about conclusions that generalize to the population but not conclusions that only apply to specific individuals in the sample. Consider the case of COVID-19. Imagine a research team creates a vaccine that they hope generates immunity to COVID-19. We care very little if the immunity only works for the specific individuals in the study. However, we care a great deal if the immunity works, or is likely to work, for all Canadians or all humans. We study samples but typically wish to make conclusions that apply to the population. Thus, even if you are an experimental researcher it’s critical that you think in terms of populations and not samples. Indeed, statistical tests (such as the t-test) are a means of helping researchers use sample data to make conclusions at the population level. In this chapter, our focus is on describing populations. When we calculate a number that summarises an attribute of all of the people/animals in the population we refer to it as a parameter. 4.3 A small population In this section we review how to calculate three commonly used population parameters (mean, variance, and standard deviation). Populations are typically quite large but for simplicity we focus on a population composed of the weights of just three chocolate chip cookies. We refer to the three cookies as \\(X_1\\), \\(X_2\\), and \\(X_3\\). The cookies have the weights of 8, 10, and 12 grams, respectively. 4.3.1 Mean (\\(\\mu\\)) It can be helpful to create a model that describes our data. Of course, the model won’t describe every participant perfectly and each participant will differ to some extent from the model. Model: To create a model we first need data, which in this example will be the weight of three different chocolate chip cookies. As mentioned previously, the weights of the three cookies are designated by \\(X_1\\), \\(X_2\\), and \\(X_3\\). A simple model for our cookie weight data is the mean. At the population level the mean is represented by the symbol \\(\\mu\\) see Formula (4.1) below. At the sample level a different notation is used. \\[\\begin{equation} \\mu = \\frac{\\sum{X}}{N} \\tag{4.1} \\end{equation}\\] Using that equation with values: \\[ \\begin{aligned} \\mu &amp;= \\frac{\\sum{X}}{N} \\\\ &amp;= \\frac{X_1 + X_2 + X_3}{3}\\\\ &amp;= \\frac{8 + 10 + 12}{3}\\\\ &amp;= \\frac{30}{3}\\\\ &amp;= 10\\\\ \\end{aligned} \\] We can think of the “mean cookie” as our model for our cookie weight data, see Figure 4.2. The “mean cookie” is represented by \\(\\mu\\) in equations. FIGURE 4.2: Variance as a fit index for the mean Error: As mentioned previously, each participant (i.e., cookie) differs to some extent from our model (“mean cookie”). In general this can be conceptualized as: \\[ \\begin{aligned} X_i &amp;= model + error_i \\\\ \\end{aligned} \\] More specifically, the difference between the weight of any individual cookie (\\(X_i\\)) and the model (\\(\\mu\\)) is indicated by \\(error_i\\) as shown below. \\[ \\begin{aligned} X_i &amp;= \\mu + error_i \\\\ \\end{aligned} \\] The model, above is just a concise way of describing the following: \\[ \\begin{aligned} X_1 &amp;= \\mu + error_1 \\\\ X_2 &amp;= \\mu + error_2 \\\\ X_3 &amp;= \\mu + error_3 \\\\ \\end{aligned} \\] That is the weights of the three cookies (\\(X_1 = 8\\), \\(X_2 = 10\\), and \\(X_3 = 12\\)) can be conceptualized as: \\[ \\begin{aligned} X_1 &amp;= 10 + (-2) \\\\ X_2 &amp;= 10 + 0 \\\\ X_3 &amp;= 10 + 2 \\\\ \\end{aligned} \\] The mean/average of the population, \\(\\mu = 10\\), is a parameter that serves as a model for the cookie weight data. However, it’s helpful to have an index, known as variance, that indicates the extent to which the data do not correspond to the model from the model. 4.3.2 Variance (\\(\\sigma^2\\)) Variance is a simple way of calculating a single number to represent how data differ from a model. It is represented, at the population level, by the symbol \\(\\sigma^2\\); a different notation is used at the sample level. Previously, how we expressed the difference/deviation of cookie weights (data) from the model (i.e., mean) with an error term in the equation \\(X_i = \\mu +error_i\\), see Figure 4.2. The model for all the cookies is \\(\\mu = 10\\). If we consider a single cookie weight of 8 grams (a data point represented by \\(X_1\\)), the difference between the cookie from the model is -2 (i.e., error): \\[ \\begin{aligned} X_1 &amp;= 10 + (-2) \\\\ \\end{aligned} \\] We want a number that indicates the quality of the cookie model. Specifically, we want a single number that indexes overall how the data (i.e., cookie weights) differ from the model (i.e., the mean cookie). We refer to that index as variance (\\(\\sigma^2\\)). Calculating Squared Differences/Errors. To calculate variance (\\(\\sigma^2\\)), we use the errors for the cookies – how the cookies differ from the mean/model. The first step is to square the errors/differences Those squared numbers are referred to as the “squared differences” or “squared errors”. The calculation of the squared error for each cookie weight is shown below. The squared errors (or squared differences) are 4, 0, and 4. Cookie Weight Model Squared Difference/Error \\(X_1 = 8\\) \\(\\mu = 10\\) \\((X_1 - \\mu)^2 =(8 - 10)^2= 4\\) \\(X_2 = 10\\) \\(\\mu = 10\\) \\((X_2 - \\mu)^2 =(10 - 10)^2= 0\\) \\(X_3 = 12\\) \\(\\mu = 10\\) \\((X_3 - \\mu)^2 =(12 - 10)^2= 4\\) Averaging Squared Errors. To obtain variance we calculate the average of the squared errors. At the population level the variance is represented by the symbol \\(\\sigma^2\\) see Formula (4.2) below. In this formula, \\(N\\) refers to the number of people in the population. At the sample level a different notation is used. \\[\\begin{equation} \\sigma^2 = \\frac{\\sum{(X - \\mu)^2}}{N} \\tag{4.2} \\end{equation}\\] Using that equation with values: \\[ \\begin{aligned} \\sigma^2 &amp;= \\frac{\\sum{(X - \\mu)^2}}{N}\\\\ &amp;= \\frac{(X_1-\\mu)^2 + (X_2-\\mu)^2 + (X_3-\\mu)^2}{N} \\\\ &amp;= \\frac{(8-10)^2 + (10-10)^2 + (12-10)^2}{3}\\\\ &amp;= \\frac{(-2)^2 + (0)^2 + (2)^2}{3} \\\\ &amp;= \\frac{4 + 0 + 4}{3} \\\\ &amp;= \\frac{8}{3} \\\\ &amp;= 2.67 grams^2 \\\\ \\end{aligned} \\] The resulting variance is 2.67 grams\\(^2\\). The cookie weights were measured in grams. The unit for variance, however, is grams\\(^2\\) because we squared the errors as part of the calculation. Recall the formula for calculating an average (shown below) and compare it to the variance calculation (above). Notice that variance is just an average – an average of squared errors. Correspondingly, in some areas of statistics they don’t use the term variance, they use a synonym - mean squared error. \\[ \\begin{aligned} \\bar{X} &amp;= \\frac{\\sum{X}}{N} \\\\ \\end{aligned} \\] It probably strikes you as an odd choice to square the difference between each data point and the model. Why not just use the difference (e.g., \\((8 - 10) = -2\\)) when calculating variance? Why not use the absolute difference (e.g., \\(|8 - 10|= 2\\)) when calculating variance? The answer is somewhat complex, but it relates to the more general situation in statistics of trying to find models that best fit the data (which occurs by minimizing errors). When we use squared errors it is easier to apply calculus, via derivatives, to calculate a model that minimizes the errors (i.e., obtains the best fit). Long story short, for complex mathematical reasons, we use squared errors, (rather than just errors) when calculating the fit (or lack of fit) of a model. Interpretation. A variance of zero indicates that the model fits the data perfectly. In the cookie case, if the variance was zero, that would indicate that all the cookies had the same weight as the model, exactly 10 grams. To the extent that the variance is larger than zero it implies the data points (i.e., cookie weights) differ from the model (i.e., the mean cookie). By implication, a larger variance indicates larger differences among the observations (e.g., cookie weights). That is, when the variance is small, cookie weights tend to be similar to the model – and each other. In contrast, when the variance is large, cookie weights tend to be different from the model – and each other. 4.3.3 Standard Deviation (\\(\\sigma\\)) An alternative index for how data differ from the mean/model is the standard deviation. To understand standard deviation you have to understand variance. Variance is a single number that indexes how data differ from a model. The interpretation of variance is straight forward. It is the average of the squared differences/errors between the data and the model. Standard deviation is represented by the symbol \\(\\sigma\\) and can be calculated as the square root of variance as in Formula (4.3) below. \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}} \\tag{4.3} \\end{equation}\\] Using that equation with values: \\[ \\begin{aligned} \\sigma &amp;= \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\\\ &amp;= \\sqrt{\\sigma^2} \\\\ &amp;= \\sqrt{2.67} \\\\ &amp;= 1.63 grams\\\\ \\end{aligned} \\] One reason that people like standard deviation is that it presents the difference between the data and the model in the original units (e.g., grams). This is in contrast to variance which presents the difference betweeen the data and the model in squared units (e.g., 2.67 grams\\(^2\\)). Interpretation. Unfortunately, although variance has a straight forward interpretation, standard deviation does not. Sometimes standard deviation is, incorrectly, described as how much data points differ on average from the mean. A quick calculation of the average difference reveals a number (1.33) that does not correspond to the standard deviation (1.63): \\[ \\begin{aligned} \\overline{diff} &amp;= \\frac{\\sum{|X - \\mu|}}{N}\\\\ &amp;= \\frac{|8-10| + |10-10| + |12 - 10|}{3}\\\\ &amp;= \\frac{2 + 0 + 2}{3}\\\\ &amp;= \\frac{4}{3}\\\\ &amp;= 1.33\\\\ \\end{aligned} \\] As illustrated above, standard deviation is not equal to the average of the deviations from the mean. Because standard deviation is not an average, it’s much harder to describe how to interpret it. In our view, the best way to think of standard deviation is simply as the square root of variance; because variance has a straight forward interpretation. Therefore, we encourage you to think primarily in terms of variance rather than standard deviation due to the fact the interpretation of variance is more straightforward. Additionally, variance is foundational in the language used to describe regression and analysis of variance. That said, standard deviation is used in the calculation of some standardized effect sizes - so it is important to know and understand both indices. Overall, the rules for interpreting standard deviation are similar to those for variance; but the standard deviation values are smaller than variance values. In the cookie case, if the standard deviation was zero, that would indicate that all the cookies had the same weight as the model, exactly 10 grams. To the extent that the standard deviation is larger than zero it implies the data points (i.e., cookie weights) differ from the model (i.e., the mean cookie). By implication, a larger standard deviation indicates larger differences among the observations (e.g., cookie weights). That is, when the standard deviation is small, cookie weights tend to be similar to the model – and each other. In contrast, when the standard deviation is large, cookie weights tend to be different from the model – and each other. 4.4 Visualizing populations Populations are typically quite large in nature and it’s often imposssible to pratically list all of the members of the population. Consequently, it helps to have ways to visual the entire population. In Figure 4.3 we present three ways of visualizing a population. In all three graphs (A, B, C) in this figure the x-axis represents heights in centimeter and the y-axis is used to indicate which values on the x-axis are more common. In Figure 4.3A we use a large number of X’s to indicate the members of the population. Because X’s are also used in formulas to represent individual participants, a strength of this graph is that it reminds you that it is a graph reflecting a large number of individuals. In Figure 4.3B we present a standard histogram that illustrate the distribution of heights. In Figure 4.3C we present a density curve that illustrate the distribution of heights. All three approaches are useful for illustrating that most people have heights around 170 cm. FIGURE 4.3: Three ways of visualizing a population distribution 4.5 Comparisons: Same \\(\\sigma\\) In this section we review a method of comparing two population means when the populations have the same standard deviation. To facilitate comparing two populations we use the heights of males and females, measured in centimeters, as an example. On average males are taller than females. There is, however, variability in the heights of both males and females. The variability in heights is the same for males and females even though the means differ. We illustrate these differences in a series of figures below. Each figure contains three scenarios (labeled A, B, C) in which we manipulate the mean height and variability of the populations. In each scenario the standard deviation of heights is the same for the male and female populations. 4.5.1 Standardized units 4.5.1.1 Individual scores Often when we compare two means we use the original metric. In the case of the male and female heights that metric is centimeters. The original units are a useful way to convey information about the difference between two populations. In addition to the original unit it is also possible, and sometimes desirable, to use the standardized mean difference. The word standardized is used to indicate that the comparison is relative to the standard deviation. Imagine a population of male heights (\\(\\mu = 170\\), \\(\\sigma = 10\\)) from which we have obtained a single individual, Ian, whose height is 185 cm. \\[ X_{Ian} = 185cm \\] The original units are useful for describing Ian’s height but it doesn’t tell us about his height relative to the other people in the population. We have to know the mean and standard deviation of the population to know if Ian is shorter or taller than the average height - and by how much. We use a \\(z\\)-score calculation for this purpose: \\[ \\begin{aligned} z_{Ian} &amp;= \\frac{X_{Ian} - \\mu_{males}}{\\sigma_{males}}\\\\ &amp;= \\frac{185 - 170}{10}\\\\ &amp;= \\frac{15}{10}\\\\ &amp;= 1.50 \\end{aligned} \\] The above calculation is a ratio. Ratios are used to compare two numbers. The numerator (number on the top) is compared to the denominator (number on bottom) through division. The resulting number tells you how much larger the numerator is than the denominator. In this case, the numerator is the extent to which Ian is taller than the mean height for males (\\(X_{Ian} - \\mu_{males}\\)). This numerator is compared to the denominator – which is the standard deviation for males (\\(\\sigma_{males}\\)). The resulting number is 1.50 which indicates the numerator is 1.50 times larger than the denominator. In other words, Ian is 1.50 standard deviations taller than the average male. This is a standardized score for Ian’s height - it expresses the difference between his height and the mean height in standard deviation units. 4.5.1.2 Independent Group: Population Means The same approach to generating standardized scores can be applied to population means. Consider a situation where we have population of male heights (\\(\\mu = 170\\), \\(\\sigma = 10\\)) and a population of female heights (\\(\\mu = 165\\), \\(\\sigma = 10\\)). Notice that both populations have the same standard deviation. We can calculate a standardized value to compare these heights. This standardized value is called the standardized mean difference (SMD). Alternatively, it is also known as Cohen’s \\(d\\) which is represented at the population level with the symbol \\(\\delta\\). Calculation of the standardized mean difference is based on the premise that both populations have the same standard deviation. In this calculation the numerator represents the difference between the two population means. The denominator represents the population standard deviation - which is the same for both populations see Formula (4.4) below. \\[\\begin{equation} \\delta = \\frac{\\mu_{1} - \\mu_{2}}{\\sigma} \\tag{4.4} \\end{equation}\\] Using that equation with values: \\[ \\begin{aligned} \\delta &amp;= \\frac{\\mu_{males} - \\mu_{females}}{\\sigma}\\\\ &amp;= \\frac{170 - 165}{10}\\\\ &amp;= \\frac{5}{10}\\\\ &amp;= 0.50 \\end{aligned} \\] The resulting division of this ratio reveals that the numerator is 0.50 times as large (i.e., half as large) as the denominator. That is, the difference between the populations is half as large as the standard deviation. Therefore, the population mean for males is 0.50 standard deviations larger than the population mean for female; \\(\\delta = 0.50\\): 4.5.2 Cohen’s \\(d\\) units Because the unit for the standardized mean difference is the standard deviation it can be easy to interpret if you are unfamiliar with the original units. It is usually a good idea to report the difference between populations in both the original units (cm) and standardized units (\\(\\delta\\)). Figure 4.4 illustrates three different population difference scenarios (A through C). The population standard deviation is held constant across the three scenarios. You can see that as the difference between the population means increases in raw units - it does the same in \\(\\delta\\) (i.e., Cohen’s \\(d\\)) units. In raw units (i.e., cm), the difference between the population means for scenarios A through C are 5 cm, 10 cm, and 20 cm, respectively. In standardized units (i.e., standard deviations), the difference between the population means for scenarios A through C are 0.50 standard deviations, 1.0 standard deviations, and 2.0 standard deviations, respectively. In other words, the population-level Cohen’s \\(d\\)-values are 0.50, 1.0, and 2.0 for scenarios A through C. FIGURE 4.4: The difference between two population means can be expressed in the original units as indicated by \\(\\Delta M\\). Alternatively, the difference can be expressed using a Standardized Mean Difference (SMD). The SMD index is also known as the population-level \\(d\\)-value and is represented by the symbol \\(\\delta\\). The SMD is a way of expressing the difference between population means without using the original units. 4.5.3 Cohen’s \\(d\\) advantages The standardized mean difference takes into account the variability of heights around each population mean. This means that the same difference between two population means can produce different standardized mean difference values if the population standard deviation varies. In the scenarios depicted in Figure 4.5 the population standard deviation becomes increasing small - resulting in larger standardized mean difference values (i.e., \\(\\delta\\)). This larger \\(\\delta\\) value corresponds to progressively less overlap between the two populations. Thus, taking into account the standard deviation of the populations can be viewed as a strength of using the standardized mean difference. FIGURE 4.5: An advantage of using the Standardized Mean Difference (SMD) to index the difference between two population means (i.e., \\(\\delta\\)) is that it takes the population standard deviation into account. In these three examples, the difference between the populations means is the same using the original/raw units of centimeters. However, the standard deviation of the populations varies across scenarios A, B, and C. The SMD illustrates that these three scenarios are different. If you only examined the difference in the original units (i.e., \\(\\Delta M\\)) you would conclude the effect is the same across the three scenarios. However, by using SMD, indexed by \\(\\delta\\) - the population \\(d\\)-value, you see that the effect is progressively stronger from scenario A, to B, to C. This is illustrated by the fact that there is progressively less overlap between the distributions as you move from scenario A to C. 4.5.4 Cohen’s \\(d\\) caveats It is important to also look at the original units when interpreting results - not just the standardized mean difference. Examine the scenarios in Figure 4.6. Notice how the \\(\\delta\\) value stays constant across scenarios - as does the overlap of the two distributions. However, inspect the shape of the curves and the original units to see how the scenarios vary. Both the original units and the standardized mean difference (i.e., Cohen’s \\(d\\)) provide important interpretational information - don’t rely on just one of them. FIGURE 4.6: The three scenarios in this figure illustrate that a Standardized Mean Difference (i.e., population \\(d\\)-value or \\(\\delta\\)) can remain constant across scenarios when there is a change in the raw difference (i.e., \\(\\Delta M\\)) between the population means. This SMD is consistent across the three scenario despite a change in the mean difference using original units; this occurs because the standard deviations also changes across the three scenarios. 4.6 Comparisons: Different \\(\\sigma\\) The calculation of the standardized mean difference values (\\(\\delta\\)-values) above assumed that the two populations had identical standard deviations. In some scenarios, you might reasonably expect the standard deviations to be different for two populations. When the populations being compared have different standard deviations, we can still calculate a standardized mean difference (i.e, \\(\\delta\\)-value) but we need to pick one of the standard deviations to use in the formula. In many cases, it makes sense to think of one population as the frame of reference for the comparison; a “Control” population of sorts. In this case, you use the standard deviation of the control population as the reference/denominator when calculating the \\(\\delta\\)-value as illustrated in Formula (4.5) below. Make sure you recognize we are talking about a scenario with different population standard deviations (not sample standard deviations) as the appropriate situation for this type of calculation. \\[\\begin{equation} \\delta = \\frac{\\mu_{1} - \\mu_{2}}{\\sigma_{control}} \\tag{4.5} \\end{equation}\\] 4.7 Comparisons: Repeated Measures Sometimes we measures the members of a single population twice and are interested in the change across occasions. Consider a scenario where everyone in a large population (N = 1000000) attempts to loose weight over a given period of time. We weigh everyone in the population at time 1 before the weight loss attempt. Then we weigh everyone in the population at time 2 after the weight loss attempt. Think of this scenario concretely with respect to how we would record this information. Imagine a large spreadsheet with 1000000 rows - each representing a person. There are two columns. The first column contains time 1 weights. The second column contains time 2 weights. We are interested in how weights changed across the times. So we create a third column, called diff, by subtracting time 1 weight from time 2 weights. That is, diff = time 2 weight - time 1 weight. The new diff column indicates how the weights for each person have changed over the diet. We can think of this single column of differences as being a population. This column is used to calculate the repeated-measures \\(\\delta\\)-value. Specifically, the mean of the diff column (\\(bar{x}_{diff}\\)) and the standard deviation of the diff column (\\(s_{diff}\\)) are used in Formula (4.6) below: \\[\\begin{equation} \\delta = \\frac{\\mu_{diff}}{\\sigma_{diff}} \\tag{4.6} \\end{equation}\\] 4.8 Comparison Benchmarks Regardless of how the standardized mean difference (i.e., \\(\\delta\\)-value) is calculated, Cohen suggested that the values of 0.20, 0.50, and 0.80 correspond to the effect size labels of small, medium, and large, respectively (Cohen 1988). These effect sizes are illustrated in Figure 4.7. As described in an interesting blog post, these benchmark values came from reviewing a single issue of the Journal of Abnormal and Social Psychology. Basing benchmarks on such a small number of studies is potentially problematic - as is the fact that at that time all the studies were prone to publication bias. A recent investigation (see Schäfer and Schwarz 2019) of effect sizes in pre-registered studies, with no publication bias, suggests substantially lower benchmark values. FIGURE 4.7: Cohen’s (1988) effect size benchmarks You can visualize any \\(\\delta\\) value (i.e., population \\(d\\)-value) using the rpsychologist website. This website also provides a number of interesting statistics such as the percentage of overlapping values in two populations for a given \\(\\delta\\)/d value. Take a minute to use this website now. Cohen’s benchmarks for standardized mean differences are displayed in the table below. These effect size labels should be interpreted with caution. The magnitude of an effect is best considered in the context of the field of research and the consequences of an effect on individuals in both the short and long run. Cohen (1988) Label Value Small \\(\\delta\\) = .20 Medium \\(\\delta\\) = .50 Large \\(\\delta\\) = .80 4.9 Population relations Often researchers are interested in the extent to which one variable is related to another variable. For example, to what extent does variability in weight relate to variability in height? In other words, is there a relation between weight and height? One approach to this question is to calculate a regression equation relating weight to height - this provides an index of the relation in the original units of the variables. Here we focus on a second approach to describing the relation between variables, namely, the correlation. The correlation is standardized effect size for the linear relation between two variables. A population correlation is represented by the symbol \\(\\rho\\) (pronounced rho) and calculated using Formula (4.7) below. The correlation may range from -1.00 to 1.00. A strong negative correlation indicates that as one variable increases the other decreases. A positive correlation indicates that as one variable increases the other increases. There are at least thirteen ways to conceptualize a correlations (see Lee Rodgers and Nicewander 1988) but it’s easiest to think of it as an index of the extent to which the variables covary in a linear way. \\[\\begin{equation} \\rho = \\frac{\\Sigma (X - \\mu_X)(Y - \\mu_Y)}{\\sqrt{\\Sigma (X - \\mu_X)^2\\Sigma (Y - \\mu_Y)^2}} \\tag{4.7} \\end{equation}\\] Because a correlation only provides an index of a linear relation - it is important to plot the data. Weak correlations (close to zero) may indicate there is not a linear relation. But there may still be a relation between the variables - just not one that follows a straight line. Indeed, the same correlation may take many different forms. Consider the data sets from the datasauRus package presented in Figure 4.8. The graphs for each data set appear quite different. Yet, the following is true for all 12 data sets: The mean of X is 54.3 and the standard deviation is 16.8 The mean of Y is 47.8 and the standard deviation is 26.9 The correlation between X and Y is \\(\\rho\\) = -.06 Therefore, make sure you ALWAYS graph your data. The numbers only tell part of the story. FIGURE 4.8: Various data sets with the same correlation, same means, and same standard deviations 4.10 Relation benchmarks Plots of linear relations are presented in Figure 4.9. The three graphs in this figure corresponds to Cohen’s benchmarks for correlations (i.e., \\(\\rho\\)), displayed in the table below. Cohen (1988) Label Value Small \\(\\rho\\) = .10 Medium \\(\\rho\\) = .30 Large \\(\\rho\\) = .50 These effect size labels should be interpreted with caution. The magnitude of an effect is best considered in the context of the field of research and the consequences of an effect on individuals in both the short and long run. FIGURE 4.9: Populations of various strengths. 4.11 Key points Populations are described using numbers called parameters. Population-level parameters are often represented using Greek letter. Commonly used parameters include mean (\\(\\mu\\) or \\(\\bar{X}\\)), variance (\\(\\sigma^2\\)), or standard deviation (\\(\\sigma\\)). Individual scores can be expressed in standardized units. Population differences can be described in original raw units or standardized units called the standardized mean difference (SMD). SMD is based on the premise that the two populations being compared have the same standard deviation. SMD is a ratio that compares two numbers (the numerator and the denominator). Make sure you understand what is represented by both the numerator and denominator in the SMD ratio. SMD (i.e., Cohen’s d) represents the number of standard deviations between two population means. Recall both populations have the same standard deviation. SMD is indicated at the population level using the Greek letter delta (\\(\\delta\\)). At the sample level we tend to use the term “\\(d\\)-value” or “Cohen’s \\(d\\)”. 4.12 Minor Points Standardized mean differences are reported with a leading zero: \\(\\delta\\) = 0.50. Correlations are reported without a leading zero: \\(\\rho\\) = .30. The difference in reporting is an APA-style issue. Values that are bounded between 0 and 1 (or between -1 and +1) are reported without a leading zero (e.g., \\(\\rho\\) and \\(r\\)). Values that are not bounded are reported with the leading zero (e.g., \\(\\delta\\) and \\(d\\)). 4.13 Self Assessment What is the difference between parameters and statistics? Are the formulas (excluding notation differences) always the same for parameters and statistics? If not, explain why not? Grade 4 students in Ontario are taller than the Grade 3 students. Specifically, \\(\\delta\\) = 0.30. How would you describe what this value means to an audience of experts at a conference? How would you describe what this values means to community members during a talk at the local public library? We reviewed three different ways of calculating a standardized mean different (\\(\\delta\\)). What is the appropriate circumstance to use each formula? Describe a concrete scenario for each one. There is a strong relation between two variables that follows an upside-down U-shape. Would you expect there to be a strong correlation between these two variables? Why or why not? References "],["sampling-accuracy.html", "Chapter 5 Sampling Accuracy 5.1 Overview 5.2 Data for the chapter 5.3 Notation 5.4 Estimating \\(\\mu\\) 5.5 Estimating \\(\\sigma^2\\) 5.6 Estimating \\(\\sigma\\) 5.7 Estimating \\(\\delta\\) 5.8 Estimating \\(\\rho\\) 5.9 Overview 5.10 Meta-analysis 5.11 A joke 5.12 Key Points", " Chapter 5 Sampling Accuracy The following CRAN packages must be installed: Required CRAN Packages tidyverse remotes The following GitHub packages must be installed: Required GitHub Packages dstanley4/learnSampling A GitHub package can be installed using the code below: remotes::install_github(&quot;dstanley4/learnSampling&quot;) Required Data data_cor_pop.csv 5.1 Overview Researchers are usually interested in describing the attributes of a population; numbers that describe the population are called parameters. Two parameters that are frequently of interest are the mean and variance of the population. Unfortunately, it’s rarely possible to obtain information from every member of a population to calculate a parameter. Consequently, researchers use subsets of the population called samples to estimate parameters. Numbers calculated from sample data are called statistics. Typically, sample statistics are used to estimate population parameters. Sample statistics, however, often differ from population parameters. The difference between a sample statistic and the population parameter occurs because the sample data is random subset of the population data — with correspondingly fewer observations. This difference is typically refered to as sampling error. Sometimes a sample statistic will be higher than the population parameter; other times the sample statistic will be lower than the population parameter. Because random sampling is used to select the sample data the direction and magnitude of the difference between the sample statistic and the population parameter will vary randomly. Sample accuracy refers to the extent to which sample statistics correctly estimate the population parameter. We typically used the terms biased and unbiased to describe the accuracy of sample statistics. Consider a scenario where we take many thousands of samples from the same population. For each sample, we calculate a statistic (e.g., the mean). If the average of the sample statistics (e.g., sample means) equals the population parameter (e.g., population mean) then we refer to the statistic as being unbiased. In contrast, if the average of the sample statistics (e.g., sample means) does not equal the population parameter (e.g., population mean) then we refer to the statistic as being biased. Further complicating matters is the fact that the formula used for a sample statistic may, or may not, be the same as the formula used for the corresponding population parameter. This occurs because the purpose of the sample statistic is typically not to describe the sample. Rather the purpose of the sample statistic is to estimate the population parameter. Depending on the parameter, you may or may not be able to use the same formula with sample data as you would with population data. Also keep in mind that, even if you conduct experiments, the distinction between samples and populations is relevant to you. Consider a scenario where you run an experiment to test the effectiveness of a particular drug. Half the rats are assigned to a placebo condition (e.g., saline injection) whereas the other half of the rats are assigned to the drug condition (e.g., drug injection). Recognize that the placebo-condition rats are considered a sample from a much larger population of all rats who could have received the placebo. Likewise, the drug-condition rats are considered a sample of a much large population of all rats who could have received the drug. Indeed, when you conduct your analyses on this experiment the results do not tell you about the rats in your study - rather they tell you about rats in general (i.e., the larger populations of rats). Therefore, when we discuss the importance of estimating a population parameter from a sample realize that it applies to both experimental and survey research. 5.2 Data for the chapter In this chapter we will use a population of heights to learn about random sampling. To engage in the learning activities you need to activate the required packages: library(tidyverse) library(learnSampling) Next, we create a large population with 100,000 people using the get_height_population() command: pop_data &lt;- get_height_population() The print() command can be used to confirm that the population contains 100,000 people. We see that each row in pop_data represents a single person. There is a column called height that contains the heights for everyone in the population. print(pop_data) ## # A tibble: 100,000 × 3 ## id sex height ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male 177 ## 2 2 female 151 ## 3 3 female 171 ## 4 4 male 157 ## 5 5 male 169 ## 6 6 male 187 ## 7 7 female 163 ## 8 8 male 173 ## 9 9 female 172 ## 10 10 male 193 ## # ℹ 99,990 more rows 5.3 Notation In the formulas below, when we refer to the population, we use uppercase letters to indicate members (\\(X\\)) or the size (\\(N\\)). The population mean is indicated by the symbol \\(\\mu\\). In contrast, when we refer to the sample, we use lowercase letters to indicate members (\\(x\\)) or the size (\\(n\\)). The sample mean is indicated by the symbol \\(\\bar{x}\\). A single bar above a letter indicates a mean. If we calculate the average of several sample means we indicate this with the symbol \\(\\bar{\\bar{x}}\\). A double bar above a letter indicates a mean of means. Make sure you notice the similarities between subsequent population and sample formulas even though the notation often differs. 5.4 Estimating \\(\\mu\\) We are interested in the sample mean (\\(\\bar{x}\\)) to the extent that it provides an accurate estimate of the population mean (\\(\\mu\\)). The population mean is calculated using Formula (5.1). In this formula, the letter \\(N\\) corresponds to the number of people in the population. \\[\\begin{equation} \\mu = \\frac{\\sum{X}}{N} \\tag{5.1} \\end{equation}\\] We can calculate the population mean for the height column of pop_data using the summarise() and mean() commands. The mean() command uses Formula (5.1). We see in the output that the population mean is 172.50 (\\(\\mu = 172.50\\)). pop_data %&gt;% summarise(pop_mean = mean(height)) %&gt;% as.data.frame() ## pop_mean ## 1 172.5 As noted previously, we rarely have access to data from an entire population. Consequently, we use the sample mean as an estimate of the population mean. The sample mean, \\(\\bar{x}\\), is a statistic calculated using the using Formula (5.2) below. The bar above the \\(x\\), indicates that it is a mean. Notice that Formula (5.1) and Formula (5.2) are the same - even though they use different notation. In this formula, the letter \\(n\\) corresponds to the number of people in the sample. \\[\\begin{equation} \\bar{x} = \\frac{\\sum{x}}{n} \\tag{5.2} \\end{equation}\\] Because a sample mean (a statistic) is calculated using a random subset of the population it is likely to differ from the population mean (a parameter). If you, inaccurately, believe that you can learn something meaningful from a single study, this fact may be disconcerting. Statisticians know, however, that rarely can you learn anything from a single study, or even a small set of studies. Consequently, they are more interested in the extent to which sample means are right, on average. That is, they are interested in the extent to which the mean of many sample means (\\(\\bar{\\bar{x}}\\)) corresponds to the population mean (\\(\\mu\\)). The mean of many sample means can be calculated using Formula (5.3) below. In this formula, the letter \\(k\\) corresponds to the number of sample means. \\[\\begin{equation} \\bar{\\bar{x}} = \\frac{\\sum{\\bar{x}}}{k} \\tag{5.3} \\end{equation}\\] If the mean of the sample means, \\(\\bar{\\bar{x}}\\), equals the population mean, \\(\\mu\\), then the sample mean is an unbiased (or accurate) estimate of the population mean. Figure 5.1 illustrates the concept of accuracy/bias with a distribution of sample means (i.e., \\(\\bar{x}\\)). Accuracy/bias is an index of the extent to which the mean of many sample means, \\(\\bar{\\bar{x}}\\), deviates from the population mean, \\(\\mu\\). FIGURE 5.1: Sampling accuracy and precision We can assess bias, as illustrated in the above figure by drawing a large number of samples from a population with the code below. Our goal is calculate a mean for each sample so that we have a sampling distribution of means. In theory, we should take an infinite number of samples, however, to be practical we will take 50000 samples to create an approximate sampling distribution of means. We use the code below to do so: many_samples &lt;- get_M_samples(pop.data = pop_data, pop.column.name = height, n = 10, number.of.samples = 50000) many_samples &lt;- readRDS(&quot;ch_samples/many_samples_n10.RDS&quot;) We use the print() command to see the first few rows of the 50000 samples: print(many_samples) ## # A tibble: 50,000 × 5 ## study n sample_mean sample_var_n sample_var_n_1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 174.3 27.81 30.9 ## 2 2 10 161.8 83.358 92.62 ## 3 3 10 173.2 139.158 154.62 ## 4 4 10 180 321.804 357.56 ## 5 5 10 172.8 160.956 178.84 ## 6 6 10 172 159.597 177.33 ## 7 7 10 166.5 86.454 96.06 ## 8 8 10 173.5 139.446 154.94 ## 9 9 10 177.7 167.013 185.57 ## 10 10 10 168.5 262.647 291.83 ## # ℹ 49,990 more rows Each row of many_samples represents a sample of 10 people. Each column of many_samples indicates a sample statistic. You can see that for each sample/row we indicate “n” (the sample size) and “sample_mean” (the mean of the population), and a few other statistics. Even though all the samples came from the same population you can see how the values in the sample_mean column vary across samples/rows. FIGURE 5.2: Sampling distribution of the mean. The population of individuals is presented at the top and filled with X’s to remind you they are individuals. There are more individuals than X’s. The sampling distribution of means is presented in the bottom part of the graph. The sampling distribution of means is filled with \\(\\bar{x}\\)’s to remind you that it is sample means being graphed. There are more means than \\(\\bar{x}\\)’s. The population mean (green line) and the mean of sample means (blue line) are in the same spot, indicating high accuracy (i.e., no bias). Above, in Figure 5.2, we present a graph comparing the distribution of peoples heights (i.e., the population) to the distribution of sample means based on those heights (i.e., the sampling distribution). The sample means plotted are the 50000 sample means, from the sample_mean column. Recall the population mean for heights is \\(\\mu = 172.48\\) cm. Notice that most of the sample means cluster around this value. Also notice that there is considerable variability about this value. Any given sample mean (\\(\\bar{x}\\)) may differ substantially from the population mean (\\(\\mu = 172.48\\)). This variability illustrates the challenges with learning something from a single study - particularly a study with a small sample size. Many of the sample means fall quite far from the population mean. 5.4.1 Assessing bias Statisticians, recognizing the limitations of a single study, are not particularly concerned if a single sample mean deviates from the population mean. That said, statisticians are very concerned as to whether or not the results of a large number of studies are correct – on average. That is, does the average of many sample means correspond to the population mean? If, on average, the sample mean does corresponds to the population mean, it is accurate and we refer to it as an unbiased estimator. Visually, this appears to be the case. But in the code below we confirm it numerically. many_samples %&gt;% summarise(mean_of_sample_mean = mean(sample_mean)) %&gt;% as.data.frame() ## mean_of_sample_mean ## 1 172.5 We find that the average of the 50000 sample means is 172.47 which is very close to the population mean of 172.48. Note that when we did this, we used the same formula to calculate the sample mean (Formula (5.2)) as we did the population mean (Formula (5.1)), although the notations differed. The average of the sample means was not identical to the population mean but it was very close - it would have been exactly the same with many more samples (i.e., an infinite number of samples). Therefore, we conclude the sample mean provides an unbiased estimate of the population mean. In other words, it makes sense to use the sample mean as an estimate of the population mean. If we try to estimate the population mean with a sample mean we will, on average, be correct; although any given sample/study mean might be “wrong”. 5.5 Estimating \\(\\sigma^2\\) We are interested in the sample variance (\\(s^2\\)) to the extent that it provides an estimate of the population variance (\\(\\sigma^2\\)). We begin by reviewing population variance. The population variance is calculated using Formula (5.4): \\[\\begin{equation} \\sigma^2 = \\frac{\\sum{(X - \\mu)^2}}{N} \\tag{5.4} \\end{equation}\\] We can calculate the population variance for the height column of pop_data using the summarise() and var.pop() commands. The var.pop() command uses Formula (5.4). We see in the output that the population variance is 157.5 (\\(\\sigma^2 = 157.5\\)). pop_data %&gt;% summarise(pop_var = var.pop(height)) %&gt;% as.data.frame() ## pop_var ## 1 156.3 5.5.1 Assessing bias The formula for sample variance with an \\(n\\) in the denominator is, unfortunately, a biased estimator of population variance (formula below). \\[ \\begin{aligned} s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{n} \\end{aligned} \\] Estimates of the population variance are systematically too low when you use a sample variance formula with an \\(n\\) in the denominator. We can see that this is true by examining the many_samples data. In these data, the column sample_var_n contains the variance for the sample calculated with the above formula. Below we use code to obtain the average of the sample_var_n column over the 50000 samples. If this average equals the population variance of 157.5 then variance, using \\(n\\) in the denominator, is an unbiased estimator of the population variance. many_samples %&gt;% summarise(mean_of_var_n = mean(sample_var_n)) %&gt;% as.data.frame() ## mean_of_var_n ## 1 141.5 You can see the average of sample_var_n column (141.54) is much smaller than the population variance (157.5). That is, the average of the sample variances, using \\(n\\) in the denominator, was smaller than the population variance. Consequently, sample variance (using \\(n\\) in the denominator) provides a biased estimate of the population variance. If we try to estimate the population variance with sample variance (using \\(n\\) in the denominator) we will, on average, be wrong. Fortunately, there is a sample-level formula that estimates the population variance without bias (see Hayes). An unbiased estimate of the population variance can be obtained if we calculate the sample variance but divide by \\(n - 1\\) instead of \\(n\\). The unbiased estimate is calculated using Formula (5.5). The denominator of the formula below indicates the degrees of freedom associated with the variance estimate. More specifically, in this formula there are \\(n-1\\) degrees of freedom associated with the variance estimate. \\[\\begin{equation} s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{n-1} \\tag{5.5} \\end{equation}\\] In the many_samples data, the column sample_var_n_1 was generated using Formula (5.5). We can evaluate the quality of Formula (5.5), using \\(n-1\\), by averaging the values in the sample_var_n_1 column. many_samples %&gt;% summarise(mean_of_var_n_1 = mean(sample_var_n_1)) %&gt;% as.data.frame() ## mean_of_var_n_1 ## 1 157.3 We see that the average of the 50000 values using \\(n-1\\) in the denominator is 157.27 which is very close to the population variance of 157.46. These numbers would have been identical with an infinite number of samples. Consequently, when we use \\(n-1\\) in the denominator we have an unbiased estimate of the population variance. If we try to estimate the population variance with a sample variance, using \\(n-1\\) in the denominator, we will, on average, be right. You may wonder at this point, when we use \\(n-1\\) in the denominator of the sample variance, can we still think of it as the average of the squared differences from the mean? The short answer is yes. When you use \\(n-1\\) in the denominator of the sample variance you are not calculating the variance for the group people in the sample. Rather, you are estimating the variance for the much larger group of people in the population. Consequently, it makes sense to think of sample variance, using \\(n-1\\), as an estimate of the average of the squared differences/errors in the population. That is, it makes sense to think of sample variance, using \\(n-1\\), as an estimate of the average of the squared differences between each person in the population and the population mean. 5.6 Estimating \\(\\sigma\\) The population standard deviation is calculated using Formula (5.6) below. \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X- \\mu)^2}}{N}} \\tag{5.6} \\end{equation}\\] Due to the above findings for variance, we estimate the population standard deviation using Formula (5.7) below. \\[\\begin{equation} s = \\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n-1}} \\tag{5.7} \\end{equation}\\] 5.7 Estimating \\(\\delta\\) We are interested in the sample standardized mean difference (\\(d\\)) to the extent that it provides an estimate of the population standardized mean difference (\\(\\delta\\)). The population standardized mean difference is calculated using Formula (5.8) when we work for the assumption that the two population have the same variance / standard deviation: \\[\\begin{equation} \\delta = \\frac{\\mu_{1} - \\mu_{2}}{\\sigma} \\tag{5.8} \\end{equation}\\] We can calculate the population standardized mean difference for men and women once we have the respective population means and standard deviations. Recall the initial data mixed males and females. We begin by creating separate data sets for males and females: male_population_heights &lt;- pop_data %&gt;% filter(sex == &quot;male&quot;) female_population_heights &lt;- pop_data %&gt;% filter(sex == &quot;female&quot;) Next, we calculate the mean and standard deviation of each population: male_population_heights %&gt;% summarise(mean = mean(height), sd = sd.pop(height)) ## # A tibble: 1 × 2 ## mean sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 180.000 10.0036 female_population_heights %&gt;% summarise(mean = mean(height), sd = sd.pop(height)) ## # A tibble: 1 × 2 ## mean sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 165.000 10.0036 This reveals the population parameters are: \\[ \\begin{aligned} \\mu_{female} &amp;= 165 \\\\ \\mu_{male} &amp;= 180 \\\\ \\sigma = \\sigma_{female} = \\sigma_{male} &amp;= 10.1\\\\ \\end{aligned} \\] Likewise, as calculated below, the population-level standardized mean difference (\\(\\delta\\)) is 1.49. We can see this population-level difference illustrated in Figure 5.3. \\[ \\begin{aligned} \\delta &amp;= \\frac{\\mu_{male} - \\mu_{female}}{\\sigma} \\\\ &amp;= \\frac{180 - 165}{10.1} \\\\ &amp;= \\frac{15}{10.1} \\\\ &amp;= 1.49 \\\\ \\end{aligned} \\] FIGURE 5.3: Illustration of the standardized mean difference of 1.49 for male and female heights. The solid black vertical line indicates the mean for males; whereas the dotted vertical line indicates the mean for females. We typically need to estimate the population-level standardized mean difference from sample data because we rarely have access to data for an entire population. Many researchers estimate the population standardized mean difference from sample data using the Formula (5.9) below – when we assume the populations have equal variances. This value is known by many other names: \\(d\\), Cohen’s \\(d\\), and Hedges’ \\(g\\). Notice that the sample-level formula, Formula (5.9), below, is the same as the population-level formula, Formula (5.8), above, only the notation differs. \\[\\begin{equation} d = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{pooled}} \\tag{5.9} \\end{equation}\\] Unfortunately, Formula (5.9) provides a biased estimate of the population standardized mean difference for small sample sizes. That is, on average, Formula (5.9), provides \\(d\\)-values that overestimate the size of the population standardize mean difference (\\(\\delta\\)). Fortunately, we can obtain an unbiased estimate of the population-level standardized mean difference from sample data using Formula (5.10). This is one approach to calculating \\(d_{unbiased}\\) – there are others. \\[\\begin{equation} d_{unbiased} = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{pooled}} \\times [1 - \\frac{3}{4(n_1 + n_2)-9}] \\tag{5.10} \\end{equation}\\] If we try to apply either \\(d\\)-value formula ((5.9) or (5.10)) to real data we quickly encounter a problem. We don’t have the pooled standard deviation, \\(s_{pooled}\\) 5.7.1 Pooled standard deviation When we calculated the population-level standardized mean difference we knew the population variances where the same. Consequently, there was only one standard deviation (i.e., only one variance). More specifically, the male and female populations both had a standard deviation but it was the same for both populations. The population-level formula for the standardized mean difference, Formula (5.8), has only one standard deviation in it. This is because calculation of the standardized mean difference explicitly depends on the fact that both populations have the same standard deviation. Let’s consider hypothetical sample data to make the situation clear. More specifically, we will examine the sample-level statistics below for males and females. Notice that we have two standard deviations – one for males and one for females. Moreover, these two sample-level standard deviations (using \\(n\\)-1) are not the same - they are different from each other. This initially seems problematic - calculation of standardized mean difference requires that population standard deviations are identical. \\[ \\begin{aligned} \\bar{x}_{males} &amp;= 187.2 \\\\ s_{males}^2 &amp;= 92.2 \\\\ s_{males} &amp;= 9.6 \\\\ \\end{aligned} \\] And females: \\[ \\begin{aligned} \\bar{x}_{females} &amp;= 160.1 \\\\ s_{females}^2 &amp;= 66.8 \\\\ s_{females} &amp;= 8.2 \\\\ \\end{aligned} \\] Fortunately, this is sample-level data and not population-level data. Sample-level standard deviations may differ even when the population-level standard deviations are the same. In fact, sample-level standard deviations are likely to differ from the population-level standard deviation due to sampling error. Consequently, we are likely to get two different sample-level standard deviations even if the population-level standard deviations are identical for males and females. How do we resolve this situation of having two sample-level standard deviations? The first step is to switch to thinking in terms of variance rather than standard deviation. Due to the way the math works, life becomes very complicated, very quickly, if we continue to think in terms of standard deviations. Therefore, we reframe the problem into a variance problem. Variances are preferable to standard deviations because we can add and subtract variances - but not standard deviations. We have a sample variance for males (92.2) and a sample variance for females (66.8). We view each of these sample variances as an estimate of the respective population variances (see Figure 5.4). That is, the male sample variance is an estimate of the male population variance. Likewise, the female sample variance is an estimate of the female population variance. However, we also assume that the population variances for males and females are the same. Consequently, the male sample variance and the female sample variance are both estimates of the same value (see Figure 5.5). Because the two sample variances are estimates of the same population variance, we can (when the sample sizes are equal) calculate a new variance by averaging them together. This new variance, the average of the sample variances, provides us with a better estimate of the single population variance. The logic behind this approach is similar to averaging two measurements of the same distance to reduce error. We call this new variance pooled variance; and represent it with the symbol \\(s_{pooled}^2\\). FIGURE 5.4: Estimating population variances with sample variances. The male sample variance (n-1) is an estimate of male population variance. Likewise, the female sample variance (n-1) is an estimate of the female population variance. FIGURE 5.5: Two estimates of a single population variance. We assume the population variances are the same. Therefore, the male and female sample variances are both estimates of the same population variance. FIGURE 5.6: Pooled variance from the samples estimates population variance. The population variance is estimated by averaging two sample variances into a single estimate called pooled variance (\\(s_{pooled}^2\\)). When sample sizes are equal, the pooled variance is just the regular/simple average of the two sample variances (both using n-1 in the denominator). When the sample sizes are unequal (i.e., different numbers of males and females), however, we need to use a more sophisticated averaging formula to obtain the pooled variance. When the sample sizes for males and females are the same (i.e., \\(n_{males} =n_{females}\\)) we can use the Formula (5.11) below to calculate the pooled variance. \\[\\begin{equation} s_{pooled}^2 = \\frac{s_{1}^2 + s_{2}^2}{2} \\tag{5.11} \\end{equation}\\] When the sample sizes for males and females are different (i.e., \\(n_{males} \\ne n_{females}\\)) we can use the Formula (5.12) below to calculate the pooled variance. This formula can be used all of the time. We only show Formula (5.11), above, to make it clear that Formula (5.12) below is basically just averaging the variances in a way that takes sample size into account. \\[\\begin{equation} s_{pooled}^2 = \\frac{(n_1 -1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2-2} \\tag{5.12} \\end{equation}\\] We get the single standard deviation, \\(s_{pooled}\\), by taking the square root of the variance, \\(s_{pooled}^2\\). \\[ \\begin{aligned} s_{pooled} &amp;= \\sqrt{s_{pooled}^2} \\\\ \\end{aligned} \\] We apply the pooled standard variance, Formula (5.12), to the sample data: \\[ \\begin{aligned} s_{pooled}^2 &amp;= \\frac{(n_{male} -1)s_{male}^2 + (n_{female} -1)s_{female}^2}{n_{male} + n_{female}-2} \\\\ &amp;= \\frac{(10 -1)92.2 + (10 -1)66.8}{10 + 10 -2} \\\\ &amp;= 79.5 \\end{aligned} \\] Then we obtain the pooled standard deviation, below, for the standardized mean difference formula. \\[ \\begin{aligned} s_{pooled} &amp;= \\sqrt{79.5} \\\\ &amp;= 8.9\\\\ \\end{aligned} \\] AN ASIDE ON POOLING. Variance pooling is an extraordinarily important part of statistics - we see it used in many topics. To prepare you for that future learning, examine the more general version of the variance pooling equation below. Go through each line of the math below to see how the pooling formula used above is only a specific case of the more general formula below. No really do it - don’t skip this task! Actually take a minute to understand the general version of the pooling formula below. You’ll thank me in future weeks. This is foundational knowledge. In the math below we use a = 2 to indicate there are two sample variances being pooled. The denominator of the formula below indicates the degrees of freedom associated with the variance estimate. More specifically, in this formula there are \\((n_1-1)+(n_2-1)\\) (or \\(a(n-1)\\)) degrees of freedom associated with the variance estimate. \\[ \\begin{aligned} s_{pooled}^2 &amp;= \\frac{\\sum_{i=1}^{a}(n_i-1)(s_i^2)}{\\sum_{i=1}^{a}(n_i-1)} \\\\ &amp;= \\frac{(n_1-1)s_1^2 + (n_2 -1)s_2^2}{(n_1-1)+(n_2-1)} \\\\ &amp;= \\frac{(n_1-1)s_1^2 + (n_2 -1)s_2^2}{(n_1+n_2-2)} \\\\ &amp;= \\frac{(n_{male} -1)s_{male}^2 + (n_{female} -1)s_{female}^2}{n_{male} + n_{female}-2} \\\\ \\end{aligned}\\\\ \\] Also note the simplification of the general formula below when the sample sizes (\\(n_i\\)) are all the same. In particular notice how we express the denominator when the sample sizes are the same. This will become relevant when we get to ANOVA. Recall a = 2 to indicate there are two sample variances. \\[ \\begin{aligned} s_{pooled}^2 &amp;= \\frac{\\sum_{i=1}^{a}(n_i-1)(s_i^2)}{\\sum_{i=1}^{a}(n_i-1)} \\\\ &amp;= \\frac{(n-1)s_1^2 + (n-1)s_2^2}{a(n-1)} \\\\ \\end{aligned}\\\\ \\] 5.7.2 Calculating \\(d\\) Recall that in the above we calculated the pooled standard deviation, \\(s_{pooled} = 8.9\\). Using this value we can calculate the standardized mean difference. We do so below using unbiased formula, Formula (5.10) below. \\[ \\begin{aligned} d_{unbiased} &amp;= d \\times [1 - \\frac{3}{4(n_{males} + n_{females})-9}] \\\\ &amp;= \\frac{\\bar{x}_{males} - \\bar{x}_{females}}{s_{pooled}} \\times [1 - \\frac{3}{4(n_{males} + n_{females})-9}] \\\\ &amp;= \\frac{187.2 - 160.1}{8.9} \\times [1 - \\frac{3}{4(10 + 10)-9}] \\\\ &amp;= 3.0 \\times 0.96\\\\ &amp;= 2.9\\\\ \\end{aligned} \\] 5.7.3 Assessing bias Sample-level \\(d_{unbiased}\\)-values, calculated above, often differ from the population-level standardized mean difference (i.e., \\(\\delta\\)) due to sampling error. We can confirm that \\(d_{unbiased}\\)-values are actually unbiased with a simulation. That is, we can confirm that the average of many \\(d_{unbiased}\\)-values equals the population standardized mean difference (i.e., \\(\\delta\\)) using a simulation. First, we obtain the heights from the male and female populations and place them into male_heights and female_heights, respectively. male_heights &lt;- male_population_heights %&gt;% pull(height) female_heights &lt;- female_population_heights %&gt;% pull(height) Next, we obtain a large number of samples from each population and place them in many_samples. many_samples&lt;- get_d_samples_from_population_data(pop1 = male_heights, pop2 = female_heights, cell.n = 10, number.of.samples = 50000) We can examine the contents of many_samples using the print() command. Each row of many_samples represents a single study. Each study has two samples: 10 males and 10 females. For both males and females we calculate the mean and variance. As well, we calculate the \\(d\\) and \\(d_{unbiased}\\) for each row. If you examine the first row carefully you see that the data in this row corresponds to the hand calculation example. print(many_samples) ## # A tibble: 50,000 × 7 ## n_per_cell mean1 var1_n_1 mean2 var2_n_1 d d_unbiased ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 187.2 92.18 160.1 66.77 3.04 2.91 ## 2 10 183 107.33 166 77.78 1.77 1.69 ## 3 10 181 39.78 168.8 144.18 1.27 1.22 ## 4 10 175.4 80.27 162.5 89.39 1.4 1.34 ## 5 10 184.1 90.99 163.6 28.93 2.65 2.54 ## 6 10 181.8 96.4 165.6 106.27 1.61 1.54 ## 7 10 176.6 97.16 167 161.56 0.84 0.81 ## 8 10 188 211.56 160.8 204.62 1.89 1.81 ## 9 10 185.3 54.68 165.8 91.51 2.28 2.18 ## 10 10 179.3 69.34 161.9 55.43 2.2 2.11 ## # ℹ 49,990 more rows Recall the population-level standardized mean difference, \\(\\delta\\), was 1.49. We can see the extent to which the average of the sample-level \\(d\\) and \\(d_{unbiased}\\) values compare to this population-level value. many_samples %&gt;% summarise(mean_d = mean(d), mean_d_unbiased = mean(d_unbiased)) ## # A tibble: 1 × 2 ## mean_d mean_d_unbiased ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.55696 1.49119 You can see that the mean of the sample-level \\(d\\) values is 1.56 which is higher than the population-level standardized mean difference (\\(\\delta\\) = 1.49). In contrast, you can see that the mean of the sample-level \\(d_{unbiased}\\) values is 1.49 which corresponds to the population-level standardized mean difference (\\(\\delta\\) = 1.49). 5.7.4 Illustrating variability An inspection of the first few rows of the many_samples data, above, illustrates that many of the \\(d_{unbiased}\\) values differed from the population-level standardized mean difference of \\(\\delta = 1.49\\). We can see the variability in sample-level \\(d_{unbiased}\\) values in the histogram below. FIGURE 5.7: Histogram of \\(d_{unbiased}\\) when \\(\\delta = 1.49\\) We can calculate the full range of sample-level \\(d_{unbiased}\\) values with the commands below: many_samples %&gt;% summarise(d_min = min(d_unbiased), d_max = max(d_unbiased)) ## # A tibble: 1 × 2 ## d_min d_max ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.38 5.15 We see from the output that \\(d_{unbiased}\\) values were as small as -0.38 and as large as 5.15. All of these values are estimates of the population-level standardized mean difference of \\(\\delta = 1.49\\). You can see that many of the sample-level estimates differed considerably from the population-level value. The negative \\(d\\)-value (i.e., the minimum) is a case where the study (i.e, sample-level result) would have found that women are taller than men - a reversal of what is actually true at the population-level. This range of results illustrates the extent to which the findings for a single sample/study may deviate from the underlying truth for the entire population. We see that when the sample size is small (n = 10 per group) that the study results are likely to differ extraordinarily from what is true at the population level. This suggests that the “old school” suggestion of 10 participants per group when conducting a study leads to findings with little informational value. 5.8 Estimating \\(\\rho\\) The population-level correlation, \\(\\rho\\), is estimated by the sample-level correlation, \\(r\\). The value for \\(r\\) can be calculated using Formula (5.13) below. \\[\\begin{equation} r = \\frac{\\Sigma (x - \\bar{x})(y - \\bar{y})}{\\sqrt{\\Sigma (x - \\bar{x})^2\\Sigma (y - \\bar{y})^2}} \\tag{5.13} \\end{equation}\\] Sample-level correlations, \\(r\\), often differ from the population-level correlation (\\(\\rho\\)) due to sampling error. We can confirm that sample correlations are not substantially biased with a simulation. That is, we can confirm that the average of many sample correlations (\\(r\\)) roughly equals the population correlation (\\(\\rho\\)) using a simulation. We say “roughly equal” because \\(r\\) is technically a biased estimator of \\(\\rho\\) but the bias is sufficiently small that it can be ignored (Schmidt and Hunter 2014). We have the height and weight for 300000 people that comprise our population (fictious data). This data can be loaded with the command below. The data can be downloaded here: data_cor_pop.csv library(tidyverse) pop_data &lt;- read_csv(file = &quot;data_cor_pop.csv&quot;) ## Rows: 300000 Columns: 2 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): weight, height ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The print() command reveals there are 300000 rows and two columns (weight and height). Each row represents a different person in the population. print(pop_data) ## # A tibble: 300,000 × 2 ## weight height ## &lt;dbl&gt; &lt;dbl&gt; ## 1 143.2 163.1 ## 2 155.8 171.4 ## 3 143.8 160.4 ## 4 152 189.9 ## 5 154.8 173.4 ## 6 147.1 160.1 ## 7 145.1 176.9 ## 8 152.9 178.9 ## 9 150.8 177.2 ## 10 146.5 166.7 ## # ℹ 299,990 more rows We can obtain the population correlation, \\(\\rho\\), by correlating the weight and height columns: cor(pop_data) ## weight height ## weight 1.0 0.5 ## height 0.5 1.0 We see from this matrix that the population correlation (N = 300000) between weight and height is \\(\\rho\\) = .50 (with rounding), see Figure 5.8A. To examine the extent to which this population correlation, \\(\\rho\\) = .50, is estimated by the sample statistic, \\(r\\), we need to take a large number of samples. Therefore, we take 50000 samples (each n = 75) and calculate the correlation for each: set.seed(1) many_samples &lt;- get_r_samples_from_population_data(data = pop_data, n = 75, number.of.samples = 50000) We can examine the first few rows of many_samples using the print() command. There are 50000 rows and each row represent a different sample of 75 people. The correlation between height and weight for each sample is presented in the \\(r\\) column. print(many_samples) ## # A tibble: 50,000 × 3 ## sample.number n r ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 75 0.38 ## 2 2 75 0.5 ## 3 3 75 0.56 ## 4 4 75 0.52 ## 5 5 75 0.49 ## 6 6 75 0.37 ## 7 7 75 0.57 ## 8 8 75 0.4 ## 9 9 75 0.59 ## 10 10 75 0.5 ## # ℹ 49,990 more rows If you examine the first row carefully you see a sample correlation of \\(r = .38\\) based on \\(n\\) = 75. This sample correlation is illustrated in Figure 5.8B and it is just one of the 50000 sample correlations. The distribution of the 50000 sample correlations is illustrated in in Figure 5.8C. Even though the population correlation is \\(\\rho = .50\\) there is considerable variability in the sample correlations, \\(r\\). Each sample correlation is based on a subset of the population data (i.e., 75 of the 300000 rows). Consequently, the sample correlations differ from the population correlation due to sampling error. The differences among the sample correlations can be quite large. Indeed, as the code below reveals, some sample correlation were as low as \\(r = .06\\) and as high as \\(r = .77\\) – even though the population correlation was \\(\\rho = .50\\). many_samples %&gt;% summarise(min_r = min(r), max_r = max(r)) ## # A tibble: 1 × 2 ## min_r max_r ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.06 0.77 5.8.1 Assessing bias Even though the sample correlations usually differed from the population correlation we are not concerned. We recognize that there is little to be learned from a single study. We are more concerned as to whether the average of a large number of sample correlations is correct. We assess this with the code below. many_samples %&gt;% summarise(mean_r = mean(r)) ## # A tibble: 1 × 1 ## mean_r ## &lt;dbl&gt; ## 1 0.497233 You can see that the mean of the sample-level correlations is \\(\\bar{r} = .497\\) which is very close to the population-level correlation \\(\\rho = .50\\) (.4999951 without rounding). Consequently, for practical purposes, we don’t worry about the sample correlation being a biased estimator of the population correlation. On average, sample correlation are correct - even though any single sample correlation is likely incorrect due to sampling error (i.e., the fact it is based on a small subset of the population). FIGURE 5.8: Correlation sampling distribution 5.9 Overview In this chapter we have illustrated how population parameters can be estimated by sample statistics; these are summarized below: Parameter Estimated by this statistic Mean \\(\\mu = \\frac{\\sum{X}}{N}\\) \\(\\bar{x} = \\frac{\\sum{x}}{n}\\) Variance \\(\\sigma^2 = \\frac{\\sum{(X - \\mu)^2}}{N}\\) \\(s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{n-1}\\) \\(s_{pooled}^2 = \\frac{(n_1 -1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2-2}\\) Standard deviation \\(\\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\) \\(s =\\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n-1}}\\) \\(s_{pooled} = \\sqrt{\\frac{(n_1 -1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2-2}}\\) Cohen’s \\(d\\) or SMD \\(\\delta= \\frac{\\mu_{1} - \\mu_{2}}{\\sigma}\\) \\(d = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{pooled}}\\) \\(d_{unbiased} = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{pooled}} \\times [1 - \\frac{3}{4(n_1 + n_2)-9}]\\) Correlation \\(\\rho = \\frac{\\Sigma (X - \\mu_X)(Y - \\mu_Y)}{\\sqrt{\\Sigma (X - \\mu_X)^2\\Sigma (Y - \\mu_Y)^2}}\\) \\(r = \\frac{\\Sigma (x - \\bar{x})(y - \\bar{y})}{\\sqrt{\\Sigma (x - \\bar{x})^2\\Sigma (y - \\bar{y})^2}}\\) 5.10 Meta-analysis It may seem odd that we used so many simulations to investigate the properties of statistics. Surely, researchers don’t do that “in the real world”. In fact, researchers who are aware of the enormous impact of sampling error know that single studies have little informational value. They recognize that any single study has a high probability of being misleading. Consequently, these individuals survey the literature and find all the studies on a single topic (possibly thousands of studies). An average of the results of all of the thousands of studies can then be calculated and reported. This process is referred to as conducting a meta-analysis; and it perfectly corresponds to the process we used in the simulations. A meta-analysis finds “the truth” of what is happening at the population level by averaging all of the studies on that topic. 5.11 A joke Now that you understand the logic for assessing bias, we present an old statistics joke. “A physicist, a chemist, and a statistician go hunting. The physicist shoots at a deer and misses by 2 meters to the left. The chemist shoots and misses by 2 meters to the right. The statistician immediately yells”We got it!” 5.12 Key Points Samples are of interest because they help us estimate attributes of a population. Sample statistics estimate population parameters. Due to the fact that sample statistics are based on a random subset of the population (i.e., a sample) they often differ substantially from the population parameter. This illustrates that informational value of a single study is typically quite low. A statistic is unbiased if the average of the sample statistics, over many thousand of samples, equals the population parameter. To avoid bias, sometimes the formula for a sample statistics differs from the formula for the population parameter. Meta-analyses are used in the “real world” the way we used simulations in this chapter. References "],["sampling-precision.html", "Chapter 6 Sampling Precision 6.1 Overview 6.2 Population / Individuals 6.3 Sampling distribution 6.4 Precision indices 6.5 A short cut 6.6 Estimates of precision 6.7 Bias of precision estimates 6.8 Where are we? 6.9 Precision for means: Causes 6.10 Precision for \\(d\\)-values: Causes 6.11 Precision for sample correlations (\\(r\\)): Causes", " Chapter 6 Sampling Precision 6.1 Overview In the previous chapter we focused on examining the extent to which we could estimate a population parameter from a statistic in an unbiased manner. That is, we focused on the fact that, due to sampling error, a sample estimate (i.e., a statistic) of a population parameter will likely be wrong. Sample statistics may overestimate, or underestimate, a population parameter - often substantially. Consequently, we learned to think about the accuracy of sample statistics. More specifically, we learned to think about accuracy not in terms of a single study, but rather in terms of whether sample statistics correctly estimate the population parameter on average over many studies (as illustrated in Figure 6.1). When the average of many sample statistics (e.g., sample means) equals the population parameter (e.g., population mean) there is no bias and the sample statistic can be considered accurate (on average). In this chapter, we assume there is no bias and that sample means are, on average, accurate. In doing so, we focus now on the precision of sample means as estimates of the population mean, see Figure 6.1. That is, assuming no bias, when you conduct a single study you know the sample mean will differ from the population mean due to sampling error. You might wonder, how much does my sample mean differ from the population mean? That’s the question we address in this chapter. When we conduct a single study we can’t know how much that particular sample mean differs from the population mean, but we can estimate the extent to which sample means differ from the population mean in general. We focus on two indices of precision – variance of sample mean and standard error of sample means. By the end of the chapter you will understand how to a) conceptualize these indices with respect to a large number of samples/studies and b) estimate them for a single study. FIGURE 6.1: Sampling accuracy and precision 6.2 Population / Individuals We use a population of heights to learn about random sampling and the precision of sample estimates. We can obtain that population with the code below: library(tidyverse) library(learnSampling) pop_data &lt;- get_height_population() The glimpse() command can be used to confirm that the population contains 100,000 people. glimpse(pop_data) ## Rows: 100,000 ## Columns: 3 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, … ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;,… ## $ height &lt;dbl&gt; 177, 151, 171, 157, 169, 187, 163, 173, 172… We can use the head() command to see the first 10 rows of the 100,00 rows. We see that each row in pop_data represents a single person. There is a column called height that contains the height for everyone in the population. head(pop_data, 10) ## # A tibble: 10 × 3 ## id sex height ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male 177 ## 2 2 female 151 ## 3 3 female 171 ## 4 4 male 157 ## 5 5 male 169 ## 6 6 male 187 ## 7 7 female 163 ## 8 8 male 173 ## 9 9 female 172 ## 10 10 male 193 We can calculate the descriptive parameters for the population with the code below: pop_data %&gt;% summarise(pop_mean = mean(height), pop_var = var.pop(height), pop_sd = sd.pop(height)) %&gt;% as.data.frame() ## pop_mean pop_var pop_sd ## 1 172.5 156.3 12.5 We see the mean of the population of heights is 172.5 (\\(\\mu = 172.5\\)) and the variance is 157.5 (\\(\\sigma^2 = 157.5\\)). Correspondingly, the standard deviation of the population of heights is 12.5 (\\(\\sigma = 12.5\\)); simply the square root of the variance. \\[ \\begin{aligned} \\mu &amp;= 172.5 \\\\ \\sigma^2 &amp;= 157.5 \\\\ \\sigma &amp;= 12.5 \\\\ \\end{aligned} \\] 6.3 Sampling distribution A sampling distribution is composed of an infinite number of samples. For pedagogical purposes, we will use a sampling distribution of just 50000 samples. This large number of samples will lead us to roughly the same conclusions as using an infinite, but impractical, number of samples. We obtain 50000 samples with code below: many_samples &lt;- get_M_samples(pop.data = pop_data, pop.column.name = height, n = 10, number.of.samples = 50000) We can use the head() command to see the first few rows of many_samples. Every row represents a sample of 10 people. That is, each row represents a study with 10 participants for which we measured their heights. For each row/study we have the sample mean and variance (using n-1) for the 10 heights. head(many_samples) ## # A tibble: 6 × 5 ## study n sample_mean sample_var_n sample_var_n_1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 174.3 27.81 30.9 ## 2 2 10 161.8 83.358 92.62 ## 3 3 10 173.2 139.158 154.62 ## 4 4 10 180 321.804 357.56 ## 5 5 10 172.8 160.956 178.84 ## 6 6 10 172 159.597 177.33 We will study the precision of sample means as estimates of the population mean using the values in the sample_mean column. Each of the 50000 values in the sample_mean column is a sample mean (\\(\\bar{x}\\)), based on \\(n\\) = 10. Each sample mean provides an estimate of the population mean (\\(\\mu = 172.5\\)). Sample means often overestimate, or underestimate, the population mean. We can see this using the code below which reveals that the smallest sample mean is 155.6 (\\(\\bar{x} = 155.6\\)) and the largest sample mean is 188.8 (\\(\\bar{x} = 188.8\\)). Both are estimates of the population mean (\\(\\mu = 172.5\\)) but differ from that value due to sampling error (i.e., the fact that sample means are based on a small subset of the population). You can see that the conclusions of any single study may differ rather substantially from what is true for the overall population. many_samples %&gt;% summarise(min_mean = min(sample_mean), max_mean = max(sample_mean)) ## # A tibble: 1 × 2 ## min_mean max_mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 155.6 188.8 The 50000 means in the sample_mean column form a sampling distribution; specifically, the sampling distribution of the mean. It’s critical to distinguish between a) the distribution of the sample means and b) the distribution of population heights. Examine Figure 6.2 below. The top part of this figure illustrates the variability in the heights for the 100,000 people (represented by X’s). There are more people than X’s; the X’s are a reminder it is a distribution of people’s heights. The bottom part of this figure illustrates the variability in sample means - each based on 10 people. Each sample mean is an estimate of the mean height of the population (172.5 cm). There are more sample means than \\(\\bar{x}\\)’s; the \\(\\bar{x}\\)’s are a reminder it is a distribution of sample means. When we describe how precisely sample means estimate the mean height for the population we are referring to the width of this distribution of means. FIGURE 6.2: Sampling distribution of the mean. The population of individual height’s is presented at the top and filled with X’s to remind you they are individuals. There are more individuals than X’s. The sampling distribution of means is presented in the bottom part of the graph. The sampling distribution of means is filled with \\(\\bar{x}\\)’s to remind you that sample means are being graphed. There are more means than \\(\\bar{x}\\)’s. The population mean (green line) and the mean of sample means (blue line) are in the same spot, indicating high accuracy (i.e., no bias). 6.4 Precision indices The most common way of referring to the width of this distribution of sample means is with the term standard error. Standard error is simply the standard deviation of the sample means. We calculate the standard error of the mean using Formula (6.1) below. We use \\(K\\) in the formula to refer to the number of sample means in the distribution (i.e., \\(K = \\infty\\)). Note that it doesn’t really make sense to divide by infinity (i.e., \\(\\infty\\)) so statistician’s don’t normally express the standard deviation of this distribution using the formula below - instead they talk about the expected value for the standard deviation. But for learning purposes - we present the formula below as means to bridge this learning with past learning. \\[\\begin{equation} \\sigma_{\\bar{x}} = \\sqrt{\\frac{\\sum{(\\bar{x} - \\bar{\\bar{x}})^2}}{K}} \\tag{6.1} \\end{equation}\\] Notice that the formula for standard error is actually just the formula for standard deviation but with notation adapted to reflect the fact we are examining sample means (i.e., \\(\\bar{x}\\)’s) instead of people (i.e., X’s). Often we will not talk about standard error but rather the variance of the distribution of sample means. When we do so, we are talking about the same thing - the variability in sample means - but using different units. You can see that the formula for the variance of sample means, see Formula (6.2) below, is just a squared version of the standard error equation, Formula (6.1) above. \\[\\begin{equation} \\sigma_{\\bar{x}}^2 = \\frac{\\sum{(\\bar{x} - \\bar{\\bar{x}})^2}}{K} \\tag{6.2} \\end{equation}\\] How do we interpret variance of sample means? Assuming the sample mean provides an unbiased estimate of the population mean (i.e., \\(\\bar{\\bar{x}} = \\mu\\)), then the variance of the sample means is simply the average of the squared differences between sample means and the population mean. Standard error is simply the square root of the variance of sample means. Both are different ways of describing the same thing - the precision with which sample means estimate the population mean. If the variance of sample means is large, it indicates sample means differ considerably (on average) from the population mean. If the variance of sample means is small, it indicates sample means differ only slightly (on average) from the population mean. If the variance of sample means is zero, all the sample means are exactly the same as the population mean. We can calculate the variance of sample means below: many_samples %&gt;% summarise(var_of_means = var.pop(sample_mean)) %&gt;% as.data.frame() ## var_of_means ## 1 15.73 The standard error of sample mean is just the square root of this value: many_samples %&gt;% summarise(sd_of_means = sqrt(var.pop(sample_mean))) %&gt;% as.data.frame() ## sd_of_means ## 1 3.967 Thus, we can indicate the precision with which sample means (\\(\\bar{x}\\)) estimate the population mean (\\(\\mu\\)) using the notation below. We use \\(\\sigma_{\\bar{x}}^2\\) to refer to the variance of sample means and \\(\\sigma_{\\bar{x}}\\) to refer to the standard error of sample means. Again notice in the notation that for both of these symbols there is a subscript with an \\(\\bar{x}\\) to remind us we are talking about the variability of sample means (and not people). \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= 15.73 \\\\ \\sigma_{\\bar{x}} &amp;= 3.97 \\\\ \\end{aligned} \\] 6.5 A short cut In the previous section we used a computer simulation to calculate the variance of sample means. This involved several steps: Obtaining the entire population Creating 50000 samples (each n = 10) from the population Calculating the mean for each of the 50000 samples Calculating the variance of the 50000 sample means as per Formula (6.2) All those steps are a lot of work - and a lot of computer time. Interestingly, we don’t need to go to all that work to determine the variance of sample means. There is a quicker way to obtain the variance of sample means. That is, there is a quicker way to determine the precision with which sample means estimate the population mean when we use a sample size of \\(n\\) = 10. Statisticians have created a short-cut formula for obtaining the variance of sample means. The equation by itself, see Formula (6.3) below, is extraordinarily simple to use. The formula, however, is not easy to understand. Over many years of teaching, we’ve seen students struggle to understand why the formula is structured in the way that it is - looking for a straightforward logic as to why the short-cut formula works. We encourage you to avoid going down that road. Understanding Formula (6.3) below is beyond the scope of this chapter and, likely, your mathematical background at this point. It was derived via a complex mathematical proof discussed in W. Hays (1994) if you want more information. Simply accept that there is a short-cut formula and do not try to understand the logic of the formula. Variance sample means: \\[\\begin{equation} \\sigma_{\\bar{x}}^2 = \\frac{\\sigma^2}{n} \\tag{6.3} \\end{equation}\\] You can see that with this formula the variance of sample means, Formula (6.3), is simply the variance of heights of the individuals in the population divided by the sample size. Oddly, this short cut works and provides the variance of sample means. Notice that we don’t need to have the entire population, create thousands of samples, or go through any of the calculation work. All we need to know is the variance of the population and the sample size. In the context of the current example we can compute the variance of the sample means using this information. Recall the variance of people’s heights in the population is \\(\\sigma^2=157.5\\) and our sample size is \\(n\\) = 10. \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma^2}{n} \\\\ &amp;= \\frac{157.5}{10}\\\\ &amp;= \\frac{157.5}{10}\\\\ &amp;= 15.75 \\end{aligned} \\] We see that the variance of the sample means from this short-cut approach is 15.75 (i.e., \\(\\sigma_{\\bar{x}}^2\\) = 15.75). Likewise, the variance of the sample means from our simulation was 15.73; incredibly close. If we had used substantially more than 50000 samples in the simulation the 15.73 value would have been 15.75 – making the two results identical. Thus, the short-cut formula gives us the variance of sample means without having to do all the simulation work. Of course, the variance of sample means is only one of the two ways we can describe the precision of sample estimates of the population mean. Many people prefer to use standard error (i.e. standard deviation of sample means) instead. The calculation for standard error is below: \\[ \\begin{aligned} \\sigma_{\\bar{x}} &amp;= \\sqrt{\\frac{\\sigma^2}{n}} \\\\ &amp;= \\sqrt{\\frac{157.5}{10}}\\\\ &amp;= \\sqrt{\\frac{157.5}{10}}\\\\ &amp;= \\sqrt{15.75} \\\\ &amp;= 3.97 \\\\ \\end{aligned} \\] 6.6 Estimates of precision As wonderful as it is to have a short-cut formula for obtaining the variance of sample means (or standard error) there is a catch. The formula requires knowledge of the population variance (or standard deviation). In a research scenario we don’t know the mean or standard deviation of the population - that’s why we’re conducting research. Consequently, we can’t directly calculate the variance of sample means (or standard error) using Formula (6.3) above. Fortunately, if we have only a single study, we can use the sample variance (with n-1 in the denominator) to estimate the population variance. This estimate of the population variance may be higher or lower than the actual population variance - but it will, on average, be correct across many studies. With this estimate of the population variance in hand we can calculate an estimate of the variance of sample means (i.e, the precision of sample means) using Formula (6.4) below. Estimated variance sample means: \\[\\begin{equation} s_{\\bar{x}}^2 = \\frac{s^2}{n} \\tag{6.4} \\end{equation}\\] Estimated standard error for the mean: \\[\\begin{equation} s_{\\bar{x}} = \\sqrt{\\frac{s^2}{n}} = \\frac{s}{{\\sqrt{n}}} \\tag{6.5} \\end{equation}\\] FIGURE 6.3: Precision: Conceptual and calculation approaches. 6.6.1 A worked example Consider a worked example below. Imagine a scenario where we do not know anything about the population of heights. Therefore we conduct a study by taking a random sample of 10 people to estimate the average height of people in the population. The sample mean is 174.3cm and the sample variance is 30.90. Therefore, our best guess of the unknown population mean is 174.3cm. Because we used a sample (a small subset of the population) we know that 174.3cm is unlikely to be the actual mean height for the population. The mean height for the population might be higher or lower than 174.3cm. Nonetheless, at this point our best guess is that the mean of the population is 174.3cm; but we recognize that this is just an estimate of the population mean and is likely off by some amount. Wouldn’t it be great to know the extent to which the sample mean in our study might differ from the population mean? As budding statisticians, we know we can’t know that for this particular sample/study, but we can try to figure out how much sample means differ from the population mean on average (when using \\(n\\) = 10) to help us understand the precision of our current sample/study estimate (174.3cm) of the population mean. This precision information is exactly what is conveyed by the variance of sample means, \\(\\sigma_{\\bar{x}}^2\\), or standard error of sample means, \\(\\sigma_{\\bar{x}}\\). We can’t obtain these precision indices but we can estimate them (via \\(s_{\\bar{x}}^2\\) and \\(s_{\\bar{x}}\\)). We want to know the precision with which sample means estimate the population mean. Said another way, we want to know the variance of sample means (or standard error of sample means) but we only have a single study. Therefore, we have to rely on the short-cut formula for determining the variability in sample means. Unfortunately, the short-cut formula for the variance of sample means formula requires the population variance which we don’t know. We do, however, have an estimate of the population variance from our sample. The sample variance is 30.9cm\\(^2\\). Therefore, our best guess of the unknown population variance is 30.9cm\\(^2\\). Because we used a sample (a small subset of the population) we know that 30.9cm\\(^2\\) is likely to be different than the actual variance for the population. The variance for the population might be higher or lower than 30.9cm\\(^2\\). So even though our estimate of the population variance may be “off” in this single sample we know it will be accurate in the long run averaging over many studies (see previous chapter). Nonetheless, at this point our best guess is that the variance of the population is 30.9cm\\(^2\\). We use this information to estimate the variance of sample means: \\[ \\begin{aligned} s^2_{\\bar{x}} &amp;= \\frac{s^2}{n} \\\\ &amp;= \\frac{30.90}{10} \\\\ &amp;= 3.09 \\\\ \\end{aligned} \\] Or alternatively, estimate standard error: \\[ \\begin{aligned} s_{\\bar{x}} &amp;= \\sqrt{\\frac{s^2}{n}} \\\\ &amp;= \\sqrt{\\frac{30.90}{10}} \\\\ &amp;= \\sqrt{3.09} \\\\ &amp;= 1.76 \\\\ \\end{aligned} \\] At this point we don’t know the precision of sample means when \\(n\\) = 10; that is, we don’t know \\(\\sigma_{\\bar{x}}^2\\) or \\(\\sigma_{\\bar{x}}\\). We do have an estimate of the precision of sample means (i.e., with via \\(s_{\\bar{x}}^2\\) and \\(s_{\\bar{x}}\\)). Whenever you calculate the variance of sample means, or standard error, based on sample data it is ALWAYS just an estimate of the actual variance of sample means or the actual standard error. That fact that an estimate of the variance of sample means is the best you can do with sample data is evident when we calculate the estimate for the many samples. We do that with the R code below: many_samples &lt;- many_samples %&gt;% mutate(est_se2 = sample_var_n_1/n) The command above calculated the estimate variance of sample means (est_se2 or \\(s_{\\bar{x}}^2\\)) for each of the 50000 samples. We can see the first few samples with the head() command. head(many_samples) ## # A tibble: 6 × 5 ## study n sample_mean sample_var_n_1 est_se2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 174.3 30.9 3.09 ## 2 2 10 161.8 92.62 9.262 ## 3 3 10 173.2 154.62 15.462 ## 4 4 10 180 357.56 35.756 ## 5 5 10 172.8 178.84 17.884 ## 6 6 10 172 177.33 17.733 Inspect the est_se2 column. Notice how each of these values is an \\(s_{\\bar{x}}^2\\) number that is an estimate of \\(\\sigma_{\\bar{x}}^2\\) = 15.75. The first estimate, \\(s_{\\bar{x}}^2\\) = 3.09, is the one we did by hand above. This estimate of the variance of sample means (\\(s_{\\bar{x}}^2\\) = 3.09) is much lower than the actual variance of sample means (\\(\\sigma_{\\bar{x}}^2\\) = 15.75). Notice how the other values in the est_se2 column tend to either overestimate, or underestimate, the variance of sample means. That is, the other studies also overestimate, or underestimate, the precision with which sample means (\\(n\\) = 10) estimate the population mean. 6.7 Bias of precision estimates At this point, you might be a bit concerned about our ability to estimate the variance of sample means ( \\(\\sigma_{\\bar{x}}^2\\) = 15.75). In the simulation above, we calculated an estimate of the variance of sample means (\\(s_{\\bar{x}}^2\\)) and placed those values in the est_se2 column. We saw that the values in this column mostly differed from the actual variance of sample means (15.75). You might wonder if these estimates of the variance of sample means are themselves biased? The answer is no - they are not biased. Our estimate of the population variance (n-1 in the denominator) was not biased so there is no reason to suspect that the estimate of the variance of sample means is biased. But we can confirm this by averaging over the 50000 samples. If the average of the est_est2 column is 15.75 (or very close) than \\(s_{\\bar{x}}^2\\) is not a biased estimate of \\(\\sigma_{\\bar{x}}^2\\) = 15.75. many_samples %&gt;% summarise(avg.of.est_se2 = mean(est_se2)) ## # A tibble: 1 × 1 ## avg.of.est_se2 ## &lt;dbl&gt; ## 1 15.7271 We see that the average of the 50000 estimates of the variance of sample means (\\(s_{\\bar{x}}^2\\)) is 15.73 which is very close to the actual value of \\(\\sigma_{\\bar{x}}^2\\) = 15.75. Any sample estimate of the variance of sample means will be off somewhat due to sampling error. But, on average, estimates of the variance of sample means will be unbiased/accurate. 6.8 Where are we? So where are we? We’ve learned that sample means may overestimate, or underestimate, the population mean. Over many studies, on average, sample means will approximate the population mean. In other words, on average, sample means are accurate. As well, we’ve learned that the variability in estimates of the population mean is referred to as precision. We can index the precision of sample means, for a given sample size, using two indices: variance of sample means and standard error of sample means (i.e., standard deviation of sample means). Both of these formulas require knowledge of the population variance - something we never have. When we conduct a single study, we get a sample mean. We cannot estimate how precise that sample mean is; we can however, estimate how precise sample means are in general – via the variance of sample means (or standard error). There is a formula for calculating the variance of sample means, Formula (6.3), but it requires knowledge of the (unknown) population variance. Consequently, we use an estimate of the population variance in these formulas rather than the actual population variance, see Formula (6.4). As a result, when we calculate variance of sample means (or standard error) using an estimate of the population variance (from our sample) we only obtain an estimates of the variance of sample means. Moreover, this estimate of the variance of sample means is itself influenced by sampling error - but accurate when averaged over many studies. 6.9 Precision for means: Causes The formula for variance of the sampling distribution for the mean reveals that it is influenced only by the population variance and sample size. In a research scenario we don’t have any control over the population variance (it is what it is), but we do have control over sample size. Increasing the sample size decreases the variability in sample means - as illustrated in the formulas below. \\[ \\begin{aligned} \\sigma^2 &amp;= 157.5 \\\\ \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma^2}{n} = \\frac{157.5}{10} = 15.75 \\\\ \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma^2}{n} = \\frac{157.5}{50} = 3.15 \\\\ \\end{aligned} \\] We also show in Figure 6.4 the difference that increasing the sample size makes. The larger the sample size the less sample means vary due to sampling error. A larger sample size means a more precise estimate of the population parameter. FIGURE 6.4: Two sampling distributions illustrating variability in sample means based on sample size. The top graph show the variance of individual’s heights in the population. The graph is filled with X’s to remind you it is a graph of individuals. There are more individuals than X’s. The bottom two graphs show variability in sample means due to random sampling. Each of the lower two graphs is a sampling distribution for a given sample size. Each sampling distribution contains 50000 sample means. Each sample mean is the average of the heights of individuals in a sample. The sampling distributions are filled with \\(\\bar{x}\\)’s to remind you that it is sample means being graphed. There are more means than \\(\\bar{x}\\)’s. You can see that by using a larger sample size (\\(n = 50\\), for 50000 samples) there is less variability in sample means than when using a smaller sample size (\\(n = 10\\)), for 50000 samples. Additionally, a larger sample size makes the sample estimate (\\(s^2\\)) of the population variance (\\(\\sigma^2\\)) more precise. Consequently, it makes the sample estimate (\\(s_{\\bar{x}}^2\\) or \\(s_{\\bar{x}}\\)) of the variance of sample means (\\(\\sigma_{\\bar{x}}^2\\) or \\(\\sigma_{\\bar{x}}\\)) more precise. Therefore, increasing the sample size increases the precision of a) sample means (\\(\\bar{x}\\)) and b) precision estimates (\\(s_{\\bar{x}}^2\\) or \\(s_{\\bar{x}}\\)). The precision of sample means is influenced by: population variance/standard deviation (\\(\\sigma^2\\) / \\(\\sigma^2\\)) sample size (%n%) 6.10 Precision for \\(d\\)-values: Causes Whenever we use a statistic (e.g., \\(\\bar{x}\\)) to estimate a population-level parameter (e.g., \\(\\mu\\)) the statistic will differ from the parameter due to sampling error. This is true of all statistics. Therefore, it is possible to generate a sampling distribution for all statistics. In this section we will focus on the factors that influence the precision with which \\(d\\)-value estimate the population-level standardized mean difference (\\(\\delta\\)). More specifically, we focus on how the sample size and the size of the population-level effect influence width of the sampling distribution. 6.10.1 Sample size Consider the case of estimating the standardized mean difference (i.e., \\(\\delta\\)) between the heights of males and females, illustrated in Figure 6.5. The population-level height information is presented in Figure 6.5A. The sampling distribution for sample-level \\(d_{unbiased}\\) values when you use 10 people per group (10 males, 10 females) is presented in Figure 6.5B. We could calculate the variance of \\(d_{unbiased}\\) values or the standard error (i.e., standard deviation) of \\(d_{unbiased}\\) values; just like we did for sample mean. You can see in Figure 6.5B that even though the population-level standardized mean difference is 1.49 (\\(\\delta\\) = 1.49) there was considerable variability in sample-level \\(d_{unbiased}\\) values. Indeed, inspecting the sample-level data reveals when the population difference is \\(\\delta\\) = 1.49, the sample level differences range from \\(d_{unbiased}\\) = -0.38 to \\(d_{unbiased}\\) = 5.50. That’s a considerable range. When \\(d_{unbiased}\\) = -0.38 you would conclude, in your study, that the mean female height is .38 standard deviations higher than the mean male height. Yet a colleague, conducting exactly the same study, could obtain a sample that leads him to observe \\(d_{unbiased}\\) = 5.50. This would indicate to him that the mean male height is 5.50 standard deviations higher than the mean female height; an opposite conclusion. The results of both your study and the colleague’s study differ considerably from the truth that \\(\\delta\\) = 1.49 – which indicates the mean male height is 1.49 standard deviations higher than the mean female height. We investigated the extent to which study-level \\(d_{unbiased}\\) values differ from the population \\(\\delta\\) = 1.49 when using 100 people per group for the comparison (i.e., 100 males, 100 females). You can see in Figure 6.5C that in this scenario the precision of the sample estimates (\\(d_{unbiased}\\)) of the population parameter (\\(\\delta\\)) still vary considerably - but substantially less than they did when it was 10 per group. With a 100 people per group, the \\(d_{unbiased}\\) values range from 0.91 to 2.21. For the lower end of this range, 0.91, a researcher would conclude that the mean for males is .91 standard deviations higher than the mean for females. At the upper end of this range, 2.21, a researcher would conclude that the mean for males is 2.21 standard deviations higher than the mean for females. The variability in \\(d_{unbiased}\\) values is still large - but at least the conclusions are a) all in the same direction, and b) would all be considered large (i.e, above 0.80) according to Cohen’s standards. These simulations illustrate a few important points. First, drawing any conclusion from a single study is problematic. The findings for a single study may differ substantially from the population parameter. Second, increasing sample size increases the precision with which a sample statistic estimates a population parameter. Third, drawing conclusions from small sample size studies (e.g., \\(n\\) = 10 per group) is extraordinarily problematic. Finally, we note that single studies (even with small sample size) are useful because they serve as data points for a future meta-analysis. FIGURE 6.5: Sampling distribution for \\(d_{unbiased}\\) 6.10.2 Population effect size (\\(\\delta\\)) The precision with which sample \\(d\\)-values estimate the population standardized mean difference is influenced by the magnitude of the population standardized mean difference. This situation differs sharply from that of sample means. The magnitude of the population mean does not influence the shape of the distribution of sample means. The distribution of sample means is normal - regardless of whether the population mean is high or low. In contrast, the shape of the the sampling distribution of \\(d\\)-values changes depending on the magnitude of the population effect (i.e., \\(\\delta\\)). In Figure 6.6 we illustrate two scenarios. Both scenarios are based on a repeated measures design. For each scenario we created a population-level difference and then obtained 50000 sample \\(d\\)-values. The sample size, \\(n\\) = 10 for each \\(d\\)-value, was held constant across the two scenarios. In contrast, the population standardized mean difference was different across the two scenarios. In Figure 6.6A the population standardized mean difference was \\(\\delta\\) = 0.50. In contrast, in Figure 6.6B the population standardized mean difference was \\(\\delta\\) = 2.00. Compare the shape of the distributions of \\(d\\)-values in Figure 6.6A and Figure 6.6B. You can see how the shape changes when the population level effect size changes. Consequently, the size of the population standardized mean difference influences the precision with which sample \\(d\\)-values estimate the population level effect (\\(\\delta\\)). FIGURE 6.6: d-value skew graphs Standard Error There is a standard error for \\(d\\)-values (i.e., \\(SE_d\\) or \\(SE_{d_{unbiased}}\\)). It is simply the standard deviation of the thousands of \\(d\\)-values in the simulation. More generally, it is an index of how much \\(d\\)-values vary from a population standardized mean difference (\\(\\delta\\)) due to sampling error for a given sample size. In geometric terms, the standard error is an index of the width of a sampling distribution. The standard error is larger in Figure 6.5B than in Figure 6.5C. The precision of sample \\(d\\)-values is influenced by: population standardized mean difference (\\(\\delta\\)) - influences shape of the distribution sample size (\\(n\\)) 6.11 Precision for sample correlations (\\(r\\)): Causes Sample correlations (\\(r\\)) are likely to differ from the population correlation (\\(\\rho\\)) due to sampling error. The precision with which sample correlations estimate the population correlation is influenced by sample size and the magnitude of the population correlation. These are reviewed below. 6.11.1 Sample size Consider a situation where the population-level relation between two variables is \\(\\rho = .30\\). For example, when looking at 300000 people the correlation between weight and height is .30. Then consider two scenarios where we sample from the population. In Scenario 1, we use a sample size of 50 people. We take 50000 samples (each comprised of 50 people) and calculate for each sample the correlation (\\(r\\)). The distribution of these sample correlations for Scenario 1 are presented in Figure 6.7A. In Scenario 2, we use a sample size of 500 people (ten times more per sample). We take 50000 samples (each comprised of 500 people) and calculate for each sample the correlation (\\(r\\)). The distribution of these sample correlations for Scenario 2 are presented in Figure 6.7B. If you contrast these two graphs you see the sampling distribution is narrower in Scenario 2 where the sample size is larger, see Figure 6.7B. This means that sampling error is less when the sample size is larger. In other words, a larger sample size results in more precise sample correlations. FIGURE 6.7: Correlation precision and sample size 6.11.2 Population effect size (\\(\\rho\\)) Consider a situation where we look at the correlation between height and weight in two different cities. Each city has 300000 people and we consider each city a population. In City 1 the population correlation is \\(\\rho = .30\\) whereas in City 2 the population correlation is \\(\\rho = .70\\). For City 1, we take 50000 samples (each comprised of 50 people) and calculate for each sample the correlation (\\(r\\)). The distribution of these sample correlations for City 1 are presented in Figure 6.8A. We repeat the process and obtain 50000 sample correlations (each comprised of 50 people) for City 2; the distribution of these sample correlations is presented in Figure 6.8B. If you contrast these two graphs you see the sampling distribution is narrower for City 2 where the population correlation is stronger, see Figure 6.8B. This means that sampling error is less when the population correlation (\\(\\rho\\)) is stronger. In other words, a larger population correlation results in more precise sample correlations. Importantly, also notice how the shape of the sampling distribution varies across the two effect-size scenarios (Figure 6.8 A vs B). FIGURE 6.8: Correlation precision and effect size Standard Error As with the other statistics reviewed, for sample correlations (\\(r\\)) you can calculate a standard error (\\(SE_r\\)). It is simply the standard deviation of the thousands of sample correlations (\\(r\\)’s) in the simulation. More generally, it is an index of how much sample correlations vary from a population correlation (\\(\\rho\\)) due to sampling error for a given sample size. In geometric terms, the standard error is an index of the width of a sampling distribution. The standard error is larger in Figure 6.7A than in Figure 6.7B. The precision of sample correlations (\\(r\\)) is influenced by: population correlation (\\(\\rho\\)) - influences shape of the distribution sample size (\\(n\\)) References "],["an-emphasis-on-workflow.html", "Chapter 7 An Emphasis on Workflow 7.1 Required Packages 7.2 Objective 7.3 Begin with the end in mind 7.4 Data collection considerations 7.5 Example: Single Occassion Survey", " Chapter 7 An Emphasis on Workflow 7.1 Required Packages The data files below are used in this chapter. Required Data data_ex_between.csv data_ex_within.csv data_item_scoring.csv The following CRAN packages must be installed: Required CRAN Packages apaTables Hmisc janitor psych skimr tidyverse Important Note: You should NOT use library(psych) at any point! There are major conflicts between the psych package and the tidyverse. We will access the psych package commands by preceding each command with psych:: instead of using library(psych). 7.2 Objective Due to a number of high profile failures to replicate study results (Nosek 2015) it’s become increasingly clear that there is a general crisis of confidence in many areas of science (Baker 2016). Statistical (and other) explanations have been offered (Simmons, Nelson, and Simonsohn 2011) for why it’s hard to replicate results across different sets of data. However, scientists are also finding it challenging to recreate the numbers in their own papers using their own data. Indeed, the editor of Molecular Brain asked authors to submit the data used to create the numbers in published papers and found that the wrong data was submitted for 40 out of 41 papers (Miyakawa 2020). Consequently, some researchers have suggested that it is critical to distinguish between replication and reproducibility (Patil P. 2019). Replication refers to trying to obtain the same results from a different data set. Reproducibility refers to trying to obtain the same results from the same data set. Unfortunately, some authors use these two terms interchangeably and fail to make any distinction between them. I encourage you to make the distinction and the use the terms consist with use suggested by (Patil P. 2019). It may seem that reproducibility should be a given - but it’s not. Correspondingly, there is a trend for journals and authors to adopt Transparency and Openness Promotion (TOP) guidelines. These guidelines involve such things as making your materials, data, code, and analysis scripts available on public repositories so anyone can check your data. A new open science journal rating system has even emerged called the TOP Factor. The idea is not that open science articles are more trustworthy than other types of articles – the idea is that trust doesn’t play a role. Anyone can inspect the data using the scripts and data provided by authors. It’s really just the same as making your science available for auditing the way financial records can be audited. But just like in the world of business, some people don’t like the idea of making it possible for others to audit their work. The problems reported in Molecular Brain (doubtless common to many journals) are likely avoided with open science - because the data and scripts needed to reproduce the statistics in the articles are uploaded prior to publication. The TOP open science guidelines have made an impact and some newer journals, such as Meta Psychology, have fully embraced open science. Figure 7.1 shows the header from an article in Meta Psychology that clearly delineates the open science attributes of the article that used computer simulations (instead of participant data). Take note that the header even indicates who verified that the analyses in the article were reproducible. FIGURE 7.1: Open science in an article header In Canada, the majority of university research is funded by the Federal Government’s Tri-Agency (i.e., NSERC, SSHRC, CIHR). The agency has a new Data Management Policy in which they state that “The agencies believe that research data collected through the use of public funds should be responsibly and securely managed and be, where ethical, legal and commercial obligations allow, available for reuse by others. To this end, the agencies support the FAIR (Findable, Accessible, Interoperable, and Reusable) guiding principles for research data management and stewardship.” [emphasis added] The perspective of the funding agency on data ownership differs substantially from that of some researchers who incorrectly believe “they own their data”. In Canada at least, the government makes it clear that when tax payers fund research (through the Tri-Agency) the research data is public property. Additionally the Tri-Agency Data Management Statement of Principles clearly indicates the responsibilities of funded researchers: “Responsibilities of researchers include: incorporating data management best practices into their research; developing data management plans to guide the responsible collection, formatting, preservation and sharing of their data throughout the entire lifecycle of a research project and beyond; following the requirements of applicable institutional and/or funding agency policies and professional or disciplinary standards; acknowledging and citing datasets that contribute to their research; and staying abreast of standards and expectations of their disciplinary community.” As a result of this perspective on data, it’s important that you think about structuring your data for reuse by yourself and others before you collect it. Toward this end, properly documenting your data file and analysis scripts is critical. 7.3 Begin with the end in mind In this chapter we will walk you though the steps from data collection, data entry, loading raw data, and the creation of data you will analyze (analytic data) via pre-processing scripts. These steps are outlined in Figure 7.2. This figure makes a clear distinction between raw data and analytic data. Raw data refers to the data as you entered it into a spreadsheet or received it from survey software. Analytic data is the data that has been structured and processed so that it is ready for analysis. This pre-processing could include such things as identifying categorical variables to the computer, averaging multiple items into a scale score, and other tasks. It’s critical that you don’t think of the analysis of your data as being completely removed from the data collection and data entry choices you make. Poor choices at the data collection and data entry stage can make your life substantially more complicated when it comes time to write the pre-processing script that will convert your raw data to analytic data. The mantra of this chapter is begin with the end in mind. FIGURE 7.2: Data science pipeline by Roger Peng It’s difficult to being with the end in mind when you haven’t read later chapters. So, here we will be provide you with some general thoughts around different approaches to structuring data files and the naming conventions you can use when creating those data files. Indeed, in this chapter we strongly advocate that you use a naming convention for file, variable, and column names. This convention will save you hours of hassles and permit easy application of certain tidyverse commands. However, we must stress that although the naming convention we advocate is based on the tidyverse style guide, it is not “right” or “correct” - there are other naming conventions you can use. Any naming convention is better than no naming convention. The naming convention we advocate here will solve many problems. We encourage to use this system for weeks or months over many projects - until you see the benefits of this system, and correspondingly its shortcomings. After you are well versed in the strengths/weaknesses of the naming conventions used here you may choose to create your own naming convention system. 7.3.1 Structuring data: Obtaining tidy data When conducting analyses in R it is typically necessary to have data in a format called tidy data (Wickham 2014). Tidy data, as defined by Hadley, involves (among other requirements) that: Each variable forms a column. Each observation forms a row. The tidy data format can be initially challenging for some researchers to understand because it is based on thinking about, and structuring data, in terms of observations/measurements instead of participants. In this section we will describe common approaches to entering animal and human participant data and how they can be done keeping the tidy data requirement in mind. It’s not essential that data be entered in a tidy data format but it is essential that you enter data in a manner that makes it easy to later convert data to a tidy data format. When dealing with animal or human participant data it’s common to enter data into a spreadsheet. Each row of the spreadsheet is typically used to represent a single participant and each column of the spreadsheet is used to represent a variable. Between participant data. Consider Table 7.1 which illustrates between participant data for six human participants running 5 kilometers. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Finally, there is a last column elapsed_time which is a variable with one observation per row – also conforming to tidy data specification. Thus, single occasion between subject data like this conforms to the tidy data specification. There is usually nothing you need to do to convert between-participant data (or cross-sectional data) to be in a tidy data format. TABLE 7.1: Between participant data entered one row per participant id sex elapsed_time 1 male 40 2 female 35 3 male 38 4 female 33 5 male 42 6 female 36 Within participant data. Consider Table 7.2 which illustrates within participant data for six human participants running 5 kilometers - but on three different occasions. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Next, there are three different columns (march, may, july) each of which contains elapsed time (in minutes) for the runner in a different month. Elapsed run times are spread out over three columns so elapse_time is not in a tidy data format. Moreover, it’s not clear from the data file that march, may, and july are levels of a variable called occasion. Nor is it clear that elapsed_times are recorded in each of those columns (i.e., the dependent variable is unknown/not labeled). Although this format is fine as a data entry format it clearly has problems associated with it when it comes time to analyze your data. TABLE 7.2: Within participant data entered one row per participant id sex march may july 1 male 40 37 35 2 female 35 32 30 3 male 38 35 33 4 female 33 30 28 5 male 42 39 37 6 female 36 33 31 TABLE 7.3: A tidy data version of the within participant data id sex occasion elapsed_time 1 male march 40 1 male may 37 1 male july 35 2 female march 35 2 female may 32 2 female july 30 3 male march 38 3 male may 35 3 male july 33 4 female march 33 4 female may 30 4 female july 28 5 male march 42 5 male may 39 5 male july 37 6 female march 36 6 female may 33 6 female july 31 Thus, a major problem with entering repeated measures data in the one row per person format is that there are hidden variables in the data and you need insider knowledge to know what the columns represent. That said, this is not necessarily a terrible way to enter your data as long as you have all of this missing information documented in a data code book. Disadvantages one row per participant Advantages one row per participant 1) Predictor variable (occasion) is hidden and spread over multiple columns 1) Easy to enter this way 2) Unclear that each month is a level of the predictor variable occasion 3) Dependent variable (elapsed_time) is not indicated 4) Unclear that elapsed_time is the measurement in each month column Fortunately, the problems with Table 7.2 can be largely resolved by converting the data to the a tidy data format. This can be done with the pivot_long() command that we will learn about in the Cookbook chapter. Thus, we can enter the data in the format of Table 7.2 and later convert it to a tidy data format. After this conversion the data will be appear as in Table 7.3. For elapsed_time variable this data is now in the tidy data format. Each row corresponds to a single elapsed_time observed. Each column corresponds to a single variable. Somewhat problematically, however, sex is repeated three times for each person (i.e., over the three rows) - and this can be confusing. However, if the focus in on analyzing elapsed time this tidy data format makes sense. Importantly, there is an id column for each participant so R knows that this information is repeated for each participant and is not confused by repeating the sex designation over three rows. Indirectly, this illustrates the importance of having an id column to indicate each unique participant. Why did we walk you through this technical treatment of structuring data at this point in time - so that you pay attention to the advice that follows. You can see at this point that you may well need to restructure your data for certain analyses. The ability to do so quickly and easily depends upon following the advice in this chapter around naming conventions for variables and other aspects of your analyses. You can imagine the challenges for converting the data in Figure 7.2 to the data in Figure 7.3 by hand. You want to be able to automate that process and others - which is made substantially easier if you follow the forthcoming advice about naming conventions in the tidyverse. 7.4 Data collection considerations Data can be collected in a wide variety of ways. Regardless of the method of data collection researchers typically come to data in one of two ways: 1) a research assistant enters the data into a spreadsheet type interface, or 2) the data is obtained as the output from computer software (e.g., Qualtrics, SurveyMonkey, Noldus, etc.). Regardless of the approach, it is critical to name your variables appropriately. For those using software, such as Qualtrics, this means setting up the software to use appropriate variable names PRIOR to data collection - so the exported file has desirable column names. For spreadsheet users, this means setting up the spreadsheet in which the data will be recorded with column names that are amenable to the future analyses you want to conduct. Although failure to take this thoughtful approach at the data collection stage can be overcome - it is only overcome with substantial manual effort. Therefore, as noted previously, we strongly encourage you to follow the naming conventions we espouse here when you set up your data recording regime. Additionally, we encourage you to give careful thought in advance to the codes you will use to record missing data. 7.4.1 File naming conventions I strongly suggest you check out these excellent slides by Danielle Navarro on file name convention best practices. 7.4.2 Data column naming conventions To make your life easier down the road, it is critical you set up your spreadsheet or online survey such that it uses a naming convention prior to data collection. The naming conventions suggested here are adapted from the tidyverse style guide. Lowercase letters only If using multiple words in a name (a good idea), only use the underscore (“_“) character to separate words in the name. Avoid short decontextualized variable names like q1, q2, q3, etc. Do use moderate length column names. Aim to achieve a unique prefix for related columns so that those columns can be selected using the starts_with() command discussed in the previous chapter. Be sure to avoid short two or three letter prefixes for item names. Instead, use unique moderate length item prefixes so that it will be easy to select those columns using start_with() such that you don’t accidentally get additionally columns you don’t want - that have a similar prefix. Likert items. Be sure to indicate the following information in the name of each Likert item or you will make your life substantially more complicated when you start to analyze your data. The information to include: a) the name of the measure, b) the item number for the measure, c) that it is a Likert item, d) the number of Likert response options, and e) whether the item is reverse keyed. That’s five things to include in each Likert item name. But it’s easy to do so. Consider two “affective commitment” items, the 2nd and 3rd items on scale. Both items use a 5-point Likert response format. However, item 3 is reverse keyed. Names that conform to this convention are: aff_com3_likert5, aff_com3_likert5rev. Using this naming convention ensures you can easily select and convert the items later. You can select by “likert5”, “likert5rev” or select by “aff_com”. If you have a column name that represents the levels of two repeated measures variables only use the underscore character to separate the levels of the different variables. See within-participant ANOVA section below for details. Column content. Avoid numerical representation of categorical variables. Don’t use 1 or 2 to represent a variable like sex. Use male and female in your spreadsheet - likewise in your survey program. Similarly, for between participant variables like drug_condition don’t use 1 or 2 use “drug” and “placebo” but the actual drug name would be even better than the word “drug.” Following this approach ensure the data can “stand alone” for resuse by others (especially if a data codebook (example) is not provided.) Note you will covert categorical variables such as sex (male/female) to numeric representations in your script - but then it will be clear what each value means. 7.4.3 Likert-type items A Likert-type item is typically composed of a statement with which participants are asked to agree or disagree. For example, participants could be asked to indicate the extent to which they agree with a number of statements such as “I like my job”. Then they would be presented with response scale such as: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. A common question is, how should I enter the data? Export text responses not numbers Software such as Qualtrics gives you the option of exporting the label (e.g., “Strongly Agree”) or a value (e.g., 1). Make sure you export the text lable (“Strongly Agree). That way, the data file stands alone - and doesn’t require additional knowledge to know what 1 means. You can easily convert the labels to numbers later. High numbers should be associated with more of the construct being measured. When designing your survey or data collection tools, it is important that you set the response options appropriately. If your scale measures job satisfaction, it is important that you collect data in a manner that ensures high numbers on the job satisfaction scale indicate high levels of job satisfaction. Therefore, assigning numbers makes sense using the 5-point scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. With this approach high response numbers indicate more job satisfaction. However, using the opposite scale would not make sense: 1 - Strongly Agree, 2 - Moderately Agree, 3 - Neutral, 4, Moderately Disagree, 5 - Strongly Disagree. With this opposite scale high numbers on a job satisfaction scale would indicate lower levels of job satisfaction - a very confusing situation. Avoid this situation, assign numbers so that higher numbers are associated with more of the construct being measured. Use appropriate item names. As described in the naming convention section, use moderate length names with different labels for each subscale. Use moderate length column names unique to each subscale. Imagine you have a survey with an 18-item commitment scale (Meyer, Allen, and Smith 1993) composed of three 6-item subscales: affective, normative, and continuance commitment. It would be a poor choice to prefix the labels of all 18 columns in your data with “commit” such that the names would be commit1, commit2, commit3, etc. The problem with this approach is that it fails to distinguish among the three subscales in naming convention; making it impossible to select the items for a single subscale using starts_with(). A better, but still poor choice for a naming convention would be use use a two letter prefix for the three scale such ac, nc, and cc. This would result in names for the columns like ac1, ac2, ac3, etc. This is an improvement because you could apparently (but likely not) select the columns using starts_with(“ac”). The problem with these short names is that there could be many columns in data set that start with “ac” beside the affective commitment items. You might want to select the affective commitment items using starts_with(“ac”); but you would get all the affective commitment item columns; but also all the columns measuring other variables that also start with “ac”. Therefore, it’s a good idea to use a moderate length unique prefix for column names. For example, you might use prefixes like affect_com, norm_com, and contin_com for the three subscales. But see below because you need to include more than this in each name. Indicate these 5 things in each Likert item name. Be sure to indicate the following information in the name of each Likert item or you will make your life substantially more complicated when you start to analyze your data. The information to include: 1) the name of the measure, 2) the item number for the measure, 3) that it is a Likert item, 4) the number of Likert response options, and 5) whether the item is reverse keyed. That’s five things to include in each Likert item name. But it’s easy to do so. Consider two “affective commitment” items, the 2nd and 3rd items on scale. Both items use a 5-point Likert response format. However, item 3 is reverse keyed. Names that conform to this convention are: aff_com2_likert5, aff_com3_likert5rev. Using this naming convention ensure you can easily select and convert the items later. You can select by “likert5”, “likert5rev” or select by “aff_com” (or both). Indicate in the item name if the item is reversed keyed. Sometimes with Likert-type items, an item is reverse keyed. For example, on a job satisfaction scale, participants will typically respond to items that reflect job satisfaction using the scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. Higher numbers indicate more job satisfaction. Sometimes, however, some items will use the same 1 to 5 response scale but be worded in the opposite manner such as “I hate my job”. Responding with a 5 to this item would indicate high job dissatisfaction. But the columns for job satisfaction items should have high values that indicate high job satisfaction not high job dissatifaction. Consequently, we flag the names of columns with reversed responses (i.e., reverse-key items) so that we know to treat those column differently later. Columns with reverse-keyed items need to be processed by a script so that the values are flipped and scored in the right direction. The procedure for doing so is outlined in the next point. Indicate in the item name the range for reverse-key items. If an item is reverse keyed, the process for the flipping the scores depends upon the range of a scale. Although 5-point scales are common, any number of points are possible. The process for correcting a reverse-key item depends upon: 1) the number of points on the scale, and 2) the range of the points on the scale. The reverse-key item correction process is different for an item that uses a 5-point scale ranging from 1 to 5 versus from 0 to 4. Both are 5-point scales but your correction process will be different. Therefore, for reverse-key items add a suffix at the end of each item name that indicates an item is reverse keyed and the range of the item. For example, if the third job satisfaction item was reversed keyed on scale using a 1 to 5 response format you might name the item: job_sat3_likert5rev. The suffix “_likert5rev” indicates the item is Likert item that is reverse keyed and the range of responses used on the item is 1 to 5. Be sure to set up your survey with this naming convention when you collect your data. If you collect items over multiple time points use a prefix with a short code to indicate the time followed by an underscore. For example, if you had a multi-item self-esteem scale you might call the column for the first time “t1_esteem1_likert5rev”. This indicate that you have for time 1 (t1), the first self-esteem item (esteem1) and that item is a likert item that is reverse keyed on a 1 to 5 scale. 7.5 Example: Single Occassion Survey This section outlines a workflow appropriate for when you have cross-sectional single occasion survey data. Examples for other designs are presented in the Cookbook chapter. The data corresponds to a design where the researcher has measured, age, sex, eye color, self-esteem, and job satisfaction. Two of these, self-esteem and job satisfaction, were measured with multi-item scales with reverse-keyed items. To Begin: Use the Files tab to confirm you have the data: data_item_scoring.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: Single occasion survey # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_survey &lt;- read_csv(file = &quot;data_item_scoring.csv&quot;, na = my_missing_value_codes) ## Rows: 300 Columns: 14 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): sex, eye_color ## dbl (12): id, age, esteem1_likert5, esteem2_likert5, est... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We load the initial data into a raw_data_survey but immediately make a copy we will work with called analytic_data_survey. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_survey &lt;- raw_data_survey Remove empty row and columns from your data using the remove_empty_cols() and remove_empty_rows(), respectively. As well, clean the names of your columns to ensure they conform to tidyverse naming conventions. library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test # Initial cleaning analytic_data_survey &lt;- analytic_data_survey %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fema… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … 7.5.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fema… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … We have two variables, sex and eye_color, that are categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. You can quickly convert all character columns to factors using the code below: analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = c(sex, eye_color), .fns = as_factor)) The participant identification number in the id column is a numeric column, so we have to handle that column on its own. analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;fct&gt; male, female, male, female, mal… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, h… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … Inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. Note: If you have factors like sex that have numeric data in the column (e.g, 1 and 2) instead of male/female you need to handle the situation differently. The preceding section, Experiment: Within N-way, illustrates how to handle this scenario. 7.5.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 male :147 blue : 99 ## 2 : 1 female :149 brown: 98 ## 3 : 1 intersex: 2 hazel:100 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = fct_relevel(sex, &quot;intersex&quot;, &quot;female&quot;, &quot;male&quot;)) For eye color, we want a future graph to have the most common eye colors on the left so we reorder the factor levels: analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(eye_color = fct_infreq(eye_color)) You can see the new order of the factor levels with summary(): analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 intersex: 2 hazel:100 ## 2 : 1 female :149 blue : 99 ## 3 : 1 male :147 brown: 98 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 7.5.3 Numeric screening For numeric variables, it’s important to find and remove impossible values. For example, in the context of this example you want to ensure none of the Likert responses are impossible (e.g., outside the 1- to 5-point rating scale) or clearly data entry errors. Because we have several numeric columns that we are screening, we use the skim() command from the skimr package. The skim() command quickly provides basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. Start by examining the range of non-scale items. In this case it’s only age. Examine the output to see if any of the age values are unreasonable. As noted, in the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the minimum and maximum values for the variable. Check to make sure none of the age values are unreasonably low or high. If they are, you may need to check the original data source or replace them with missing values. library(skimr) analytic_data_survey %&gt;% select(age) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17 20 24 With respect to the multi-item scales, it makes sense to look at sets of items rather than all of the items at once. This is because sometimes items from different scales use different response ranges. For example, one measure might use a response scale with a range from 1 to 5; whereas another measure might use a response scale with a range from 1 to 7. This is undesirable from a psychometric point of view, as discussed previously, but if it happens in your data - look at the scale items separately to make it easy to see out of range values. We begin by looking at the items in the first scale, self-esteem. Possible items responses for this scale range from 1 to 5; make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 esteem1_likert5 24 3.39 0.54 3 3 5 ## 2 esteem2_likert5 28 2.35 0.48 2 2 3 ## 3 esteem3_likert5 31 3.96 0.37 3 4 5 ## 4 esteem4_likert5 15 3.54 0.50 3 4 4 ## 5 esteem5_likert5rev 35 2.22 0.47 1 2 3 Follow the same process for the job satisfaction items. Write that code on your own now. Possible item responses for the job satisfaction scale range from 1 to 5, make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 jobsat1_likert5 25 3.34 0.51 3 3 5 ## 2 jobsat2_likert5rev 27 1.51 0.61 1 1 3 ## 3 jobsat3_likert5 28 2.84 0.37 2 3 3 ## 4 jobsat4_likert5 35 4.29 0.70 3 4 5 ## 5 jobsat5_likert5 24 4.57 0.61 3 5 5 7.5.4 Scale scores For each person, scale scores involve averaging scores from several items to create an overall score. The first step in the creation of scales is correcting the values of any reverse-keyed items. 7.5.4.1 Reverse-key items The way you deal with reverse-keyed items depends on how you scored them. Imagine you had a 5-point scale. You could have scored the scale with the values 1, 2, 3, 4, and 5. Alternatively, you could have scored the scale with the values 0, 1, 2, 3, and 4. The mathematical approach you use to correcting reverse-keyed items depends upon whether the scale starts with 1 or 0. In this example, we scored the data using the value 1 to 5; so that is the approach illustrated here. See the extra information box for details on how to fixed reverse-keyed items when the scale begins with zero. In this data file all the reverse-keyed items were identified with the suffix “_likert5rev” in the column names. This suffix indicates the item was reverse keyed and that the original scale used the response points 1 to 5. We can see those items with the glimpse() command below. Notice that there are two reverse-keyed items - each on difference scales. analytic_data_survey %&gt;% select(ends_with(&quot;_likert5rev&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 2 ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … To correct a reverse-keyed item where the lowest possible rating is 1 (i.e, 1 on a 1 to 5 scale), we simply subtract all the scores from a value one more than the highest point possible on the scale (i.e., one more than 5). For example, if a 1 to 5 response scale was used we subtract each response from 6 to obtain the recoded value. Original value Math Recoded value 1 6 - 1 5 2 6 - 2 4 3 6 - 3 3 4 6 - 4 2 5 6 - 5 1 The code below: selects columns that end with “_likert5rev” (i.e., both esteem and jobsat scales) subtracts the values in those columns from 6 renames the columns by removing “_likert5rev” from the name because the reverse coding is complete analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(6 - across(.cols = ends_with(&quot;_likert5rev&quot;)) ) %&gt;% rename_with(.fn = str_replace, .cols = ends_with(&quot;_likert5rev&quot;), pattern = &quot;_likert5rev&quot;, replacement = &quot;_likert5&quot;) You can use the glimpse() command to see the result of your work. If you compare these new values to those obtained from the previous glimpse() command you can see they have changed. Also notice the column names no longer indicate the items are reverse keyed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17… ## $ sex &lt;fct&gt; male, female, male, female, male, … ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, haze… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, … ## $ esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4… ## $ jobsat2_likert5 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, … If your scale had used response options numbered 0 to 4 the math is different. For each item you would use subtract values from the highest possible point (i.e, 4) instead of one larger than the highest possible point. Original value Math Recoded value 0 4 - 0 4 1 4 - 1 3 2 4 - 2 2 3 4 - 3 1 4 4 - 4 0 Thus, the mutate command would instead be: mutate(4 - across(.cols = ends_with(“_likert5rev”)) ) 7.5.4.2 Creating scores The process we use for creating scale scores deletes item-level data from analytic_data_survey. This is a desirable aspect of the process because it removes information that we are no longer interested in from our analytic data. That said, before we create scale score, we create a backup on the item-level data called analytic_data_survey_items. We will need to use this backup later to compute the reliability of the scales we are creating. analytic_data_survey_items &lt;- analytic_data_survey We want to make a self_esteem scale and plan to select items using starts_with(“esteem”). But prior to doing this we make sure the start_with() command only gives us the items we want - and not additional unwanted items. The output below confirms there are not problems associated with using starts_with(“esteem”). analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, … ## $ esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3,… Likewise, we want to make a job_sat scale and plan to select items using starts_with(“jobsat”). The code and output below using starts_with(“jobsat”) only returns the items we are interested in. analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4… ## $ jobsat2_likert5 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, … We calculate the scale scores using the rowwise() command. The mean() command provides the mean of columns by default - not people. We use the rowwise() command in the code below to make the mean() command work across columns (within participants) rather than within columns. The mutate command calculates the scale score for each person. The c_across() command combined with the starts_with() command ensures the items we want averaged together are the items that are averaged together. Notice there is a separate mutate line for each scale. The ungroup() command turns off the rowwise() command. We end the code block by removing the item-level data from the data set. Important: Take note of how we name the scale variables (e.g., self_esteem, job_sat). We use a slightly different convention than our items. That is, these scale labels were picked so that they would not be selected by a starts_with(“esteem”) or starts_with(“jobsat”). Why - because we later use those commands to remove the item-level data. We would not want the command designed to remove the item-level data to also remove the scale we just calculated! This example illustrates how carefully you need to think about your naming conventions. analytic_data_survey &lt;- analytic_data_survey %&gt;% rowwise() %&gt;% mutate(self_esteem = mean(c_across(starts_with(&quot;esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(job_sat = mean(c_across(starts_with(&quot;jobsat&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;esteem&quot;)) %&gt;% select(-starts_with(&quot;jobsat&quot;)) We can see our data now has the self_esteem column, a job_sat column, and that all of the item-level data has been removed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 6 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA… ## $ sex &lt;fct&gt; male, female, male, female, male, fema… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, b… ## $ self_esteem &lt;dbl&gt; 3.200, 3.800, 3.800, 3.000, 3.400, 3.5… ## $ job_sat &lt;dbl&gt; 4.00, 5.00, 4.40, 3.50, 4.00, 3.80, 3.… You now have two data sets analytic_data_survey and analytic_data_survey_items. You can calculate descriptive statistics, correlations and most analyses using the analytic_data_survey. To obtain the reliability of the scales you just created though you will need to use the analytic_data_survey_items. Both sets of data are ready for analysis. References "],["qualtrics.html", "Chapter 8 Qualtrics 8.1 Overview 8.2 Required 8.3 Items to Qualtrics 8.4 Qualtrics data to R 8.5 Extra Help", " Chapter 8 Qualtrics 8.1 Overview There are two tasks covered in this chapter: 1) importing surveys into Qualtrics and 2) moving data from Qualtrics to R. An overview of each process is below. 1. Importing items into Qualtrics Create a survey codebook in Excel (or similar program) that has each item and the response options for it Convert the survey codebook to Qualtrics Advanced Text Format Import the items Tweak the survey after import 2. Qualtrics Data to R Export the raw data from Qualtrics with maximal information in the data file Load the raw data into R Convert the raw data to analytic data. This conversion includes: assigning values to response options (e.g., “Strongly Disagree” to a numeric value), flipping response for reverse-worded (i.e., reverse-keyed) items, and creating scale scores. The overall steps are quite simple and I walk you through each step in detail below. 8.2 Required The files below are used in this chapter. Right click to save each file. Relevant files survey_codebook.csv items_qualtrics_format.txt data_qualtrics.csv script_qualtrics.R The following CRAN packages must be installed: Required CRAN Packages tidyverse janitor remotes The following GitHub packages must be installed: Required GitHub Packages dstanley4/qualtricsMaker A GitHub package can be installed using the code below: remotes::install_github(&quot;dstanley4/qualtricsMaker&quot;) 8.3 Items to Qualtrics There are two approaches to entering survey items into Qualtrics. Enter the items one at a time using the Qualtrics web interface. Create a text file in the Qualtrics Advanced Text Format and import the items. Be warned though a bit of tweaking is often still needed using the web interface. Here we focus only on the second approach to entering items into Qualtrics. 8.3.1 Create survey codebook The easy way to create items in the Advanced Text Format is to use the qualtricsMaker package. With this package you create a spreadsheet with the items and then convert that spreadsheet to the Advanced Text Format. Consider a scenario where we want to create a survey that contains 2 demographics items, 18 commitment items, and 4 job satisfaction items. The three-component model of commitment items are from (Meyer, Allen, and Smith 1993) and the job satisfaction items are from (Thompson and Phua 2012). We begin by creating a survey codebook spreadsheet in Excel (or similar program). The spreadsheet should have the columns with the names: block, item_name, item_text, type, response_options. The completed file ( survey_codebook.csv) is illustrated below. I provide a detailed description of what should be placed in each column after the figure. Remember to work from the guiding principle that one row is used for one item/question. 8.3.1.1 block column Enter any text you wish to use as a label for a block of items. 8.3.1.2 item_name column To make your life easier down the road, it is critical you set up your spreadsheet or online survey such that it uses a naming convention prior to data collection. Recall the naming convention from the previous chapter. 8.3.1.3 item_text column The item_text column contains the text for each item. Note that if you use commas in your item text do not save this file as a .csv file - it will not work. Rather save it as .tsv file (tab separated values). Then use read_tsv command instead of the read_csv command in the code that follows later. 8.3.1.4 type column Code for type column Qualtrics Item Type Additional Information matrix matrix If the first item in a block has type matrix all items in the block will be used to construct the matrix question. Unfortunately, importing item_names for matrix questions is not supported by Qualtrics. You will need to manually restore your item_names following the directions below for matrix items. MC multiple choice vertical format MC_horizontal multiple choice horizontal format MC_multi_horizontal multiple choice horizontal format multiple answers MC_select select box MC_multi_select select multiple boxes MC_dropdown dropdown 8.3.1.5 response_options column The Likert items for the different blocks use different response options. The commitment items use a 7-point response option whereas the job satisfaction items use a 5-point response option. 8.3.1.5.1 Year of birth We used a number of response options to indicate Year of Birth. We entered them as per below. Each response option is separated by a semi-colon. Each of the years below will be in the dropdown button. Entered in the response_option column as below. Each option separated by a semicolon. 1940;1941;1942;1943;1944;1945;1946;1947;1948;1949;1950;1951;1952;1953;1954;1955;1956;1957;1958;1959;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010 8.3.1.5.2 Sex 3 points Please note that in psychology we distinguish between sex and gender. We use the term “sex” to refer to underlying biology. In contrast, we use the term “gender” to refer a psychological construct (i.e., gender identity). A person’s sex and gender may be quite different. Depending on the theory used in a study, you might be interested in sex or gender or both. In this example, we are using sex but not gender. If you were asking about gender - you would have a much longer (and quite different) list of options. Entered in the response_option column as below. Each option separated by a semicolon. male; female; intersex 8.3.1.5.3 Commitment 7 points Entered in the response_option column as below. Each option separated by a semicolon. Strongly Disagree;Moderately Disagree;Slightly Disagree;Neither Agree nor Disagree;Slightly Agree;Moderately Agree;Strongly Agree 8.3.1.5.4 Job Satisfaction 5 points Entered in the response_option column as below. Each option separated by a semicolon. Strongly Disagree; Disagree; Neutral; Agree; Strongly Agree 8.3.2 Convert to Qualtrics format 8.3.2.1 Create TXT file Once you have the survey codebook above, you need to convert it to an Advanced Text Format .txt file. We can do so by following the steps below. The instructions differ slightly for RStudio Cloud and RStudio - both are presented. 8.3.2.1.1 RStudio on your computer Create a folder on your computer called “new_survey” with the survey_codebook.csv file in it. Open RStudio. Then go to File &gt; New Project… and select Existing Directory. Then find and select the “new_survey” folder on your hard drive and click the Create Project button. You should previously have installed the remotes and tidyverse packages from the CRAN and the qualtricsMaker package from GitHub. If you haven’t done this already type the commands below into the Console. Installing Packages from the CRAN install.packages(&quot;tidyverse&quot;, dep = TRUE) install.packages(&quot;remotes&quot;, dep = TRUE) Installing Package from GitHub Then install the qualtricsMaker package from GitHub: remotes::install_github(&quot;dstanley4/qualtricsMaker&quot;) Create a new script by going to File &gt; New File &gt; R Script. A new script will appear in RStudio. Immediately press Control-S to save the script. Call it “script_convert.R”. Place the code below into the script and press Control-S to save your work. library(tidyverse) library(qualtricsMaker) survey_codebook &lt;- read_csv(&quot;survey_codebook.csv&quot;, show_col_types = FALSE) make_survey(survey_codebook, filename = &quot;items_qualtrics_format.txt&quot;) Run the script by pressing the Source button in the top right of the the Script window. Doing so will create the items_qualtrics_format.txt file that you will import into Qualtrics. You can see the file after it has been created in the Files tab below. Congratulations - your items are ready to be imported into Qualtrics. Move on to the “Import the items” section. 8.3.2.1.2 RStudio Cloud Open a new project in RStudio Cloud You should previously have installed the remote and tidyverse package from the CRAN and the qualtricsMaker package from GitHub. If you haven’t done this already type the commands below into the Console.However, if you are working from a class project I created these packages will have been installed already for you. install.packages(&quot;tidyverse&quot;, dep = TRUE) install.packages(&quot;remotes&quot;, dep = TRUE) Then install the qualtricsMaker package from GitHub: remotes::install_github(&quot;dstanley4/qualtricsMaker&quot;) Import the survey_codebook.csv file by using the Upload button in the Files tab in the lower right panel. Click the Choose File button and select the survey_codebook.csv file on your hard drive. Then click OK. The survey_codebook.csv file will then appear on the Files tab within RStudio Cloud. Create a new script by going to File &gt; New File &gt; R Script. A new script will appear in RStudio. Immediately press Control-S to save the script. Call it “script_convert.R”. Place the code below into the script and press Control-S to save your work. library(tidyverse) library(qualtricsMaker) survey_codebook &lt;- read_csv(&quot;survey_codebook.csv&quot;, show_col_types = FALSE) make_survey(survey_codebook, filename = &quot;items_qualtrics_format.txt&quot;) Run the script by pressing the Source button in the top right of the the Script window. Doing so will create the items_qualtrics_format.txt file that you will import into Qualtrics. You can see the file after it has been created in the Files tab below. Select the checkbox beside the file you just created: items_qualtrics_format.txt Select Export from the More menu. Leave the filename as it appears and click the Download button. The file will appear in the Downloads directory on your computer. Move the file (items_qualtrics_format.txt) from the Download folder to an easy to access place (e.g. the Desktop). Congratulation - your items are ready to be imported into Qualtrics. Move on to the “Import the items” section. 8.3.3 Import the items In the previous step you created items_qualtrics_format.txt. This file is illustrated below - well the middle of the file is illustrated below. You can think of the file created by the survey_maker command above as a starting point. You could open up this file in a text editor and make further modifications or additions based on learning the Advanced Text Format. For example, you might not want the matrix items to be in their own block. If so, you would simply remove the block text [[Block:Commitment]] from the text file prior to following the directions below. If you’re a Mac user you might find the text editor BBEdit helpful for this purpose – the free version is sufficient. After you download BBEdit you have access to all of its features. Once the free trial ends - the core features are still free. You only need to purchase it if you want the advanced features (and you won’t need the advanced features). 8.3.3.0.1 Begin the import by logging into Qualtrics Begin by logging into Qualtrics via the University of Guelph site for Data Collection &amp; Surveys. After you log in, continue below. 8.3.3.0.2 Click the Create New Project button in the top right of the page. 8.3.3.1 Under Projects from Scratch select “Survey” by clicking on it. 8.3.3.2 Click the Get Started button in the lower right of the page. 8.3.3.3 Enter project name, click Create Project 8.3.3.4 Use the Tools menu to import the items Select the Import option and then specify the filename (items_qualtrics_format.txt) for the Advanced Text import file with the items in it. Congratulation - your items have been imported into Qualtrics. 8.3.3.5 Remove the default Empty Question block at the top of the survey Scroll to the top of the survey and you will see a Default Question Block above your first block. Click on the three dots “…” in the top right of this block and select Delete to remove it. 8.3.4 Tweak survey in Qualtrics Now you need to tweak the survey using the Qualtrics interface. Recall that when you import items into Qualtrics, matrix-style questions will loose their item names. You need to manually restore the item names for matrix questions now. Begin by scrolling down to a block of matrix-style questions. Select the block so it gets a blue rectangle around it. On the left side menu go the “Question Behaviour” menu and select “Recode values” Click the Export Question Tags check box. You will see the default names for the items (instead of the ones you indicated in the codebook). Click the Close button in the lower right when you’re done. Manually restore the item names from the survey codebook by clicking the blue name for each item and fixing it. Don’t forget to use proper item naming formats and include the reverse-key status of each item. Collect your data! 8.4 Qualtrics data to R Following data collection, you can obtain your data from Qualtrics. As of 2021, Tri-Agency (NSERC, SSHRC, CIHR) policy is that data collected with Tri-Agency funded research must be available for reuse by others. The data should follow the FAIR (Findable, Accessible, Interoperable, and Reusable) principle. Consequently, when you export the data from Qualtrics (and eventually post it) we want to ensure it has as much information in it as possible. This principle guides the options we select below. 8.4.1 Export from Qualtrics 8.4.1.1 Click the Data &amp; Analysis tab 8.4.1.2 Click the Export &amp; Import button Then click the Export Data… option. 8.4.1.3 Click Use Choice Text To ensure the data can be used by others, as per the FAIR policy described above, we SELECT the “Use Choice Text” option when exporting data. If you were to (non-optimally) select “Use numeric value” as an export option then the resulting data file would be missing information that make it difficult for others to use. Then click the Download button. The file will appear in your downloads directory. It will likely be a zipped filed. Unzip the file before proceeding. On a Mac, you can just double click the file to do so. Rename the file to data_qualtrics.csv 8.4.2 Load data in R To load the data in R follow the steps below Create a new folder called “survey_data” Drag the data_qualtrics.csv file to the survey_data folder. Use the steps described previously on this page, use RStudio to create project for this data. You may be tempted to load the data using the usual read_csv() process. This will be problematic due to the way the data is formatted in the .csv file. (see image below.) You can see by inspecting the data screenshot above that a variety of columns have been included before our first data column (year_of_birth) which appears on the far right of screenshot. Also note that there are three header rows at the top of the file before the data which starts on row 4. So there are few problems that need to be addressed when we load data from Qualtrics. Obtain the names from the first header row but read the data from row 4 on. Remove some, or all, of the additional columns on the left side of data set added by Qualtrics Ensure our variable names follow the tidyverse style guide Ensure we DO NOT modify the data_qualtrics.csv file in Excel or other program. That would destroy the reproducibility of our work in the very first step. 8.4.3 Convert to analytic data The issues described above are all solved with the script below. Additionally, in the script we also: Convert categorical variable to factors Screen both numeric variables and factors for impossible values Convert Likert items from labels (e.g., “Strongly Disagree”) to numbers (e.g., 1) Flip reverse-key (i.e., reverse worded items) so they are scored in the right direction Combine items (e.g., aff_com1_likert7) for the same measure into a single scale score (e.g., affective_commitment) Remove item columns (e.g., aff_com1_likert7) after that have been combined into the scale score while retaining the scale score column column (e.g., affective_commitment) 8.4.3.1 Load Data # Date: YYYY-MM-DD # Name: your name here # Example: Single occasion survey # Load data library(tidyverse) library(janitor) library(skimr) # read the file in the normal way and get just the names of the columns. # put the names into the col_names survey_file &lt;- &quot;data_qualtrics.csv&quot; col_names &lt;- names(read_csv(survey_file, n_max = 0, show_col_types = FALSE)) # skip the first 3 rows than read in the data and use names from above raw_data &lt;- read_csv(survey_file, col_names = col_names, skip = 3, show_col_types = FALSE) Remove the Qualtrics columns you probably don’t want: # list of Qualtrics columns you probably don&#39;t want cols_to_maybe_remove &lt;- c(&quot;StartDate&quot;, &quot;EndDate&quot;, &quot;Status&quot;, &quot;IPAddress&quot;, &quot;Progress&quot;, &quot;Duration (in seconds)&quot;, &quot;Finished&quot;, &quot;RecordedDate&quot;, &quot;ResponseId&quot;, &quot;RecipientLastName&quot;, &quot;RecipientFirstName&quot;, &quot;RecipientEmail&quot;, &quot;ExternalReference&quot;, &quot;LocationLatitude&quot;, &quot;LocationLongitude&quot;, &quot;DistributionChannel&quot;, &quot;UserLanguage&quot;) # check to see overlap with actual column names in data file # put names to remove (that are present) into cols_to_remove cols_to_remove_id &lt;- col_names %in% cols_to_maybe_remove cols_to_remove &lt;- col_names[cols_to_remove_id] # remove the unwanted Qualtrics columns raw_data &lt;- raw_data %&gt;% select(!all_of(cols_to_remove)) We make a copy of the data called analytic_data_survey since the goals of the next several steps are to prepare the data for analysis. analytic_data_survey &lt;- raw_data In the command below, we ensure our column names follow the tidyverse style guide by using the clean_names() command from the janitor package. Additionally, we remove empty row and columns from the data using the remove_empty_cols() and remove_empty_rows(), respectively. # Initial cleaning ## Convert column names to tidyverse style guide ## Remove empty rows and columns analytic_data_survey &lt;- analytic_data_survey %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 24 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953,… ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;,… ## $ aff_com1_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Neither Ag… ## $ aff_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly D… ## $ aff_com3_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com4_likert7rev &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Stron… ## $ aff_com5_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com6_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Strongly… ## $ contin_com1_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Neither Ag… ## $ contin_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com3_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Slightl… ## $ contin_com4_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Neither … ## $ contin_com5_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com6_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com1_likert7rev &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com2_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com3_likert7 &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com4_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com5_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ norm_com6_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly D… ## $ job_aff1_likert5 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly A… ## $ job_aff2_likert5 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ job_aff3_likert5 &lt;chr&gt; &quot;Neutral&quot;, &quot;Strongly Disagree… ## $ job_aff4_likert5 &lt;chr&gt; &quot;Disagree&quot;, &quot;Strongly Agree&quot;,… 8.4.3.2 Creating factors Following initial cleaning, we identify categorical variables as factors. glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 24 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953,… ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;,… ## $ aff_com1_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Neither Ag… ## $ aff_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly D… ## $ aff_com3_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com4_likert7rev &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Stron… ## $ aff_com5_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com6_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Strongly… ## $ contin_com1_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Neither Ag… ## $ contin_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com3_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Slightl… ## $ contin_com4_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Neither … ## $ contin_com5_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com6_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com1_likert7rev &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com2_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com3_likert7 &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com4_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com5_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ norm_com6_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly D… ## $ job_aff1_likert5 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly A… ## $ job_aff2_likert5 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ job_aff3_likert5 &lt;chr&gt; &quot;Neutral&quot;, &quot;Strongly Disagree… ## $ job_aff4_likert5 &lt;chr&gt; &quot;Disagree&quot;, &quot;Strongly Agree&quot;,… 8.4.3.2.1 Sex There is only one variable that we will convert to a factor and that’s the sex variable. We convert it to a factor with the command below. # Convert variables to factors as needed ## Convert sex to factor analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = as_factor(sex)) 8.4.3.2.2 Participant identification In many analyses we will need a column with a unique identification number that is a factor. We don’t see one in this data set so we create it with the name participant_id. ## Participant identification to factor ## Participant identification column must be created first ## get the number of rows in the data set N &lt;- dim(analytic_data_survey)[1] ## create set of factor levels participant_id_levels &lt;- factor(seq(1, N)) ## put the factor levels into a column called participant_id analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(participant_id = participant_id_levels) You can ensure all of these columns are now factors using the glimpse() command. analytic_data_survey %&gt;% select(sex, participant_id) %&gt;% glimpse() ## Rows: 150 ## Columns: 2 ## $ sex &lt;fct&gt; female, female, female, male, inter… ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, … 8.4.3.3 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels for sex: # Screen factors ## screen analytic_data_survey %&gt;% select(sex) %&gt;% summary() ## sex ## female :43 ## male :49 ## intersex:58 You can see in the above output there are three levels for sex in this data set: female, male and intersex. You can also see beside each of these labels the number of participants for each category. Notice the order of the factor levels: female, male, and intersex. This will be the order they appear in graphs and tables. We might want another order. If we did want another order, we could use the code below to establish the custom order for graphs and tables: female, intersex, male. Of course, you can use any order you want that makes sense for the context and your research question. ## change to desired order analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;intersex&quot;, &quot;male&quot;)) 8.4.3.4 Numeric screening For numeric variables, it’s important to find and remove impossible values. We only have year_of_birth as a numeric column - so we can easily check for nonsense values with the code below: # Screen numeric variables analytic_data_survey %&gt;% select(year_of_birth) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 year_of_birth 0 1974 20.06 1940 1970 2010 8.4.3.5 Item values from labels Recall that when we obtained our data from Qualtrics we selected “Use Choice Text”. That means that, as desired, we didn’t get values for our Likert items - we obtained labels. Now we need to switch the Likert responses to numeric values. The advantage of handling our data in this way is that anyone examining our data and code in an open access repository can see exactly what options survey participants were provided with for each question. But prior to beginning check out the type for each Likert column below using the glimpse() command notice that are all of type “chr”. analytic_data_survey %&gt;% glimpse() ## Rows: 150 ## Columns: 25 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953,… ## $ sex &lt;fct&gt; female, female, female, male,… ## $ aff_com1_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Neither Ag… ## $ aff_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly D… ## $ aff_com3_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com4_likert7rev &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Stron… ## $ aff_com5_likert7rev &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Neither… ## $ aff_com6_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Strongly… ## $ contin_com1_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Neither Ag… ## $ contin_com2_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com3_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Slightl… ## $ contin_com4_likert7 &lt;chr&gt; &quot;Moderately Agree&quot;, &quot;Neither … ## $ contin_com5_likert7 &lt;chr&gt; &quot;Slightly Agree&quot;, &quot;Slightly A… ## $ contin_com6_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com1_likert7rev &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com2_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com3_likert7 &lt;chr&gt; &quot;Neither Agree nor Disagree&quot;,… ## $ norm_com4_likert7 &lt;chr&gt; &quot;Moderately Disagree&quot;, &quot;Moder… ## $ norm_com5_likert7 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ norm_com6_likert7 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly D… ## $ job_aff1_likert5 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly A… ## $ job_aff2_likert5 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ job_aff3_likert5 &lt;chr&gt; &quot;Neutral&quot;, &quot;Strongly Disagree… ## $ job_aff4_likert5 &lt;chr&gt; &quot;Disagree&quot;, &quot;Strongly Agree&quot;,… ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10… When you have type chr for a column the computer sees the content of each column as text that is different for each participant. It doesn’t even realize that two people who both responded “Slightly Agree” responded the same way. It sees every response in the column as unique. Check that out for the aff_com1_likert7 item. analytic_data_survey %&gt;% select(aff_com1_likert7) %&gt;% summary() ## aff_com1_likert7 ## Length:150 ## Class :character ## Mode :character 8.4.3.5.1 Commitment Items To convert a column from a character column to factor column we need know the exact labels used in that column. We can obtain those with the code below. In this code, we use just one item aff_com1_likert7. We determine the response options for that one item and will apply them to all the commitment items. We use the pull() command to obtain all the participant responses from the aff_com1_likert7. column. Then we use the unique() command to give use just one instance of each response. # Convert Commitment items to numeric values ## Check levels for a likert7 item analytic_data_survey %&gt;% pull(aff_com1_likert7) %&gt;% unique() ## [1] &quot;Slightly Agree&quot; ## [2] &quot;Neither Agree nor Disagree&quot; ## [3] &quot;Moderately Disagree&quot; ## [4] &quot;Strongly Agree&quot; ## [5] &quot;Strongly Disagree&quot; ## [6] &quot;Moderately Agree&quot; ## [7] &quot;Slightly Disagree&quot; Now that we know the labels used in that column (and all the commitment columns that end in likert7) we create an ordered version of those labels with the code below. I copied and pasted the text (e.g., “Strongly Disagree”) from the above output to avoid typos. ## write code to create ordered factor labels/levels likert7_factor &lt;- c(&quot;Strongly Disagree&quot;, &quot;Moderately Disagree&quot;, &quot;Slightly Disagree&quot;, &quot;Neither Agree nor Disagree&quot;, &quot;Slightly Agree&quot;, &quot;Moderately Agree&quot;, &quot;Strongly Agree&quot;) Now we turn each column of text (i.e., type chr) into a factor column using the factor levels/labels we created: ## assign factor levels analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = ~ factor(.x, levels = likert7_factor))) In this code we use the mutate() command to modify columns. We use the across() command to have the single mutate command apply to several columns. Within the across() command we use contains(“likert7”) to apply the mutate() command only to columns with likert7 in the name. You can see why column naming conventions are so important! Next, within the across() command, we specify .fns. This indicate the function/command that will apply to the columns selected. The ~ before “factor” just warns the computer that what comes next is a function/command name. The factor() command is what will be applied to the columns. The .x within the factor command is a placeholder for each column name that will be obtained by the previous contains() command. After this mutate() command is used you can see the columns with likert7 in the name are now factors. glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 25 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953,… ## $ sex &lt;fct&gt; female, female, female, male,… ## $ aff_com1_likert7 &lt;fct&gt; Slightly Agree, Neither Agree… ## $ aff_com2_likert7 &lt;fct&gt; Slightly Agree, Slightly Disa… ## $ aff_com3_likert7rev &lt;fct&gt; Strongly Disagree, Neither Ag… ## $ aff_com4_likert7rev &lt;fct&gt; Moderately Disagree, Strongly… ## $ aff_com5_likert7rev &lt;fct&gt; Strongly Disagree, Neither Ag… ## $ aff_com6_likert7 &lt;fct&gt; Moderately Agree, Strongly Ag… ## $ contin_com1_likert7 &lt;fct&gt; Strongly Agree, Neither Agree… ## $ contin_com2_likert7 &lt;fct&gt; Slightly Agree, Slightly Agre… ## $ contin_com3_likert7 &lt;fct&gt; Strongly Disagree, Slightly D… ## $ contin_com4_likert7 &lt;fct&gt; Moderately Agree, Neither Agr… ## $ contin_com5_likert7 &lt;fct&gt; Slightly Agree, Slightly Agre… ## $ contin_com6_likert7 &lt;fct&gt; Moderately Disagree, Moderate… ## $ norm_com1_likert7rev &lt;fct&gt; Neither Agree nor Disagree, S… ## $ norm_com2_likert7 &lt;fct&gt; Moderately Disagree, Moderate… ## $ norm_com3_likert7 &lt;fct&gt; Neither Agree nor Disagree, N… ## $ norm_com4_likert7 &lt;fct&gt; Moderately Disagree, Moderate… ## $ norm_com5_likert7 &lt;fct&gt; Strongly Disagree, Strongly D… ## $ norm_com6_likert7 &lt;fct&gt; Strongly Agree, Strongly Disa… ## $ job_aff1_likert5 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly A… ## $ job_aff2_likert5 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ job_aff3_likert5 &lt;chr&gt; &quot;Neutral&quot;, &quot;Strongly Disagree… ## $ job_aff4_likert5 &lt;chr&gt; &quot;Disagree&quot;, &quot;Strongly Agree&quot;,… ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10… Now examine aff_com1_likert7 again - you can see it has levels: analytic_data_survey %&gt;% select(aff_com1_likert7) %&gt;% summary() ## aff_com1_likert7 ## Strongly Disagree :21 ## Moderately Disagree :21 ## Slightly Disagree :21 ## Neither Agree nor Disagree:18 ## Slightly Agree :28 ## Moderately Agree :21 ## Strongly Agree :20 The second step in our conversion process is converting each column from a factor to the numeric value associated with the factor level (i.e. Strongly Disagree becomes 1). We do so with the code below: ## covert factor levels to numeric values analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = as.numeric)) You can see there are numeric values with the code below. Notice the column types for likert7 items is now dbl (short for double - a type of high precision number.) glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 25 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953,… ## $ sex &lt;fct&gt; female, female, female, male,… ## $ aff_com1_likert7 &lt;dbl&gt; 5, 4, 2, 7, 1, 6, 6, 2, 4, 2,… ## $ aff_com2_likert7 &lt;dbl&gt; 5, 3, 4, 2, 6, 7, 2, 3, 2, 7,… ## $ aff_com3_likert7rev &lt;dbl&gt; 1, 4, 2, 5, 7, 2, 6, 3, 5, 4,… ## $ aff_com4_likert7rev &lt;dbl&gt; 2, 7, 6, 6, 3, 5, 7, 6, 7, 2,… ## $ aff_com5_likert7rev &lt;dbl&gt; 1, 4, 6, 5, 3, 7, 5, 7, 5, 5,… ## $ aff_com6_likert7 &lt;dbl&gt; 6, 7, 1, 7, 5, 6, 5, 7, 4, 2,… ## $ contin_com1_likert7 &lt;dbl&gt; 7, 4, 1, 3, 2, 1, 1, 6, 7, 3,… ## $ contin_com2_likert7 &lt;dbl&gt; 5, 5, 3, 2, 7, 6, 1, 3, 5, 2,… ## $ contin_com3_likert7 &lt;dbl&gt; 1, 3, 7, 7, 5, 2, 7, 2, 7, 1,… ## $ contin_com4_likert7 &lt;dbl&gt; 6, 4, 3, 7, 7, 5, 4, 5, 3, 2,… ## $ contin_com5_likert7 &lt;dbl&gt; 5, 5, 1, 4, 4, 6, 5, 5, 5, 2,… ## $ contin_com6_likert7 &lt;dbl&gt; 2, 2, 1, 4, 7, 7, 6, 6, 4, 5,… ## $ norm_com1_likert7rev &lt;dbl&gt; 4, 3, 6, 3, 3, 6, 5, 7, 3, 7,… ## $ norm_com2_likert7 &lt;dbl&gt; 2, 6, 1, 5, 3, 2, 3, 1, 7, 1,… ## $ norm_com3_likert7 &lt;dbl&gt; 4, 4, 7, 3, 1, 1, 4, 3, 3, 5,… ## $ norm_com4_likert7 &lt;dbl&gt; 2, 2, 1, 6, 4, 6, 1, 1, 5, 5,… ## $ norm_com5_likert7 &lt;dbl&gt; 1, 1, 1, 5, 7, 4, 3, 6, 4, 4,… ## $ norm_com6_likert7 &lt;dbl&gt; 7, 1, 2, 4, 6, 6, 1, 4, 6, 2,… ## $ job_aff1_likert5 &lt;chr&gt; &quot;Strongly Agree&quot;, &quot;Strongly A… ## $ job_aff2_likert5 &lt;chr&gt; &quot;Strongly Disagree&quot;, &quot;Strongl… ## $ job_aff3_likert5 &lt;chr&gt; &quot;Neutral&quot;, &quot;Strongly Disagree… ## $ job_aff4_likert5 &lt;chr&gt; &quot;Disagree&quot;, &quot;Strongly Agree&quot;,… ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10… If you run the summary() command on the aff_com1_likert7 you see we have values now. analytic_data_survey %&gt;% select(aff_com1_likert7) %&gt;% summary() ## aff_com1_likert7 ## Min. :1.00 ## 1st Qu.:2.00 ## Median :4.00 ## Mean :4.03 ## 3rd Qu.:6.00 ## Max. :7.00 Side note: We did a lot of explaining as we went through the steps above. But it’s not as long as it seems. Some of the steps can even be combined. See the concise version of the code we used repeated below: # Convert Commitment items to numeric values ## Check levels for a likert7 item analytic_data_survey %&gt;% pull(aff_com1_likert7) %&gt;% unique() ## write code to create ordered factor labels/levels likert7_factor &lt;- c(&quot;Strongly Disagree&quot;, &quot;Moderately Disagree&quot;, &quot;Slightly Disagree&quot;, &quot;Neither Agree nor Disagree&quot;, &quot;Slightly Agree&quot;, &quot;Moderately Agree&quot;, &quot;Strongly Agree&quot;) ## assign factor levels and then covert to numeric analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = ~ factor(.x, levels = likert7_factor))) %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = as.numeric)) 8.4.3.5.2 Job Satisfaction Conversion We’ll skip all the explanation steps for Job Satisfaction and just show you the essentials. First, get the labels for the items: # Convert Job Satisfaction items to numeric values ## Check levels for a likert5 item analytic_data_survey %&gt;% pull(job_aff1_likert5) %&gt;% unique() ## [1] &quot;Strongly Agree&quot; &quot;Neutral&quot; ## [3] &quot;Strongly Disagree&quot; &quot;Agree&quot; ## [5] &quot;Disagree&quot; Second, create the ordered factor labels: ## write code to create ordered factor labels/levels likert5_factor &lt;- c(&quot;Strongly Disagree&quot;, &quot;Disagree&quot;, &quot;Neutral&quot;, &quot;Agree&quot;, &quot;Strongly Agree&quot;) Third, convert the columns to a factor and then to numbers. ## assign factor levels and then covert to numeric analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert5&quot;), .fns = ~ factor(.x, levels = likert5_factor))) %&gt;% mutate(across(.cols = contains(&quot;likert5&quot;), .fns = as.numeric)) 8.4.3.6 Scale scores For each person, scale scores involve averaging scores from several items to create an overall score. The first step in the creation of scales is correcting the values of any reverse-keyed items. 8.4.3.6.1 Reverse-key items A complete discussion of the logic for flipping reverse-key (i.e., reverse worded) items is provide the previous chapter in the Scale Scores section. We use that logic in the script below. # Reverse key items ## Reverse key likert7 items analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(8 - across(.cols = ends_with(&quot;_likert7rev&quot;)) ) %&gt;% rename_with(.fn = str_replace, .cols = ends_with(&quot;_likert7rev&quot;), pattern = &quot;_likert7rev&quot;, replacement = &quot;_likert7&quot;) You can see that are all keyed in the right direction now. glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 25 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 1953, … ## $ sex &lt;fct&gt; female, female, female, male, … ## $ aff_com1_likert7 &lt;dbl&gt; 5, 4, 2, 7, 1, 6, 6, 2, 4, 2, … ## $ aff_com2_likert7 &lt;dbl&gt; 5, 3, 4, 2, 6, 7, 2, 3, 2, 7, … ## $ aff_com3_likert7 &lt;dbl&gt; 7, 4, 6, 3, 1, 6, 2, 5, 3, 4, … ## $ aff_com4_likert7 &lt;dbl&gt; 6, 1, 2, 2, 5, 3, 1, 2, 1, 6, … ## $ aff_com5_likert7 &lt;dbl&gt; 7, 4, 2, 3, 5, 1, 3, 1, 3, 3, … ## $ aff_com6_likert7 &lt;dbl&gt; 6, 7, 1, 7, 5, 6, 5, 7, 4, 2, … ## $ contin_com1_likert7 &lt;dbl&gt; 7, 4, 1, 3, 2, 1, 1, 6, 7, 3, … ## $ contin_com2_likert7 &lt;dbl&gt; 5, 5, 3, 2, 7, 6, 1, 3, 5, 2, … ## $ contin_com3_likert7 &lt;dbl&gt; 1, 3, 7, 7, 5, 2, 7, 2, 7, 1, … ## $ contin_com4_likert7 &lt;dbl&gt; 6, 4, 3, 7, 7, 5, 4, 5, 3, 2, … ## $ contin_com5_likert7 &lt;dbl&gt; 5, 5, 1, 4, 4, 6, 5, 5, 5, 2, … ## $ contin_com6_likert7 &lt;dbl&gt; 2, 2, 1, 4, 7, 7, 6, 6, 4, 5, … ## $ norm_com1_likert7 &lt;dbl&gt; 4, 5, 2, 5, 5, 2, 3, 1, 5, 1, … ## $ norm_com2_likert7 &lt;dbl&gt; 2, 6, 1, 5, 3, 2, 3, 1, 7, 1, … ## $ norm_com3_likert7 &lt;dbl&gt; 4, 4, 7, 3, 1, 1, 4, 3, 3, 5, … ## $ norm_com4_likert7 &lt;dbl&gt; 2, 2, 1, 6, 4, 6, 1, 1, 5, 5, … ## $ norm_com5_likert7 &lt;dbl&gt; 1, 1, 1, 5, 7, 4, 3, 6, 4, 4, … ## $ norm_com6_likert7 &lt;dbl&gt; 7, 1, 2, 4, 6, 6, 1, 4, 6, 2, … ## $ job_aff1_likert5 &lt;dbl&gt; 5, 5, 3, 1, 1, 3, 4, 4, 1, 4, … ## $ job_aff2_likert5 &lt;dbl&gt; 1, 5, 5, 2, 3, 1, 3, 3, 5, 5, … ## $ job_aff3_likert5 &lt;dbl&gt; 3, 1, 2, 5, 5, 3, 3, 1, 5, 2, … ## $ job_aff4_likert5 &lt;dbl&gt; 2, 5, 4, 1, 5, 3, 1, 3, 3, 5, … ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,… None of the job satisfaction items were reverse-keyed. 8.4.3.6.2 Creating scores Important: Take note of how we name the scale variables (e.g., affective_commitment) with a different prefix than the items (e..g, aff_com1_likert7). This allows us to delete the items (i.e., columns that start with aff_com) but not the final scale score (i.e., a column starts with affective_commitment). This example again illustrates how carefully you need to think about your column and scale naming conventions. # Create scale scores ## mutate commands create scale scores ## select commands with &quot;-&quot; remove items after scale creation analytic_data_survey &lt;- analytic_data_survey %&gt;% rowwise() %&gt;% mutate(affective_commitment = mean(c_across(starts_with(&quot;aff_com&quot;)), na.rm = TRUE)) %&gt;% mutate(continuance_commitment = mean(c_across(starts_with(&quot;contin_com&quot;)), na.rm = TRUE)) %&gt;% mutate(normative_commitment = mean(c_across(starts_with(&quot;norm_com&quot;)), na.rm = TRUE)) %&gt;% mutate(job_satisfaction = mean(c_across(starts_with(&quot;job_aff&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;aff_com&quot;)) %&gt;% select(-starts_with(&quot;contin_com&quot;)) %&gt;% select(-starts_with(&quot;norm_com&quot;)) %&gt;% select(-starts_with(&quot;job_aff&quot;)) We can see our data now has the columns: affective_commitment, continuance_commitment, normative_commitment, job satisfaction. As well, all of the items used to create the scale scores have been removed. glimpse(analytic_data_survey) ## Rows: 150 ## Columns: 7 ## $ year_of_birth &lt;dbl&gt; 1961, 2006, 2008, 1985, 195… ## $ sex &lt;fct&gt; female, female, female, mal… ## $ participant_id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, … ## $ affective_commitment &lt;dbl&gt; 6.000, 3.833, 2.833, 4.000,… ## $ continuance_commitment &lt;dbl&gt; 4.333, 3.833, 2.667, 4.500,… ## $ normative_commitment &lt;dbl&gt; 3.333, 3.167, 2.333, 4.667,… ## $ job_satisfaction &lt;dbl&gt; 2.75, 4.00, 3.50, 2.25, 3.5… 8.4.4 Complete script The complete script script_qualtrics.R for the data file data_qualtrics.csv is presented below. # Date: YYYY-MM-DD # Name: your name here # Example: Single occasion survey # Load data library(tidyverse) library(janitor) library(skimr) # read the file in the normal way and get just the names of the columns. # put the names into the col_names survey_file &lt;- &quot;data_qualtrics.csv&quot; col_names &lt;- names(read_csv(survey_file, n_max = 0, show_col_types = FALSE)) # skip the first 3 rows than read in the data and use names from above raw_data &lt;- read_csv(survey_file, col_names = col_names, skip = 3, show_col_types = FALSE) # list of Qualtrics columns you probably don&#39;t want cols_to_maybe_remove &lt;- c(&quot;StartDate&quot;, &quot;EndDate&quot;, &quot;Status&quot;, &quot;IPAddress&quot;, &quot;Progress&quot;, &quot;Duration (in seconds)&quot;, &quot;Finished&quot;, &quot;RecordedDate&quot;, &quot;ResponseId&quot;, &quot;RecipientLastName&quot;, &quot;RecipientFirstName&quot;, &quot;RecipientEmail&quot;, &quot;ExternalReference&quot;, &quot;LocationLatitude&quot;, &quot;LocationLongitude&quot;, &quot;DistributionChannel&quot;, &quot;UserLanguage&quot;) # check to see overlap with actual column names in data file # put names to remove (that are present) into cols_to_remove cols_to_remove_id &lt;- col_names %in% cols_to_maybe_remove cols_to_remove &lt;- col_names[cols_to_remove_id] # remove the unwanted Qualtrics columns raw_data &lt;- raw_data %&gt;% select(!all_of(cols_to_remove)) analytic_data_survey &lt;- raw_data # Initial cleaning ## Convert column names to tidyverse style guide ## Remove empty rows and columns analytic_data_survey &lt;- analytic_data_survey %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() glimpse(analytic_data_survey) # Convert variables to factors as needed ## Convert sex to factor analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = as_factor(sex)) ## Participant identification to factor ## Participant identification column must be created first ## get the number of rows in the data set N &lt;- dim(analytic_data_survey)[1] ## create set of factor levels participant_id_levels &lt;- factor(seq(1, N)) ## put the factor levels into a column called participant_id analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(participant_id = participant_id_levels) # Screen factors ## screen analytic_data_survey %&gt;% select(sex) %&gt;% summary() ## change to desired order analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;intersex&quot;, &quot;male&quot;)) # Screen numeric variables analytic_data_survey %&gt;% select(year_of_birth) %&gt;% skim() # Convert Commitment items to numeric values ## Check levels for a likert7 item analytic_data_survey %&gt;% pull(aff_com1_likert7) %&gt;% unique() ## write code to create ordered factor labels/levels likert7_factor &lt;- c(&quot;Strongly Disagree&quot;, &quot;Moderately Disagree&quot;, &quot;Slightly Disagree&quot;, &quot;Neither Agree nor Disagree&quot;, &quot;Slightly Agree&quot;, &quot;Moderately Agree&quot;, &quot;Strongly Agree&quot;) ## assign factor levels and then covert to numeric analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = ~ factor(.x, levels = likert7_factor))) %&gt;% mutate(across(.cols = contains(&quot;likert7&quot;), .fns = as.numeric)) # Convert Job Satisfaction items to numeric values ## Check levels for a likert5 item analytic_data_survey %&gt;% pull(job_aff1_likert5) %&gt;% unique() ## write code to create ordered factor labels/levels likert5_factor &lt;- c(&quot;Strongly Disagree&quot;, &quot;Disagree&quot;, &quot;Neutral&quot;, &quot;Agree&quot;, &quot;Strongly Agree&quot;) ## assign factor levels and then covert to numeric analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = contains(&quot;likert5&quot;), .fns = ~ factor(.x, levels = likert5_factor))) %&gt;% mutate(across(.cols = contains(&quot;likert5&quot;), .fns = as.numeric)) # Reverse key items ## Reverse key likert7 items analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(8 - across(.cols = ends_with(&quot;_likert7rev&quot;)) ) %&gt;% rename_with(.fn = str_replace, .cols = ends_with(&quot;_likert7rev&quot;), pattern = &quot;_likert7rev&quot;, replacement = &quot;_likert7&quot;) ## No likert5 items are reverse-keyed but if they were ## You would adapt the code above replacing 8 (one higher than 7) to 6 (one higher than 5) # Create scale scores ## mutate commands create scale scores ## select commands with &quot;-&quot; remove items after scale creation analytic_data_survey &lt;- analytic_data_survey %&gt;% rowwise() %&gt;% mutate(affective_commitment = mean(c_across(starts_with(&quot;aff_com&quot;)), na.rm = TRUE)) %&gt;% mutate(continuance_commitment = mean(c_across(starts_with(&quot;contin_com&quot;)), na.rm = TRUE)) %&gt;% mutate(normative_commitment = mean(c_across(starts_with(&quot;norm_com&quot;)), na.rm = TRUE)) %&gt;% mutate(job_satisfaction = mean(c_across(starts_with(&quot;job_aff&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;aff_com&quot;)) %&gt;% select(-starts_with(&quot;contin_com&quot;)) %&gt;% select(-starts_with(&quot;norm_com&quot;)) %&gt;% select(-starts_with(&quot;job_aff&quot;)) glimpse(analytic_data_survey) 8.5 Extra Help 8.5.1 Qualtrics University of Guelph Qualtrics Login page University of Guelph Qualtrics FAQ page 8.5.2 Surveys Dillman Method book can be found here Duke University Survey site References "],["effect-sizes-from-publications.html", "Chapter 9 Effect sizes from publications 9.1 Required 9.2 Objective 9.3 Estimating \\(\\delta\\) 9.4 Estimating \\(\\rho\\)", " Chapter 9 Effect sizes from publications 9.1 Required The following CRAN packages must be installed: Required CRAN Packages MBESS cocor 9.2 Objective In this chapter we focus on obtaining effect size estimates (e.g., correlation or standardized mean difference) from published articles. Often studies do not report effect sizes (e.g., \\(d\\)-values) or if they do, they don’t report the confidence interval for that effect. We will learn more about confidence intervals in a future chapter, but for now, think of a confidence interval for a study/sample effect (e.g., \\(d\\)-value) as providing a plausible range of values for the population parameter (e.g., \\(\\delta\\)). Below we learn the R commands needed to obtain effect sizes with confidence intervals from published articles. The ability to obtain effect size estimate from articles will be come extraordinarily important later in the course. Specifically, when it is time to engage in sample size planning (e.g., for your thesis) you will need to obtain estimates of the effects you are interested in from past research. The commands in this chapter will help you to do so. 9.3 Estimating \\(\\delta\\) 9.3.1 Independent Groups When conducting an independent-groups t-test there are a few different ways to calculate \\(d\\)-values. We focus on two approaches in this chapter. First, researchers may assume that any intervention (e.g., control group vs. experimental group) will only affect group means (and not group standard deviations / variances). This is the most common scenario. When researchers have this belief they use a pooled variance approach to calculating the \\(d\\)-value. That is, the variances of the two groups are pooled (i.e., averaged) to create the denominator for the \\(d\\)-value. Second, researchers may assume that the intervention (e.g., control group vs. experimental group) will affect both group means and standard deviations/variances. This is a less common situation. When researchers have this belief they use one group (e.g., a control group) as the frame of reference for the comparison. That is, the standard deviation for a single group (e.g., the control group) is used as the denominator for the \\(d\\)-value. 9.3.1.1 \\(d\\) denominator: Pooled variance 9.3.1.1.1 Reported \\(d\\)-value no confidence interval Scenario: A researcher reports a \\(d\\)-value of 1.67 in their article and that each condition has 50 people. The researcher believed that the intervention would only influence means and not standard deviations and used a pooled-variance \\(d\\)-value. They did not report a confidence interval. We can calculate a confidence interval for the independent groups \\(d\\)-value (pooled variance) with the command below: library(MBESS) ci.smd(sm = 1.67, n.1 = 50, n.2 = 50) ## $Lower.Conf.Limit.smd ## [1] 1.211 ## ## $smd ## [1] 1.67 ## ## $Upper.Conf.Limit.smd ## [1] 2.123 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 1.67, 95% CI [1.21, 2.12]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 1.21 to 2.12. 9.3.1.1.2 Using cell means only Scenario: A researcher does not report a \\(d\\)-value but indicates the descriptive statistics for the groups. For the first group \\(M\\) = 5.00, \\(SD\\) = 1.20, and \\(n\\) = 50. For the second group, \\(M\\) = 3.00, \\(SD\\) = 1.00, and \\(n\\) = 50. We can calculate the standardized mean difference (i.e., independent groups \\(d\\)-value) using the command below: library(MBESS) smd(Mean.1 = 5, s.1 = 1.2, n.1 = 50, Mean.2 = 3, s.2 = 1.0, n.2 = 50) ## [1] 1.811 Our sample estimate of \\(\\delta\\) is \\(d\\) = 1.81. The 95% confidence interval is obtained below: library(MBESS) ci.smd(sm = 1.81, n.1 = 50, n.2 = 50) ## $Lower.Conf.Limit.smd ## [1] 1.34 ## ## $smd ## [1] 1.81 ## ## $Upper.Conf.Limit.smd ## [1] 2.273 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 1.81, 95% CI [1.34, 2.27]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 1.34 to 2.27. 9.3.1.1.3 t: Equal group sizes Scenario: A researcher does not report a \\(d\\)-value, descriptive statistics, or n per group. Fortunately, the researcher did report the \\(t\\)-value, \\(t\\)(98) = 8.30, and the fact that the groups were the same size. After reading the article you believe the intervention is only likely to affect the means of the groups and not the variances - so a pooled variance term for the denominator of the \\(d\\)-value is appropriate. We can use the information provided to calculate a \\(d\\)-value and confidence interval. To calculate the \\(d\\)-value we need to know the number of people per group. We can obtain that information from the degrees of freedom (98) using the formula below. \\[ \\begin{aligned} df &amp;= n_1 + n_2 - 2 \\end{aligned} \\] Because the two groups were the same size we just use \\(n\\) instead of \\(n_1\\) and \\(n_2\\). We know \\(df\\) = 98: \\[ \\begin{aligned} 98 &amp;= n + n - 2\\\\ 98 &amp;= 2n - 2\\\\ 98 + 2 &amp;= 2n \\\\ 100 &amp;= 2n \\\\ \\frac{100}{2} &amp;= n \\\\ 50 &amp;= n \\\\ \\end{aligned} \\] library(MBESS) ci.smd(ncp = 8.30, n.1 = 50, n.2 = 50) ## $Lower.Conf.Limit.smd ## [1] 1.201 ## ## $smd ## [1] 1.66 ## ## $Upper.Conf.Limit.smd ## [1] 2.112 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 1.66, 95% CI [1.20, 2.11]. This \\(d\\)-value value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 1.20 to 2.11. 9.3.1.1.4 t: Unequal group sizes Scenario: A researcher does not report a \\(d\\)-value or descriptive statistics. Fortunately, the researcher did report the \\(t\\)-value, \\(t\\)(98) = 9.36, and the fact that the groups had 65 and 35 people. After reading the article you believe the intervention is only likely to affect the means of the groups and not the variances - so a pooled variance term for the denominator of the \\(d\\)-value is appropriate. We can use the information provided to calculate a \\(d\\)-value and confidence interval. library(MBESS) ci.smd(ncp = 9.36, n.1 = 65, n.2 = 35) ## $Lower.Conf.Limit.smd ## [1] 1.465 ## ## $smd ## [1] 1.962 ## ## $Upper.Conf.Limit.smd ## [1] 2.453 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 1.96. This value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval, 95% CI [1.47, 2.45] indicates that a plausible range for \\(\\delta\\) is 1.47 to 2.45. 9.3.1.2 \\(d\\) denominator: Control group variance In this section we focus on calculating the \\(d\\)-value when the researcher of the published article has used the standard deviation of a single group as the denominator for the \\(d\\)-value. This is usually done when the researcher believes an experimental intervention will influence both group means and group standard deviations. Researcher who use this approach will hopefully have made it clear what they have done. If you’re not sure what a researcher has done I suggest using the pooled variance approach for the denominator above. 9.3.1.2.1 Reported \\(d\\)-value no confidence interval Scenario: A researcher reports a \\(d\\)-value of 0.85 in their article and that each condition has 50 people. The researcher believed that the intervention would influence both means and standard deviations. Therefore, when they calculated the \\(d\\)-value they used the standard deviation of a single group (the control group). We can calculate a confidence interval for the independent groups \\(d\\)-value (control group variance) with the command below: library(MBESS) ci.smd.c(smd.c = 0.85, n.C = 50, n.E = 50) ## $Lower.Conf.Limit.smd.c ## [1] 0.4199 ## ## $smd.c ## [1] 0.85 ## ## $Upper.Conf.Limit.smd.c ## [1] 1.273 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 0.85, 95% CI [0.42, 1.27]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 0.42 to 1.27. 9.3.1.2.2 Using cell means Scenario: A researcher does not report a \\(d\\)-value but indicates the descriptive statistics for the groups. For the experimental group \\(M\\) = 5.00, \\(SD\\) = 1.20, and \\(n\\) = 50. For the control group, \\(M\\) = 3.00, \\(SD\\) = 1.40, and \\(n\\) = 50. After reading the article you believe that intervention might well influence both means and standard deviations. Therefore, you think a control group standard deviation should be the frame of reference for the \\(d\\)-value. You can calculate the standardized mean difference (i.e., independent groups \\(d\\)-value) using the command below: library(MBESS) smd.c(Mean.T = 5, Mean.C = 3, s.C = 1.4, n.C = 50) ## [1] 1.429 The confidence interval can be obtained with the command: library(MBESS) ci.smd.c(smd.c = 1.43, n.E = 50, n.C = 50) ## $Lower.Conf.Limit.smd.c ## [1] 0.942 ## ## $smd.c ## [1] 1.43 ## ## $Upper.Conf.Limit.smd.c ## [1] 1.908 The population effect (\\(\\delta\\)) is unknown but our sample estimate is \\(d\\) = 1.43, 95% CI [0.94, 1.91]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 0.94 to 1.91. 9.3.2 Repeated Measures For a repeated measures design, different \\(d\\)-values can be reported. We focus on the formula below for repeated measures \\(d\\)-values. In this formula the symbol, \\(\\bar{x}_{diff}\\), indicates the mean of the column of differences between the two times. Likewise, \\(s_{diff}\\), is used to indicate the standard deviation of the column of differences. \\[ \\begin{aligned} d &amp; = \\frac{\\bar{x}_{diff}}{s_{diff}} \\end{aligned} \\] 9.3.2.1 Reported repeated \\(d\\)-value, no confidence interval Scenario: A researcher reports a repeated-measures \\(d\\) = .50 and \\(n\\) = 150. The 95% confidence interval for this \\(d\\)-value (i.e., standardized mean difference) can be obtained with the command below. library(MBESS) ci.sm(sm = .50, N = 150) ## $Lower.Conf.Limit.Standardized.Mean ## [1] 0.3295 ## ## $Standardized.Mean ## [1] 0.5 ## ## $Upper.Conf.Limit.Standardized.Mean ## [1] 0.669 In this repeated measures design the population effect (\\(\\delta\\)) is unknown but our sample estimate of weight loss is \\(d = 0.50\\), 95% CI [0.33, 0.67]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 0.33 to 0.67. 9.3.2.2 Using repeated measures mean difference Scenario: A researcher reports a repeated-measures design on weight loss but does not report the \\(d\\)-value or confidence interval. He does report, however, that weights decreased on average by 3.00 lbs, SD = 6.00. In order to understand what the M = 3.00 lbs, SD = 6.00 refers to, consider the following situation. The participants “weigh in” at time 1. They diet for three months. Then they “weight out” at time 2. There are now two columns (time1, time2) with weight information - one participant per row. We can create a third column, called diff, by subtracting time 1 weight from time 2 weights. That is, diff = time 2 weight - time 1 weight. The new diff column indicates how the weights for each person have changed over the diet. The mean of the diff column is \\(\\bar{x}_{diff}\\) = 3.00 and the standard deviation of the diff column is \\(s_{diff}\\) = 6.00. This information can be used in the formula below: \\[ \\begin{aligned} d &amp; = \\frac{\\bar{x}_{diff}}{s_{diff}}\\\\ &amp;= \\frac{-3.00}{6.00} \\\\ &amp;= -.50 \\end{aligned} \\] We tend to always report \\(d\\)-values as positive values so \\(d\\) = .50. We can get a confidence interval around this value with the code below. library(MBESS) ci.sm(sm = .5, N =150) ## $Lower.Conf.Limit.Standardized.Mean ## [1] 0.3295 ## ## $Standardized.Mean ## [1] 0.5 ## ## $Upper.Conf.Limit.Standardized.Mean ## [1] 0.669 In this repeated measures design the population effect (\\(\\delta\\)) is unknown but our sample estimate of weight loss is \\(d = 0.50\\), 95% CI [0.33, 0.67]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 0.33 to 0.67. 9.3.2.3 Using repeated measures cell information Scenario: A researcher reports a repeated-measures design on weight loss but does not report the \\(d\\)-value, mean difference, or standard deviation for the differences. He does, however, report mean and standard deviation for before and after the weight loss program. As well, he reports the correlation between before diet weights and after diet weights. The mean weight before the diet was M = 140 lbs, SD = 5 whereas after the diet the mean weight was 132 lbs, SD = 6. The correlation between time 1 and time 2 weights was \\(r\\) =.80. In order to understand what the M = 140 lbs, SD = 5 refers to, consider the following situation. The participants “weigh in” at time 1. They diet for three months. Then they “weight out” at time 2. There are now two columns (time1, time2) with weight information. We can create a third column, called diff, by subtracting time 1 weight from time 2 weights. That is, diff = time 2 weight - time 1 weight. The new diff column indicates how the weights for each person have changed over the diet. The researcher reports descriptive statistics (M, SD) for the time 1 and time 2 columns but nothing about the diff column. We can, however, figure the the mean of the diff column, \\(bar{x}_{diff}\\), and the standard deviation of the diff column, \\(s_{diff}\\), from the information provided. Recall the formula for repeated measures \\(d\\)-value: \\[ \\begin{aligned} d &amp; = \\frac{\\bar{x}_{diff}}{s_{diff}} \\end{aligned} \\] We can obtain the numerator easily: \\[ \\begin{aligned} \\bar{x}_{diff} &amp; = \\bar{x}_1 - \\bar{x}_2 \\\\ &amp;= 140 - 132 \\\\ &amp;= 8 \\\\ \\end{aligned} \\] Obtaining the values for the denominator is a bit more complicated. We can calculate \\(s_{diff}\\) but it requires the standard deviations of the before weights (\\(s_1\\) = 5) and the after weights (\\(s_1\\) = 6) - as well as the correlation between the two times (\\(r\\) = .80). We begin by calculating \\(s_{diff}^2\\) which is variance of the column of differences differences: \\[ \\begin{aligned} s_{diff}^2 &amp; = s_1^2 + s_2^2 - 2(s_1)(s_2)r_{12} \\\\ &amp; = 5^2 + 6^2 - 2(5)(6)(.80)\\\\ &amp; = 25 + 36 - 48\\\\ &amp; = 13 \\end{aligned} \\] We obtain \\(s_{diff}\\) by taking the square root of \\(s_{diff}^2\\): \\[ \\begin{aligned} s_{diff} &amp; = \\sqrt{s_{diff}^2} \\\\ &amp; = \\sqrt{13}\\\\ &amp; = 3.605551 \\end{aligned} \\] Then we calculate the \\(d\\)-value using the numerator and denominator we calculated: \\[ \\begin{aligned} d &amp; = \\frac{\\bar{x}_{diff}}{s_{diff}}\\\\ &amp; = \\frac{8}{3.605551} \\\\ &amp; = 2.218801\\\\ &amp; = 2.22\\\\ \\end{aligned} \\] The \\(d\\)-value is 2.22 but we need a confidence interval: library(MBESS) ci.sm(sm = 2.22, N =100) ## $Lower.Conf.Limit.Standardized.Mean ## [1] 1.852 ## ## $Standardized.Mean ## [1] 2.22 ## ## $Upper.Conf.Limit.Standardized.Mean ## [1] 2.584 In this repeated measures design the population effect (\\(\\delta\\)) is unknown but our sample estimate of weight loss is \\(d\\) = 2.22, 95% CI [1.85, 2.58]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 1.85 to 2.58. 9.3.2.4 Using repeated measures \\(t\\)-value Scenario: A researcher reports a repeated-measures but does not report a d-value, mean difference with SD, or even the sample size. He does, however, report that t(40) = 6.23. The 95% confidence interval for this \\(d\\)-value (i.e., standardized mean difference) can be obtained with the command below. \\[ \\begin{aligned} df &amp;= N - 1 \\\\ 40 &amp;= N - 1\\\\ 40 + 1 &amp;= N\\\\ 41 &amp;= N \\end{aligned} \\] We can use the N and \\(t\\)-value to calculate the \\(d\\)-value and CI using the command: # t-value for repeated measures if library(MBESS) ci.sm(ncp = 6.23, N = 41) ## $Lower.Conf.Limit.Standardized.Mean ## [1] 0.5962 ## ## $Standardized.Mean ## [1] 0.973 ## ## $Upper.Conf.Limit.Standardized.Mean ## [1] 1.341 In this repeated measures design the population effect (\\(\\delta\\)) is unknown but our sample estimate of weight loss is \\(d\\) = 0.97, 95% CI [0.60, 1.34]. This \\(d\\)-value will differ from the population effect (\\(\\delta\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\delta\\) is 0.60 to 1.34. 9.4 Estimating \\(\\rho\\) 9.4.1 A single correlation Correlations are frequently reported in the literature. Less common, however, is the reporting of confidence intervals for those correlations. A confidence interval for a reported correlations (e.g., \\(r\\) = .40) can be obtained if you also have the sample size (\\(n\\) = 200) with the command below: library(MBESS) ci.cc(r = .40, n = 200) ## $Lower.Limit ## [1] 0.2766 ## ## $Estimated.Correlation ## [1] 0.4 ## ## $Upper.Limit ## [1] 0.5104 From this output you see that \\(r\\) = .40, 95% CI [.28, .51]. The population effect (\\(\\rho\\)) is unknown but our sample estimate of the population correlation is \\(r = .30\\), 95% CI [.28, .51]. This sample correlation will differ from the population correlation (\\(\\rho\\)) due to sampling error. The confidence interval indicates that a plausible range for \\(\\rho\\) is .28 to .51. 9.4.2 Difference between two correlations Sometimes a researcher will conduct two studies in the same article and compare the correlations. Scenario: A researcher reports that in Study 1 he found that \\(r\\) = .32, N = 300. But in Study 2 he found a correlation of \\(r\\) = .51, N = 400. You want to know the difference between the correlations and a confidence interval for that difference. \\[ \\begin{aligned} \\Delta r &amp;= r_2 - r_1 \\\\ &amp;= .51 - .32 \\\\ &amp;= .19 \\end{aligned} \\] You can obtain a confidence interval for this difference between correlations using the code below. Note, however, that this code applies only when the two correlations are from different samples. If the correlations are from the same sample the code is more complex but possible - see the cocor package documentation for details. The code for obtaining the confidence interval for the difference between these two correlations from different samples is below. Examine the last part of the output labeled zou2007. library(cocor) cocor.indep.groups(r1.jk = .51, r2.hm = .32, n1 = 300, n2 = 400) ## ## Results of a comparison of two correlations based on independent groups ## ## Comparison between r1.jk = 0.51 and r2.hm = 0.32 ## Difference: r1.jk - r2.hm = 0.19 ## Group sizes: n1 = 300, n2 = 400 ## Null hypothesis: r1.jk is equal to r2.hm ## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided) ## Alpha: 0.05 ## ## fisher1925: Fisher&#39;s z (1925) ## z = 3.0120, p-value = 0.0026 ## Null hypothesis rejected ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r1.jk - r2.hm: 0.0668 0.3105 ## Null hypothesis rejected (Interval does not include 0) From this output you see that in Study 1 \\(r\\) = .32 and in Study 2 \\(r\\) = .51. The difference between the correlations \\(\\Delta r\\) = .19, 95% CI [.07, .31]. Both population correlations are unknown as is the difference between the population correlations. We have a sample estimate of the difference between the two population correlations, \\(\\Delta r\\) = .19, and the confidence interval indicates that difference between the population correlations may plausibly be as small as .07 or as large as .31. "],["nhst-and-sample-size.html", "Chapter 10 NHST and sample size 10.1 Required 10.2 Overview 10.3 Goals 10.4 Independent groups \\(t\\)-test 10.5 Repeated measures \\(t\\)-test 10.6 Correlations", " Chapter 10 NHST and sample size 10.1 Required The following CRAN packages must be installed: Required CRAN Packages MBESS pwr 10.2 Overview In this chapter we focus on statistical power – which is the probability of finding an effect if it exists. For example, if the power for your study is .90 that means you have a 90% chance of finding an effect if it exists. On the other hand, if your statistical power is below .50 that means you have less than a 50% chance of finding an effect if it exists. If your power is .50 you might well question whether it is even worth conducting your study - because the odds of finding an effect (if there is one) are so incredibly low. Low statistical power means you have a low chance of concluding there is an effect when an effect is present. Unfortunately, many researchers only focus on statistical power with respect to failing to find an effect when it is present. In fact, low statistical power is directly related to false positive findings. That is, when statistical power is low and you obtain a significant \\(p\\)-value, it’s likely that this is false positive finding (i.e., a falsely significant \\(p\\)-value). Low statistical power means you have a high chance of concluding there is an effect when an effect is not present. The two points above indicate that low statistical power is associated with untrustworthy research findings. When statistical power is low findings are not credible - regardless of whether they are significant or not. Unfortunately, power levels are typically quite low in psychology and related disciplines. Indeed, (Button et al. 2013) noted in their Nature Reviews Neuroscience article that the “median statistical power of studies in the neuroscience field is optimistically estimated to be between ~8% and ~31%” (p. 8). This may seem surprising given the large effects sometimes observed in neuroscience, however, “the larger reported effect sizes in cognitive neuroscience may well be the consequence of effect size exaggeration due to having smaller sample sizes (as shown above) and consequential low power” (p. 8, Szucs and Ioannidis 2017). Biomedical research, more generally, appears to have a low power problem (Dumas-Mallet et al. 2017). That said, the power levels in the rest of psychology are only marginally higher - so the entire field has substantial problem - it is not a problem limited to neuroscience or biomedical research. Interesting, it is possible to calculate the average statistical power for a journal. Doing so allows us to examine many journals and, in particular, the relation between journal impact factor (how often they are cited) and journal statistical power. It turns out there is a negative relation between journal statistical power and journal impact factor (Szucs and Ioannidis 2017). This indicates that as the journal impact factor increases there is a corresponding decrease in statistical power – which means means that journals with high impact factors (those that are most cited) tend to have the most untrustworthy findings. This conclusion is perhaps not as counter intuitive as it might seem. High impact journals often have policies that require published findings to be surprising. The most surprising finding is one that is wrong and inconsistent with previous research – the exact type of finding you will get with low statistical power. 10.3 Goals In the rest of the chapter we focus on obtaining the desired power for our study by conducting a sample size analysis based on Null Hypothesis Significance Testing (NHST) logic. We caveat all of the subsequent advise is based on the goal of obtaining research findings that are trustworthy. Many times, however, research may be conducted with a different goal. For example, an Honours Thesis in psychology may be conducted with the primary goal being a learning experience for the student. When this is the case (as it often is in training scenarios) then the sample sizes used for the project may fall substantially short of the sample sizes suggested by power analyses below. This occurs frequently due to the fact that student theses have limited time and financial resources because the focus is on learning the research process rather than producing robust findings. Conducting a sample size analysis requires a few pieces of information before you start. Desired power. Power refers to probability of obtaining a significant result (p &lt; .05) if the effect/difference exists. Often a power level of .80 or .85 is suggested when planning studies. However, a power-level of .90 is probably more advisable given how easy it is to obtain a \\(p\\)-value less than .05. Indeed, Dr. Daniel Lakens notes that only individuals with power .90 or higher are eligible for funding in his Department at the Eindhoven University of Technology. Population-level effect size. To conduct a sample-size analysis you need an estimate of the population effect size you are trying to find. Most commonly, for theses, that means you needs a estimate of the population-level \\(d\\)-value (i.e., \\(\\delta\\)) or population-level correlation (\\(\\rho\\)) that you are trying to detect. Below we present the process for conducting a sample size analysis for the independent groups \\(t\\)-test, repeated measures \\(t\\)-test, andcorrelations. For each of these analyses we go through three steps: Estimating the population effect size Determining the desired sample size for your study Determining what sample-level effect sizes will be significant in your study with the desired sample size. As we review these steps for the analyses below we omit using subjective judgment as the basis for estimating the population effect size. Sometimes researchers are tempted to use a subjective judgment as to whether the population effect is small, medium, or large based on Cohen’s (1988) benchmarks. Unfortunately, using benchmarks of this sort as the basis for determine the smallest effect size of interest (SESOI) at the population level is problematic because it is subjective. Many researchers might be tempted to take a middle of the road approach and use a medium effect size. Yet a review of meta-analytic effect sizes (i.e., population estimates) by (Hemphill 2003) indicated that 2/3 of the population effect sizes in psychology are lower than a medium effect. Consequently, taking a middle of the road approach and assuming a medium effect size is likely to result in low statistical power. Indeed, “[r]elying on a benchmark is the weakest possible justification of a SESOI and should be avoided.” (p. 262, Lakens, Scheel, and Isager 2018) Using a sample size of previous study. One approach to sample size analysis is simply to use the sample size from a previous study conducted in that research area. This is the worst possible strategy for choosing a sample size. Richard Morey and Daniel Lakens discuss how most study results in psychology are statistically unfalsifiable because the sample sizes (and corresponding power) are so low. Consequently, using a the sample size for your study based on past research is not advised. Doing so will most likely to result in a sample size that is so small any resulting findings will be untrustworthy and unfalsifiable. Conduct a proper sample size analysis. 10.4 Independent groups \\(t\\)-test 10.4.1 Population effect size 10.4.1.1 Safeguard approach The \\(d\\)-value obtained from a previous independent groups \\(t\\)-test study is highly influenced by sampling error and therefore a relatively poor estimate of the population \\(d\\)-value (\\(\\delta\\)). The population \\(d\\)-value may be substantially higher or lower than the previous study’s sample \\(d\\)-value. If the population \\(d\\)-value is higher than the previous study’s \\(d\\)-value this does not create a problem for detecting the effect being studied. On the other hand, if the population \\(d\\)-value is substantially lower than the previous study’s \\(d\\)-value then the situation is problematic. Using the previous study’s effect size in your sample size/power analysis will produce a desired sample size for your study that is insufficient to detect the effect of interest. Consequently one approach for determining the smallest effect size of interest in your power analysis is a safeguard approach (Perugini, Gallucci, and Costantini 2014). With this approach you use the lower bound of the 95% confidence interval as the population effect size in your power / sample size analysis. Let’s assume the effect size in the previous study was \\(d\\) = 0.45 and there were 50 people in each group. If the previous study did not report an effect size - you can use the approaches outlined in the “Effect sizes from publications” chapter to obtain one. The confidence interval is obtained by: library(MBESS) ci.smd(smd = .45 , n.1 = 50, n.2 = 50) ## $Lower.Conf.Limit.smd ## [1] 0.05187 ## ## $smd ## [1] 0.45 ## ## $Upper.Conf.Limit.smd ## [1] 0.8459 Thus, \\(d\\) = 0.45, 95% CI [0.05, 0.85]. Therefore, with a sample \\(d\\)-value of 0.45 (\\(n_1=n_2=50\\)) a plausible range for the population \\(d\\)-value (\\(\\delta\\)) is 0.05 to 0.85. That is, the population \\(d\\)-value (\\(\\delta\\)) could be as low as 0.05. Therefore, we would use \\(\\delta\\) = 0.05 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.4.1.2 Small telescope The small telescope approach to determining the effect size to use in your independent groups \\(t\\)-test power/sample size analysis is based on the analogy of a telescope (Simonsohn 2015). Imagine you are an astronomer. A previous study used a small telescope and claimed to see a new asteroid. You are the proud owner of a large telescope and decide to look for the asteroid. If the asteroid exists then you should be able to find it easily with your larger telescope. Conversely, if the you are unable to find the asteroid with your larger telescope it draws into question the original study. In this example, the size of the telescope is a proxy for statistical power. Use of this approach is based on the premise the previous study had a significant finding. We begin by assuming the previous study is a small telescope and has low statistical power. We use the cell sizes for the two groups from the previous study and assume a power of .33. Then we calculate the population effect that would produce a power of .33 using the code below: library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test # For n1 and n2 indicate the two group sizes alternative &lt;- &quot;greater&quot; n1 &lt;- 50 n2 &lt;- 50 # do not modify this line pwr.t2n.test(power = .33, n1 = n1, n2 = n2, alternative = alternative) ## ## t test power calculation ## ## n1 = 50 ## n2 = 50 ## d = 0.2427 ## sig.level = 0.05 ## power = 0.33 ## alternative = greater The indicates when \\(\\delta\\) = .24 the previous study (with 50 per group) would have statistical power of .33. Therefore, we would use \\(\\delta\\) = .24 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.4.1.3 Smallest sig. effect Another approach to determine the effect size to use in your independent groups \\(t\\)-test power / sample size analysis is to determine the smallest effect that would have been significant in the previous study (Lakens, Scheel, and Isager 2018). We do that with the code below: # Based on the previous study modify the settings below. # For n1 and n2 indicate the two group sizes n1 &lt;- 50 n2 &lt;- 50 # Did the previous study use a direction (one-tail test) # Set p_critical accordingly # use .975 if original study was two-tail test; # use .95 if original study was one-tail test p_critical &lt;- .95 # indicates one-tail test # do not modify the lines below df &lt;- n1 + n2 - 2 t_critical &lt;- qt(p = p_critical, df = df) d_critical &lt;- t_critical * sqrt(1/n1 + 1/n2) print(d_critical) ## [1] 0.3321 The indicates that the smallest sample \\(d\\)-value that would have been significant in the previous study is \\(d\\) = 0.33. Therefore, we would use \\(\\delta\\) = .33 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.4.2 Determining sample size To determine a sample size for your independent groups \\(t\\)-test you need to: specify an estimate of the population effect size (see the various methods reviewed above) specify desired power (probability of a significant effect if it exists) specify if you are conducting a one-tailed or two-tailed test examine a graph to see how a greater/fewer number of participants influences power examine a graph to see how a higher/lower population effect size influences power In the example below we use: Effect size estimate from the small telescope approach (\\(\\delta = .24\\)) Desired power of .90 - a 90% chance of finding our effect if it exists The fact the we wish to conduct a one-tail test (i.e., in our study we will have a directional alternative hypothesis) library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_d &lt;- .24 power &lt;- .90 pwr_out &lt;- pwr.t.test(d = pop_d, power = power, type = &quot;two.sample&quot;, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## Two-sample t test power calculation ## ## n = 298 ## d = 0.24 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater ## ## NOTE: n is number in *each* group This analysis reveals that a) if we assume the population effect size is .24 (\\(\\delta = .24\\)) and b) desire a 90% chance of finding our effect if it exits - we need 298 people/animals per group. So a total of 596 participants for these two cells. 10.4.2.0.1 A different number of participants? But what happens to our power if we get a greater/fewer number of participants? We don’t always have control over the exact number due to non-response rates etc. We can use the graph below to see how power changes as the number of participants changes. Based on an examination of this graph, we may want to adjust the number of participants. plot(pwr_out) We can see the information conveyed in the graph above for an independent groups \\(t\\)-test in table form using the code below. # Specify the population effect size from your power analysis pop_d &lt;- .24 # Do not change code below pwr_50 &lt;- round(pwr.t.test(d = pop_d, power = .50)$n) pwr_80 &lt;- round(pwr.t.test(d = pop_d, power = .80)$n) pwr_95 &lt;- round(pwr.t.test(d = pop_d, power = .95)$n) power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, n_start = round(c(1, pwr_50, pwr_80, pwr_95),2), n_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power n_start n_end ## 1 dangerously low power (0 to .50) 1 134 ## 2 low power (.50 to .80) 134 273 ## 3 adequate power (.80 to .95) 273 452 ## 4 almost certain power (&gt; .95) 452 NA This table indicates that assuming a population effect size of \\(\\delta\\) = .24 that your statistical power will be: dangerously low if you have between 1 and 134 participants per group low if you have between 134 and 273 participants per group adequate if you have between 273 and 452 participants per group excellent if you have 452 or more participants per group 10.4.2.0.2 A different effect size? Sample size calculations use an estimate of the unknown population effect size. What happens to power if the population effect size is different than what we estimated. That information is conveyed in the graph below. An examination of this output might cause you to adjust your sample size. The style of the graph produced below was inspired by Dr. Richard Morey’s excellent web app for power. library(pwr) library(tidyverse) # Indicate the type of test for your study. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; n &lt;- 298 # N per group for two.sample test max_pop_d = 1.2 #max d value on x-axis. Must be high (e.g., 3 or so) when n is low. # Do not modify code below type = &quot;two.sample&quot; # Use &quot;two.sample&quot; or &quot;paired&quot; pop_d &lt;- seq(0, max_pop_d, by = 0.01) pop_d_axis_values &lt;- seq(0, max_pop_d, by = 0.10) power_values &lt;- data.frame(pop_d = pop_d, n = n, power = NA) for (i in 1:nrow(power_values)){ pwr_analysis &lt;- pwr.t.test(d = power_values$pop_d[i], n = n, type = type, alternative = alternative) power_values$power[i] &lt;- pwr_analysis$power } pwr_50 &lt;- pwr.t.test(n = n, power = .50, type = type)$d pwr_80 &lt;- pwr.t.test(n = n, power = .80, type = type)$d pwr_95 &lt;- pwr.t.test(n = n, power = .95, type = type)$d power_plot &lt;- ggplot(data = power_values, mapping = aes(x = pop_d, y = power)) + geom_line() + scale_x_continuous(breaks = pop_d_axis_values) + scale_y_continuous(breaks = seq(0, 1, by = .10)) + labs(title = sprintf(&quot;Power plot for N = %g per group&quot;, n), subtitle = sprintf(&quot;Alternative = %s&quot;, alternative), x = &quot;Population d-value&quot;, y = &quot;Power (probability of sig. effect)&quot;) + annotate(&quot;rect&quot;, xmin = 0, xmax = pwr_50, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .5) + annotate(&quot;rect&quot;, xmin = pwr_50, xmax = pwr_80, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_80, xmax = pwr_95, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_95, xmax = max_pop_d, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .5) + theme_classic() print(power_plot) We can see the information conveyed in the graph above for an independent groups \\(t\\)-test in table form using the code below. # Specify the sample size per group from your power analysis n = 298 # N per group # Do not modify the code below pwr_50 &lt;- pwr.t.test(n = n, power = .50, type = &quot;two.sample&quot;)$d pwr_80 &lt;- pwr.t.test(n = n, power = .80, type = &quot;two.sample&quot;)$d pwr_95 &lt;- pwr.t.test(n = n, power = .95, type = &quot;two.sample&quot;)$d power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, delta_start = round(c(0, pwr_50, pwr_80, pwr_95),2), delta_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power delta_start delta_end ## 1 dangerously low power (0 to .50) 0.00 0.16 ## 2 low power (.50 to .80) 0.16 0.23 ## 3 adequate power (.80 to .95) 0.23 0.30 ## 4 almost certain power (&gt; .95) 0.30 NA This table indicates that using 298 people per group your statistical power will be: dangerously low if the actual population effect is higher than 0 and lower than \\(\\delta = 0.16\\) low if the actual population effect is between \\(\\delta = 0.16\\) and \\(\\delta = 0.23\\) adequate if the actual population effect is between \\(\\delta = 0.23\\) and \\(\\delta = 0.30\\) excellent if the actual population effect is higher than \\(\\delta = 0.30\\) 10.4.3 What will be signficant? Let’s assume that after looking at all the graphs and tables you end up sticking with your initial sample size of 298 per group. What sample-level effect sizes will be significant when you do so? # Specify the sample size per group from your power / sample size analysis n1 &lt;- 298 n2 &lt;- 298 # Use .95 for one-sided and .975 for two-sided tests percentile = .95 # Do not modify the code below t_critical &lt;- qt(percentile, df = (n1 + n2 - 2)) d_critical = t_critical * sqrt((1/n1) + (1/n2)) print(d_critical) ## [1] 0.135 This indicates that with a sample size of 298 per group that only independent groups \\(d\\)-values greater than \\(d\\) = 0.135 will be significant. 10.5 Repeated measures \\(t\\)-test 10.5.1 Population effect size 10.5.1.1 Safeguard approach The \\(d\\)-value obtained from a previous repeated measures \\(t\\)-test study is highly influenced by sampling error and therefore a relatively poor estimate of the population \\(d\\)-value (\\(\\delta\\)). The population \\(d\\)-value may be substantially higher or lower than the previous study’s sample \\(d\\)-value. If the population \\(d\\)-value is higher than the previous study’s \\(d\\)-value this does not create a problem for detecting the effect being studied. On the other hand, if the population \\(d\\)-value is substantially lower than the previous study’s \\(d\\)-value then the situation is problematic. Using the previous study’s effect size in your sample size/power analysis will produce a desired sample size for your study that is insufficient to detect the effect of interest. Consequently one approach for determining the smallest effect size of interest in your power analysis is a safeguard approach (Perugini, Gallucci, and Costantini 2014). With this approach you use the lower bound of the 95% confidence interval as the population effect size in your power / sample size analysis. Let’s assume the effect size in the previous study was \\(d\\) = 0.45 and there were 50 participants. If the previous study did not report an effect size - you can use the approaches outlined in the “Effect sizes from publications” chapter to obtain one. The confidence interval is obtained by: library(MBESS) ci.sm(sm = .45 , N = 50) ## $Lower.Conf.Limit.Standardized.Mean ## [1] 0.1568 ## ## $Standardized.Mean ## [1] 0.45 ## ## $Upper.Conf.Limit.Standardized.Mean ## [1] 0.739 Thus, \\(d\\) = 0.45, 95% CI [0.16, 0.74]. Therefore, with a sample \\(d\\)-value of 0.45 (\\(N=50\\)) a plausible range for the population \\(d\\)-value (\\(\\delta\\)) is 0.16 to 0.74. That is, the population \\(d\\)-value (\\(\\delta\\)) could be as low as 0.16. Therefore, we would use \\(\\delta\\) = 0.16 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.5.1.2 Small telescope The small telescope approach to determining the effect size to use in your repeated measures \\(t\\)-test power/sample size analysis is based on the analogy of a telescope (Simonsohn 2015). Imagine you are an astronomer. A previous study used a small telescope and claimed to see a new asteroid. You are the proud owner of a large telescope and decide to look for the asteroid. If the asteroid exists then you should be able to find it easily with your larger telescope. Conversely, if the you are unable to find the asteroid with your larger telescope it draws into question the original study. In this example, the size of the telescope is a proxy for statistical power. Use of this approach is based on the premise the previous study had a significant finding. We begin by assuming the previous study is a small telescope and has low statistical power. We use the csample size from the previous study and assume a power of .33. Then we calculate the population effect that would produce a power of .33 using the code below: library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test # For n indicate the sample size alternative &lt;- &quot;greater&quot; n &lt;- 50 # do not modify this line pwr.t.test(power = .33, n = n, type = &quot;paired&quot;, alternative = alternative) ## ## Paired t test power calculation ## ## n = 50 ## d = 0.1728 ## sig.level = 0.05 ## power = 0.33 ## alternative = greater ## ## NOTE: n is number of *pairs* The indicates when \\(\\delta\\) = 0.17 the previous study (with 50 participants) would have statistical power of .33. Therefore, we would use \\(\\delta\\) = 0.17 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.5.1.3 Smallest sig. effect Another approach to determine the effect size using in your repeated measures \\(t\\)-test power / sample size analysis is to determine the smallest effect that would have been significant in the previous study (Lakens, Scheel, and Isager 2018). We do that with the code below: # Based on the previous study modify the settings below. # For n indicate the sample size n &lt;- 50 # Did the previous study use a direction (one-tail test) # Set p_critical accordingly # use .975 if original study was two-tail test; # use .95 if original study was one-tail test p_critical &lt;- .95 # indicates one-tail test # do not modify the lines below df &lt;- n - 1 t_critical &lt;- qt(p = p_critical, df = df) d_critical &lt;- t_critical * sqrt(1/n) print(d_critical) ## [1] 0.2371 The indicates that the smallest sample \\(d\\)-value that would have been significant in the previous study is \\(d\\) = 0.24. Therefore, we would use \\(\\delta\\) = .24 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.5.2 Determining sample size To determine a sample size for your repeated measures \\(t\\)-test you need to: specify an estimate of the population effect size (see the various methods reviewed above) specify desired power (probability of a significant effect if it exists) specify if you are conducting a one-tailed or two-tailed test examine a graph to see how a greater/fewer number of participants influences power examine a graph to see how a higher/lower population effect power In the example below we use: Effect size estimate from the small telescope approach (\\(\\delta = .17\\)) Desired power of .90 - a 90% chance of finding our effect if it exists The fact the we wish to conduct a one-tail test (i.e., in our study we will have a directional alternative hypothesis) library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_d &lt;- .17 power &lt;- .90 pwr_out &lt;- pwr.t.test(d = pop_d, power = power, type = &quot;paired&quot;, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## Paired t test power calculation ## ## n = 297.7 ## d = 0.17 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater ## ## NOTE: n is number of *pairs* This analysis reveals that a) if we assume the population effect size for our repeated measures \\(t\\)-test is 0.17 (\\(\\delta = 0.17\\)) and b) desire a 90% chance of finding our effect if it exits - we need 298 participants. 10.5.2.0.1 A different number of participants? But what happens to our power if we get a greater/fewer number of participants? We don’t always have control over the exact number due to non-response rates etc. We can use the graph below to see how power changes as the number of participants changes. Based on an examination of this graph, we may want to adjust the number of participants. plot(pwr_out) We can see the information conveyed in the graph above for an repeated measures \\(t\\)-test in table form using the code below. # Specify the population effect size from your power analysis pop_d &lt;- .17 # Do not change code below pwr_50 &lt;- round(pwr.t.test(d = pop_d, power = .50, type = &quot;paired&quot;)$n) pwr_80 &lt;- round(pwr.t.test(d = pop_d, power = .80, type = &quot;paired&quot;)$n) pwr_95 &lt;- round(pwr.t.test(d = pop_d, power = .95, type = &quot;paired&quot;)$n) power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, n_start = round(c(1, pwr_50, pwr_80, pwr_95),2), n_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power n_start n_end ## 1 dangerously low power (0 to .50) 1 135 ## 2 low power (.50 to .80) 135 274 ## 3 adequate power (.80 to .95) 274 452 ## 4 almost certain power (&gt; .95) 452 NA This table indicates that assuming a population effect size of \\(\\delta\\) = .24 that your statistical power will be: dangerously low if you have between 1 and 135 participants low if you have between 135 and 274 participants adequate if you have between 274 and 452 participants excellent if you have 452 or more participants 10.5.2.0.2 A different effect size? Sample size calculations use an estimate of the unknown population effect size. What happens to power if the population effect size is different than what we estimated. That information is conveyed in the graph below. An examination of this output might cause you to adjust your sample size. library(pwr) library(tidyverse) # Indicate the type of test for your study. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; n &lt;- 298 # sample size max_pop_d = 1.2 #max d value on x-axis. Must be high (e.g., 3 or so) when n is low. # Do not modify code below type = &quot;paired&quot; # Use &quot;two.sample&quot; or &quot;paired&quot; pop_d &lt;- seq(0, max_pop_d, by = 0.01) pop_d_axis_values &lt;- seq(0, max_pop_d, by = 0.10) power_values &lt;- data.frame(pop_d = pop_d, n = n, power = NA) for (i in 1:nrow(power_values)){ pwr_analysis &lt;- pwr.t.test(d = power_values$pop_d[i], n = n, type = type, alternative = alternative) power_values$power[i] &lt;- pwr_analysis$power } pwr_50 &lt;- pwr.t.test(n = n, power = .50, type = type)$d pwr_80 &lt;- pwr.t.test(n = n, power = .80, type = type)$d pwr_95 &lt;- pwr.t.test(n = n, power = .95, type = type)$d power_plot &lt;- ggplot(data = power_values, mapping = aes(x = pop_d, y = power)) + geom_line() + scale_x_continuous(breaks = pop_d_axis_values) + scale_y_continuous(breaks = seq(0, 1, by = .10)) + labs(title = sprintf(&quot;Power plot for N = %g&quot;, n), subtitle = sprintf(&quot;Alternative = %s&quot;, alternative), x = &quot;Population d-value&quot;, y = &quot;Power (probability of sig. effect)&quot;) + annotate(&quot;rect&quot;, xmin = 0, xmax = pwr_50, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .5) + annotate(&quot;rect&quot;, xmin = pwr_50, xmax = pwr_80, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_80, xmax = pwr_95, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_95, xmax = max_pop_d, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .5) + theme_classic() print(power_plot) We can see the information conveyed in the graph above for an repeated measures \\(t\\)-test in table form using the code below. # Specify the sample size from your power analysis n = 298 # Do not modify the code below pwr_50 &lt;- pwr.t.test(n = n, power = .50, type = &quot;paired&quot;)$d pwr_80 &lt;- pwr.t.test(n = n, power = .80, type = &quot;paired&quot;)$d pwr_95 &lt;- pwr.t.test(n = n, power = .95, type = &quot;paired&quot;)$d power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, delta_start = round(c(0, pwr_50, pwr_80, pwr_95),2), delta_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power delta_start delta_end ## 1 dangerously low power (0 to .50) 0.00 0.11 ## 2 low power (.50 to .80) 0.11 0.16 ## 3 adequate power (.80 to .95) 0.16 0.21 ## 4 almost certain power (&gt; .95) 0.21 NA This table indicates that using 298 participants your statistical power will be: dangerously low if the actual population effect is higher than 0 and lower than \\(\\delta = 0.11\\) low if the actual population effect is between \\(\\delta = 0.11\\) and \\(\\delta = 0.16\\) adequate if the actual population effect is between \\(\\delta = 0.16\\) and \\(\\delta = 0.21\\) excellent if the actual population effect is higher than \\(\\delta = 0.21\\) 10.5.3 What will be signficant? Let’s assume that after looking at all the graphs and tables you end up sticking with your initial sample size of 298. What sample-level effect sizes will be significant when you do so? # Specify the sample size from your power / sample size analysis n &lt;- 298 # Use .95 for one-sided and .975 for two-sided tests percentile = .95 # Do not modify the code below t_critical &lt;- qt(percentile, df = (n - 1)) d_critical &lt;- t_critical * sqrt(1/n) print(d_critical) ## [1] 0.09558 This indicates that with a sample size of 298 that only repeated measures \\(d\\)-values greater than \\(d\\) = 0.10 (rounded) will be significant. 10.6 Correlations 10.6.1 Population effect size 10.6.1.1 Safeguard approach The sample correlation obtained from a previous study is highly influenced by sampling error and therefore a relatively poor estimate of the population correlation (\\(\\rho\\)). The population correlation may be substantially higher or lower than the previous study’s sample correlation. If the population correlation is higher than the previous study’s sample correlation this does not create a problem for detecting the effect being studied. On the other hand, if the population correlation is substantially lower than the previous study’s sample correlation then the situation is problematic. Using the previous study’s effect size in your sample size/power analysis will produce a desired sample size for your study that is insufficient to detect the effect of interest. Consequently one approach for determining the smallest effect size of interest in your power analysis is a safeguard approach (Perugini, Gallucci, and Costantini 2014). With this approach you use the lower bound of the 95% confidence interval as the population effect size in your power / sample size analysis. Let’s assume the effect size in the previous study was \\(r\\) = .35 and there were 75 people. If the previous study did not report an effect size - you can use the approaches outlined in the “Effect sizes from publications” chapter to obtain one. The confidence interval is obtained by: library(MBESS) ci.cc(r = .35 , n = 75) ## $Lower.Limit ## [1] 0.1337 ## ## $Estimated.Correlation ## [1] 0.35 ## ## $Upper.Limit ## [1] 0.5345 Thus, \\(r\\) = .35, 95% CI [.13, .53]. Therefore, with a sample correlation of .35 (\\(N=75\\)) a plausible range for the population correlation (\\(\\rho\\)) is .13 to .53. That is, the population correlation (\\(\\rho\\)) could be as low as .13. Therefore, we would use \\(\\rho\\) = .13 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.6.1.2 Small telescope The small telescope approach to determining the effect size to use in your correlation power/sample size analysis is based on the analogy of a telescope (Simonsohn 2015). Imagine you are an astronomer. A previous study used a small telescope and claimed to see a new asteroid. You are the proud owner of a large telescope and decide to look for the asteroid. If the asteroid exists then you should be able to find it easily with your larger telescope. Conversely, if the you are unable to find the asteroid with your larger telescope it draws into question the original study. In this example, the size of the telescope is a proxy for statistical power. Use of this approach is based on the premise the previous study had a significant finding. We begin by assuming the previous study is a small telescope and has low statistical power. We use the sample size from the previous study and assume a power of .33. Then we calculate the population effect that would produce a power of .33 using the code below: library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test # For n indicate the sample size alternative &lt;- &quot;two.sided&quot; n &lt;- 75 # do not modify this line pwr.r.test(n = n, power = .33, alternative = alternative) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 75 ## r = 0.1762 ## sig.level = 0.05 ## power = 0.33 ## alternative = two.sided The indicates when the population correlation is .18 (rounded), \\(\\rho\\) = .18, the previous study (with 75 participants) would have statistical power of .33. Therefore, we would use \\(\\rho\\) = .18 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.6.1.3 Smallest sig. effect Another approach to determine the effect size using in your correlation power / sample size analysis is to determine the smallest effect that would have been significant in the previous study (Lakens, Scheel, and Isager 2018). We do that with the code below: # Based on the previous study modify the settings below. # For n indicate the sample size n &lt;- 75 # Did the previous study use a direction (one-tail test) # Set p_critical accordingly # use .975 if original study was two-tail test; # use .95 if original study was one-tail test p_critical &lt;- .975 # indicates two-tail test # do not modify the lines below df &lt;- n - 2 t_critical &lt;- qt(p = p_critical, df = df) r_critical &lt;- sqrt((t_critical^2)/(df + t_critical^2)) print(r_critical) ## [1] 0.2272 The indicates that the smallest sample correlation that would have been significant in the previous study is \\(r\\) = 0.23. Therefore, we would use \\(\\rho\\) = .23 as the population effect size in next stage of the power/sample size analysis: Determining Sample Size. 10.6.2 Determining sample size To determine a sample size for your correlation study you need to: specify an estimate of the population effect size (see the various methods reviewed above) specify desired power (probability of a significant effect if it exists) specify if you are conducting a one-tailed or two-tailed test examine a graph to see how a greater/fewer number of participants influences power examine a graph to see how a higher/lower population effect power In the example below we use: Effect size estimate from the small telescope approach (\\(\\rho = .18\\)) Desired power of .90 - a 90% chance of finding our effect if it exists The fact the we wish to conduct a one-tail test (i.e., in our study we will have a directional alternative hypothesis) library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_r &lt;- .18 power &lt;- .90 pwr_out &lt;- pwr.r.test(r = pop_r, power = power, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 260.6 ## r = 0.18 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater This analysis reveals that a) if we assume the population correlation is .18 (\\(\\rho = .18\\)) and b) desire a 90% chance of finding our effect if it exits - we need 261 participants. 10.6.2.0.1 A different number of participants? But what happens to our power if we get a greater/fewer number of participants? We don’t always have control over the exact number due to non-response rates etc. We can use the graph below to see how power changes as the number of participants changes. Based on an examination of this graph, we may want to adjust the number of participants. plot(pwr_out) We can see the information conveyed in the graph above for our proposed correlation study in table form using the code below. # Specify the population effect size from your power analysis pop_r &lt;- .18 # Do not change code below pwr_50 &lt;- round(pwr.r.test(r = pop_r, power = .50)$n) pwr_80 &lt;- round(pwr.r.test(r = pop_r, power = .80)$n) pwr_95 &lt;- round(pwr.r.test(r = pop_r, power = .95)$n) power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, n_start = round(c(1, pwr_50, pwr_80, pwr_95),2), n_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power n_start n_end ## 1 dangerously low power (0 to .50) 1 118 ## 2 low power (.50 to .80) 118 239 ## 3 adequate power (.80 to .95) 239 394 ## 4 almost certain power (&gt; .95) 394 NA This table indicates that assuming a population effect size of \\(\\rho\\) = .18 that your statistical power will be: dangerously low if you have between 1 and 118 participants low if you have between 118 and 239 participants adequate if you have between 239 and 394 participants excellent if you have 394 or more participants 10.6.2.0.2 A different effect size? Sample size calculations use an estimate of the unknown population effect size. What happens to power if the population effect size is different than what we estimated. That information is conveyed in the graph below. An examination of this output might cause you to adjust your sample size. library(pwr) library(tidyverse) # Indicate the type of test for your study. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; n &lt;- 261 # sample size # Do not modify code below rho &lt;- seq(0, 1, by = 0.01) rho_axis_values &lt;- seq(0, 1, by = 0.10) power_values &lt;- data.frame(rho = rho, n = n, power = NA) for (i in 1:nrow(power_values)){ pwr_analysis &lt;- pwr.r.test(r = power_values$rho[i], n = n, alternative = alternative) power_values$power[i] &lt;- pwr_analysis$power } pwr_50 &lt;- pwr.r.test(n = n, power = .50)$r pwr_80 &lt;- pwr.r.test(n = n, power = .80)$r pwr_95 &lt;- pwr.r.test(n = n, power = .95)$r power_plot &lt;- ggplot(data = power_values, mapping = aes(x = rho, y = power)) + geom_line() + scale_x_continuous(breaks = rho_axis_values) + scale_y_continuous(breaks = seq(0, 1, by = .10)) + labs(title = sprintf(&quot;Power plot for N = %g&quot;, n), subtitle = sprintf(&quot;Alternative = %s&quot;, alternative), x = &quot;Population Correlation&quot;, y = &quot;Power (probability of sig. effect)&quot;) + annotate(&quot;rect&quot;, xmin = 0, xmax = pwr_50, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .5) + annotate(&quot;rect&quot;, xmin = pwr_50, xmax = pwr_80, ymin = 0, ymax = 1, fill = &quot;red&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_80, xmax = pwr_95, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .2) + annotate(&quot;rect&quot;, xmin = pwr_95, xmax = 1, ymin = 0, ymax = 1, fill = &quot;green&quot;, alpha = .5) + theme_classic() print(power_plot) We can see the information conveyed in the graph above for a correlation study in table form using the code below. # Specify the sample size from your power analysis n = 261 # Do not modify the code below pwr_50 &lt;- pwr.r.test(n = n, power = .50)$r pwr_80 &lt;- pwr.r.test(n = n, power = .80)$r pwr_95 &lt;- pwr.r.test(n = n, power = .95)$r power_labels &lt;- c(&quot;dangerously low power (0 to .50)&quot;, &quot;low power (.50 to .80)&quot;, &quot;adequate power (.80 to .95)&quot;, &quot;almost certain power (&gt; .95)&quot;) power_table &lt;- data.frame(power = power_labels, rho_start = round(c(0, pwr_50, pwr_80, pwr_95),2), rho_end = round(c(pwr_50, pwr_80, pwr_95, NA),2)) print(power_table) ## power rho_start rho_end ## 1 dangerously low power (0 to .50) 0.00 0.12 ## 2 low power (.50 to .80) 0.12 0.17 ## 3 adequate power (.80 to .95) 0.17 0.22 ## 4 almost certain power (&gt; .95) 0.22 NA This table indicates that using 261 participants your statistical power will be: dangerously low if the actual population effect is higher than 0 and lower than \\(\\rho = .12\\) low if the actual population effect is between \\(\\rho = .12\\) and \\(\\rho = .17\\) adequate if the actual population effect is between \\(\\rho = 0.16\\) and \\(\\rho = 0.22\\) excellent if the actual population effect is higher than \\(\\rho = 0.22\\) 10.6.3 What will be signficant? Let’s assume that after looking at all the graphs and tables you end up sticking with your initial sample size of 261. What sample-level effect sizes will be significant when you do so? # Specify the sample size from your power / sample size analysis n &lt;- 261 # Use .95 for one-sided and .975 for two-sided tests percentile = .95 # Do not modify the code below t_critical &lt;- qt(percentile, df = (n - 2)) t2 &lt;- t_critical^2 r_critical = sqrt(t2 / (t2 + (n-2))) print(r_critical) ## [1] 0.102 This indicates that with a sample size of 261 that only sample correlations greater than \\(r\\) = .10 (rounded) will be significant. References "],["equivalence-tests-and-sample-size.html", "Chapter 11 Equivalence tests and sample size 11.1 Required 11.2 Interpreting non-significant findings 11.3 When would I use an equivalence test? 11.4 Possible Outcomes 11.5 What is an equivalence test 11.6 Defining a zero effect 11.7 Equivalence - repeated measures 11.8 Equivalence - Independent groups 11.9 Equivalence Correlation 11.10 Sample sizes for equivalence testing", " Chapter 11 Equivalence tests and sample size 11.1 Required The following CRAN packages must be installed: Required CRAN Packages MBESS TOSTER 11.2 Interpreting non-significant findings A common problem in the psychological literature is the interpretation of non-significant effects. As (Kirk Kirk 1996) notes “some researchers mistakenly interpret a failure to reject the null hypothesis as evidence for accepting it”. Indeed, it is unfortunately common to see an incorrect sentences like the following examples based on fictitious data: “The difference between the mean IQ’s for males and females was non-significant, \\(t\\)(98) = 12.247, p = .458, indicating that males and females have, on average, the same intelligence.” Or alternatively an incorrect sentence like: “The correlation between height and IQ was non-significant, \\(r\\) = .15, p = .854, indicating that height was not related to IQ.” Both of these sentences are statistically incorrect because the conclusions do not follow from the reported statistics. The tendency for researchers to incorrectly conclude that there is no effect, when \\(p\\) &gt; .05, is particularly troubling (and ubiquitous) when interpreting the interaction in an ANOVA. For example, consider a 2 (sex) by 2 (occasion) between/within ANOVA where the dependent variable is reaction time. Imagine there is a significant sex x occasion interaction. The significant interaction indicates the relation between occasion and response time depends on the level of sex. The researcher describes this significant interaction by comparing the reaction times of males and females at occasion 1 and then again at occasion 2: “A comparison of the mean reaction of times of males and females at occasion 1, \\(t\\)(28) = 1.06, \\(p\\) = .300, revealed no difference in reaction time. In contrast, at occasion 2, there was a difference in the mean reaction times of males and females, t(28) = 2.15, \\(p\\) = .040. Thus, reaction time were the same, on average, for males and females at occasion 1 but not occasion 2.” In this example the researcher erred in the their interpretation of the results at occasion 1. Specifically, the researcher incorrectly concluded at occasion 1 that the reaction time for males and females was the same because \\(p\\) &gt; .05. That type of conclusion is not possible for p &gt; .05 in a standard paired comparison / \\(t\\)-test. The calculation of a \\(p\\)-value begins by assuming the null hypothesis is true; consequently, you cannot use a \\(p\\)-value as evidence the null hypothesis is true. That is, when a \\(p\\)-value exceeds the threshold for significance (.05) you cannot conclude there is no effect. This fact is discussed at the 10 minute mark in an excellent video by Daniel Lakens. You might well wonder what to do if you do want to make the conclusion there this no effect/relation. This type of conclusion is possible but you need to use the right tool to do so. One tool for concluding there is no effect is the Bayes Factor BF - but that is beyond the scope of this course. An easily accessible alternative for concluding there is no effect or relation is the equivalence test (Lakens, Scheel, and Isager 2018). 11.3 When would I use an equivalence test? You can use an equivalent test when you want to conclude there is not effect or relation. This situation might be more common that you might think. A few of the scenarios where you would like to use an equivalence test are outlined below: 11.3.1 \\(t\\)-test You calculate a \\(t\\)-test and expect to find a difference between the two conditions. Unexpectedly, the hypothesized difference is non-significant. At this point you can’t draw much of a conclusion. You can say the groups were not statistically different but you cannot say the groups were statistically the same. An equivalence test could allow you to conclude the groups are statistically equivalent. For theoretical reasons your study may begin with the primary purpose being determine if two groups are the same (e.g., two treatments for the same disease that are believed to be equally effective). 11.3.2 Correlation You calculate a correlation and expect to find a relation between the two variables. Unexpectedly, the hypothesized relation is non-significant. At this point you can’t draw much of a conclusion. You can say you didn’t find evidence for a relation but you cannot say you found evidence of no relation. An equivalence test could allow you to conclude there is no relation between the two variables. For theoretical reasons the purpose of your study may be to provide evidence that there is no relation between two variables (e.g., video game use and violent behaviors). 11.4 Possible Outcomes (Lakens, Scheel, and Isager 2018) review a variety of outcomes from an equivalence test. These are easiest to understand in the context of a t-test with two groups. You could find the two groups are: Not statistically equivalent and not statistically different Statistically equivalent and not statistically different Statistically equivalent and statistically different Not statistically equivalent and statistically different You can see from the possible outcomes above it is still possible to obtain an outcome that is difficult to interpret. The outcomes that are challenging to interpret are most likely to occur when you have small sample sizes and low statistical power for the equivalence test. To avoid an ambiguous outcome from an equivalence test make sure you conduct a sample size analysis for an equivalence test prior to running your study. The sample size demands for an equivalence test may be substantially greater than for a traditional analysis. Therefore we encourage you to conduct both a traditional sample size analysis and a sample size analysis for an equivalence test before you start collecting data. 11.5 What is an equivalence test An equivalence test is just a pair of one-sided \\(t\\)-tests that are used to establish if an effect falls within a specified range of effect sizes bounding zero. That is, an equivalence test is used to indicate if an effect/relation is close enough to zero to be considered zero for practical purposes. 11.6 Defining a zero effect How do you decide how close to zero is close enough to be practically zero? You already did so in the the previous chapter on “NHST and sample size”. In that chapter you reviewed various ways of determining the smallest effect size of interest (SESOI). Any effect below the SESOI is logically close enough to zero to, for practical purposes, be zero. However, I encourage you to review (Lakens, Scheel, and Isager 2018) to see the full discussion on this issue. For now, the most important aspect of conducting an equivalence test is the fact that you need to determine the smallest effect size of interest prior to data collection - to avoid equivalence testing being a fancy form of intentional or unintentional \\(p\\)-hacking. Take note of this point – a recent article indicated that approximately 25% of researchers have engaged in at least one form of \\(p\\)-hacking in the just last 12 months. 11.7 Equivalence - repeated measures Consider the following scenario where you begin a study with the intent to prove there is no effect. Many years ago the cereal Shreddies engaged in an interested marketing strategy. They decided to market new Diamond Shreddies as illustrated below. Imagine that you are a researcher tasked to compare the taste of the two types of Shreddies. Participants are given a bowl of the old Shreddies and then asked to rate it on a 1 to 15 point scale where higher ratings indicate a better taste. Following this they are given a bowl of the new Diamond Shreddies and asked to rate the taste. How do you go about comparing the taste ratings if your goal is to establish the taste of the old Shreddies is the same as new Diamond Shreddies? An incorrect approach to determining if the two types of cereal taste the same would be to just conduct a repeated measures \\(t\\)-test and look for a non-significant difference. A non-significant repeated measures \\(t\\)-test would leave you with no conclusion. The appropriate approach in this circumstance is to use an equivalence test. Prior to collecting data you set your smallest effect size of interest. Specifically, you imagine getting a mean for each group and calculating a difference using the original numbers on the 15-point rating scale. You decide that if that difference is between -1 and +1 (the smallest jump on the rating scale) then you will consider the two types of Shreddies to have equivalent taste. Notably, as per (Lakens, Scheel, and Isager 2018), you set this smallest effect size of interest before you examine your data - to avoid being a \\(p\\)-hacker. 11.7.1 Raw units After you collect your data (N = 50) you have ratings for old Shreddies (M = 12.1, SD = 2.50) and new Diamond Shreddies (M = 11.9, SD = 2.50). Because the same people taste both cereals you also have a correlation between the two taste ratings of \\(r\\) = .80. You run the R-code below to conduct the equivalence test. library(TOSTER) TOSTpaired.raw(m1 = 12.1, sd1 = 2.5, m2 = 11.9, sd2 = 2.5, r12 = .8, n = 50, low_eqbound = -1, high_eqbound = 1, plot = FALSE) Equivalence Test Result: The equivalence test was significant, t(49) = -3.578, p = 0.000396, given equivalence bounds of -1.000 and 1.000 (on a raw scale) and an alpha of 0.05. Null Hypothesis Test Result: The null hypothesis test was non-significant, t(49) = 0.894, p = 0.375, given an alpha of 0.05. Based on the equivalence test and the null-hypothesis test combined, we can conclude that the observed effect is statistically not different from zero and statistically equivalent to zero. We can then report that: A repeated measures \\(t\\)-test indicated that the mean taste ratings for old Shreddies (M = 12.1, SD = 2.50) and new Diamond Shreddies (M = 11.9, SD = 2.50) were not significantly different, \\(d\\) = 0.13, 95% CI [-0.15, 0.40], \\(t\\)(49) = 0.984, \\(p\\) = 0.375. Prior to conducting analyses we established that a raw difference in the -1 to +1 range (the smallest possible change on the scale) would, for practical purposes, be considered equivalent to zero. The equivalence test was significant \\(t\\)(49) = -3.578, \\(p\\) &lt; .001 indicating the means for the two conditions were equivalent. Thus, we can conclude that the taste ratings of the two types of Shreddies are not statistically different and that they are statistically equivalent. Note that the \\(d\\)-value with 95% CI was obtained with the MBESS command: ci.sm(ncp = 0.894, N = 50) 11.7.2 Standardized units You could also have run this test by indicating the smallest effect size of interest using standardized effect size (i.e., a repeated measures \\(d\\)-value). The code below produces a result identical to the above code. We simply indicate the range of values that count as practically equivalent to zero using \\(d\\)-values. library(TOSTER) TOSTpaired(m1 = 12.1, sd1 = 2.5, m2 = 11.9, sd2 = 2.5, r12 = .8, n = 50, low_eqbound_dz = -0.6325, high_eqbound_dz = 0.6325, plot = FALSE) 11.8 Equivalence - Independent groups 11.8.1 Raw units You could also have run this study as an independent groups \\(t\\)-test where different people received each type of cereal. In this case, the R-code would be as below. Note the output in this case provides an ambiguous outcome: “the observed effect is statistically not different from zero and statistically not equivalent to zero” due to our small sample size. Thus, the primary finding from this study is that we should have used a larger number of participants. library(TOSTER) TOSTtwo.raw(m1 = 12.1, sd1 = 2.5, m2 = 11.9, sd2 = 2.5, n1 = 50, n2 = 50, low_eqbound = -1, high_eqbound = 1, plot = FALSE) Equivalence Test Result: The equivalence test was non-significant, t(98) = -1.600, p = 0.0564, given equivalence bounds of -1.000 and 1.000 (on a raw scale) and an alpha of 0.05. Null Hypothesis Test Result: The null hypothesis test was non-significant, t(98) = 0.400, p = 0.690, given an alpha of 0.05. Based on the equivalence test and the null-hypothesis test combined, we can conclude that the observed effect is statistically not different from zero and statistically not equivalent to zero. 11.8.2 Standardized units The R-code again for using standardized effect sizes: library(TOSTER) TOSTtwo(m1 = 12.1, sd1 = 2.5, m2 = 11.9, sd2 = 2.5, n1 = 50, n2 = 50, low_eqbound_d = -0.4, high_eqbound_d = 0.4, plot = FALSE) 11.9 Equivalence Correlation Imagine we are interested in conducting a study to prove our theory that there is a strong positive relation between academic performance and self-esteem. Prior to conducing the study we determined our smallest effect size of interest was .20. A traditional sample size analysis (with a desire for 90% power) indicated we should use a sample size of 210 (for a one-sided test). However, we also conducted a sample size analysis for an equivalence test, in case the correlation was non-significant, which suggested a larger sample size of 267. Consequently, we collected data from 267 students. Our study found \\(r\\) = .09, \\(p\\) = 0.071 (one-tailed). This non-significant difference means there is little to conclude from this study. We can’t say there is a relation due to the non-significance. But, we also can’t say there is no relation. Fortunately, because we established our smallest effect size of interest prior to looking at our data we can run an equivalence test with the code below. library(TOSTER) TOSTr(n = 267, r = .09, low_eqbound_r = -.20, high_eqbound_r = .20, plot = FALSE) Equivalence Test Result: The equivalence test was significant, p = 0.0338, given equivalence bounds of -0.200 and 0.200 and an alpha of 0.05. Null Hypothesis Test Result: The null hypothesis test was non-significant, p = 0.142, given an alpha of 0.05. Based on the equivalence test and the null-hypothesis test combined, we can conclude that the observed effect is statistically not different from zero and statistically equivalent to zero. Because we conducted the equivalence test we can write a clear results section. The relation between academic performance and self-esteem was non-significant, \\(r\\) = .09, 95% CI [-.03, .21], \\(p\\) = 0.071 (one-sided). A follow up equivalence test (based on our apriori smallest effect size of interest, \\(\\rho\\) = .20) was significant, \\(p\\) = 0.034, which indicates that for practical purposes the relation was equivalent to zero. That is, the observed correlation, \\(r\\) = .09, was not statistically different from zero and it was statistically equivalent to zero. Notice how much clearer and stronger this results section is with the inclusion of an equivalence test. Note that the 95% CI for the correlation was obtained with the MBESS command: ci.cc(r = .09, n = 267) 11.10 Sample sizes for equivalence testing 11.10.1 Independent t-test 11.10.1.1 Standardized units library(TOSTER) powerTOSTtwo(alpha = 0.05, statistical_power = 0.90, low_eqbound_d = -0.40, high_eqbound_d = 0.40) ## Note: this function is defunct. Please use power_t_TOST instead ## The required sample size to achieve 90 % power with equivalence bounds of -0.4 and 0.4 is 136 per group, or 272 in total. ## ## [1] 135.3 11.10.1.2 Raw units library(TOSTER) powerTOSTtwo.raw(alpha = 0.05, statistical_power = 0.90, sdpooled = 2.50, low_eqbound = -1, high_eqbound = 1) ## Note: this function is defunct. Please use power_t_TOST instead ## The required sample size to achieve 90 % power with equivalence bounds of -1 and 1 is 135.3 per group, or 272 in total. ## ## [1] 135.3 11.10.2 Repeated t-test 11.10.2.1 Standardized units library(TOSTER) powerTOSTpaired(alpha = 0.05, statistical_power = 0.90, low_eqbound_dz = -0.6325, high_eqbound_dz = 0.6325) ## Note: this function is defunct. Please use power_t_TOST instead ## The required sample size to achieve 90 % power with equivalence bounds of -0.6325 and 0.6325 is 28 pairs ## ## [1] 27.05 11.10.2.2 Raw units library(TOSTER) powerTOSTpaired.raw(alpha = 0.05, statistical_power = 0.90, sdif = 1.581, low_eqbound = -1, high_eqbound = 1) ## The required sample size to achieve 90 % power with equivalence bounds of -1 and 1 is 28 pairs ## ## [1] 27.05 11.10.3 Correlation library(TOSTER) powerTOSTr(alpha = .05, statistical_power = .90, low_eqbound_r = -.20, high_eqbound_r = .20) ## The required sample size to achieve 90 % power with equivalence bounds of -0.2 and 0.2 is 267 observations ## ## [1] 266.3 References "],["precision-and-sample-size.html", "Chapter 12 Precision and sample size 12.1 Required 12.2 Overview 12.3 Precision", " Chapter 12 Precision and sample size 12.1 Required The following CRAN packages must be installed: Required CRAN Packages MBESS TOSTER 12.2 Overview In the previous two chapter, we reviewed the sample size planning using the traditional NHST approach as well as equivalence testing. In this chapter, we focus on determining the appropriate sample size for your study using the precision approach. Specifically, we review determining the desired sample size based on the effect size and desired confidence interval width. 12.3 Precision A confidence interval provides an interval estimate of the population parameter. If you took multiple samples from a population and constructed a confidence interval for each sample the confidence intervals would likely all be different. More specifically, each confidence interval is likely to have a different range and width. Over a large number of samples 95% of the confidence intervals would overlap with the population parameter. Interpretation of confidence intervals can be challenging. Prior to collecting your data we say there is a 95% chance a constructed confidence interval will overlap with the population parameter. Once you have collected your data and obtained the end points for your sample confidence interval - interpretation is more difficult. Indeed, a specific interval will either overlap with the population parameter or not; unfortunately, you don’t know which is the case. Consequently, once you have a specific confidence interval with end points, based on your data, you should only interpret it as plausible range of values for the population parameter - don’t associate the 95% probability with interpretting the confidence interval once you have constructed it. Imagine you are about to conduct a correlation study (N = 100) and you plan to construct a confidence interval around that correlation. At this point, prior to data collection, there is a 95% chance that the confidence interval you construct around the sample correlation will overlap with the population correlation (i.e., the population parameter). You collect your data and find \\(r\\) = .35, 95% CI [.16, .51]. Now that you have a specific confidence interval how do you interpret it? You can say the confidence interval is plausible range of the values for the population correlation. You should not say there is a 95% chance that population correlation is in the .16 to .51 range. Why not? Once the data is collected and you have a specific interval with end points (e.g., .16 to .51) statisticians think of the interval as overlapping with the population correlation (a probability of 1.00) or not (a probability of 0). Thus, once you have a specific interval, after data collection, the probability that it contains the population parmater is 1.00 or 0 - you just don’t know which. So a convienient way to think about the range is by using the word plausible which is not associated with a probability. We simply say the confidence interval uses sample data to provide a plausible range of values for the population parameter (e.g., population correlation). Using confidence intervals for research can, from one perspective, be a fundamental shift in your thinking about how to interpret results. Null hypothesis significance testing allows for a simplistic conclusion - the population effect size may not be exactly zero. This is a form of categorical decision making: effect / no effect. In contrast, the using confidence intervals to interpret your data is an estimation approach: How large is the effect and how uncertain is my estimate of the effect size? Moreover, using the estimation approach makes it a small jump to drawing scientific conclusions using meta-analysis (see Schmidt and Hunter 2014). You can learn more about confidence interval / estimation approach by checking out this video workshop. Alternatively, you can check out these general articles Cumming (2009), Cumming (2014), Calin-Jageman and Cumming (2019b). Calin-Jageman and Cumming (2019a). There are two parts to planning using precision: Determining the effect size of interest (reviewed in previous chapters on smallest effect size of interest). Indicating the desired width of the confidence interval. Unfortunately, there is little consensus of how to pick the desired width of the confidence interval. Our sense is that the width of the confidence interval should not be larger than the effect size. That, is your uncertainty should not be larger than the effect itself. For example, if you specified a smallest size of interest as \\(\\delta\\) = 0.40 then you would specify a width of 0.40 as well. Likewise, with correlations, if you specified a smallest effect size of interest as \\(r\\) = .30 then you would indicate a desired with of .30 as well. More stringent advise was offered, indirectly by Geoff Cumming, on social media: Below I present a series of examples for determining the sample size for your study using the less stringent approach of setting the confidence interval width to be the same as the effect size. You can read more about planning for precision in Rothman and Greenland (2018). 12.3.1 Precision Independent t-test library(MBESS) ss.aipe.smd(delta = 0.40, width = 0.40) ## [1] 196 # Returns sample size per group 12.3.2 Precision Repeated t-test library(MBESS) ss.aipe.sm(sm = 0.40, width = 0.40) ## [1] 104 12.3.3 Precision Correlation library(MBESS) rho &lt;- .30 rho2 &lt;- rho * rho ss.aipe.R2(Population.R2 = rho2, width = rho2, p = 1) ## [1] &quot;The approximate sample size is given below; you should consider using the additional&quot; ## [1] &quot;argument &#39;verify.ss=TRUE&#39; to ensure the exact sample size value is obtained.&quot; ## $Required.Sample.Size ## [1] 565 References "],["positive-predictive-value.html", "Chapter 13 Positive Predictive Value 13.1 Required 13.2 Overview 13.3 Informal explanation 13.4 Formal explanation 13.5 Calculating power for your study 13.6 Calculating PPV for your study", " Chapter 13 Positive Predictive Value 13.1 Required The following CRAN packages must be installed: Required CRAN Packages pwr TOSTER 13.2 Overview Typically when students learn about p-values and Null Hypothesis Significance testing they tend to pair two ideas. Specifically, they tend to pair a significant \\(p\\)-value with the idea there is an effect. The purpose of this section is to unpair those two ideas. Specifically, a significant \\(p\\)-value only indicates that there may be an effect. At this point you’ve learned to be cautious when it comes to interpreting \\(p\\)-values. But so far, that’s been more of a qualitative form of caution. In this section we attempt to make it a more quantitative form of caution. Specifically, we introduce you to the concept of positive predictive values (PPV) in the context of interpreting significant p-values. The PPV statistic provides us with an estimate of how likely it is there really is an effect when a p-value is significant. The approach for this chapter was inspired by a blog post. I strongly encourage you to watch this video by the Center for Open Science on the consequences of low statistical power. 13.3 Informal explanation 13.3.1 Probability of an effect When you conduct an experiment to determine if an effect exists you start off fairly confident that the effect is there - otherwise you would not invest the time and money to run the study. But some research hypotheses will be true and others will not be true. One way to think about this is by imagining 1000 hypothesis as per Figure 13.1. In this figure there are 1000 squares. Each square represents an hypothesis worth testing. Just like the real world some hypotheses are true and some are false. We’ve shaded the squares to indicate which hypotheses are true or false. More specifically, we shaded 40% of the 1000 hypotheses (i.e., 400) green to indicate they are true (i.e., there is an effect). In contrast, we shaded 60% of the 1000 hypotheses (i.e., 600) yellow to indicate they are false (i.e., there is no effect). Why did we pick the percentage of hypotheses that are true in Figure 13.1 to be 40%? We did so because 40% could well correspond to the number of hypotheses that are true in psychology research overall. It’s difficult to assess the number of hypotheses that are true in psychology given that there is a tendency for only significant effects to be published. However, we can examine pre-registered studies to obtain an estimate of the percentage of hypotheses that are true. With pre-registration scientists register their hypotheses before they collect their data. Consequently looking at the percentage of pre-registered hypotheses that are true may be a reasonable estimate of the percentage of hypotheses that are true in general. Fortunately, a recent study examined a large set of pre-registered studies and determined that just over 40% of research hypotheses are true (Scheel, Schijen, and Lakens 2020). This finding correspondingly implies that approximately 60% of research hypotheses are false. That is, approximately 60% of studies fail to find the desired effect. We used these percentages to shade Figure 13.1. More specifically, we used these numbers to indicate P(effect) = .400 and the P(no effect) = .600. FIGURE 13.1: One-thousand hypotheses represented as squares. True hypotheses are indicated by the color green. False hypotheses are indicated by the color yellow. 13.3.2 Type I Error Even when an hypothesis is false there still a chance we will obtain a significant result (i.e., \\(p\\) &lt; .05). More specifically, we will obtain a significant result 5% of the time when the null hypothesis is true and there is no effect (i.e., \\(\\alpha = .05\\)). In terms of our example, this means that of the 600 false hypotheses, we would obtain a significant result for 30 of them (i.e., .05 * 600 = 30). These 30 false positive significance tests are illustrated in Figure 13.2 below. FIGURE 13.2: False positive significance tests. False hypotheses for which the researcher obtained a significant result are indicated by dark green squares. 13.3.3 Power Unfortunately, even when an hypothesis is true we won’t always obtain a significant p-value. Low sample sizes in psychology are common. As a result most studies in psychology only have a 30% chance of finding an effect if there is one. That is, the typical value for statistical power in psychology is around .30. This typical statistical power level is illustrated in the context of our example in Figure 13.3 below. In this figure, hypotheses that were true but obtained non-significant p-values are indicated by red squares. In contrast, the hypotheses that were true and obtained significant p-values are indicated by green squares; there are 120 green squares. FIGURE 13.3: True positive signficance tests. True hypotheses for which the researcher obtained a significant result are indicated by green squares. True hypotheses for which the researcher obtained a non-significant result are indicated by red squares. 13.3.4 Calculating PPV You can see from the blue outline in Figure 13.4, below, that there two ways to obtain a significant p-value. Specifically, you can obtain a significant p-value when the null hypothesis is true (a false positive p-value). As well, you can obtain a significant p-value when the null hypothesis is false (a true positive p-value). As a researcher, when we obtain a significant p-value we don’t know if it’s a true-positive or a false-positive. We calculate PPV to determine how likely it is there is an effect when the p-value is significant. FIGURE 13.4: Illustrating there are two ways to obtain a significant effect. As illustrated in Figure 13.5, below, the positive predictive value (PPV) is simply the proportion of statistically significant hypotheses that are true positives. FIGURE 13.5: Graphical illustration of the PPV formula. Placing numbers into the equation: \\[ \\begin{aligned} PPV &amp;= \\frac{120 }{120 + 30} \\\\ &amp;= \\frac{120}{150} \\\\ &amp;= 0.80 \\end{aligned} \\] We see that the positive predictive value is .80 in this circumstance. That is, if we obtain a significant p-value there is an 80% chance there really is an effect. 13.4 Formal explanation A more formal explanation of PPV can be made by thinking in terms of the area of square that is 1.0 units by 1.0 units. The length of each side corresponds a length of 1.0 because probabilities range from 0 to 1.0. Indeed, we can represent the probability of an hypothesis being true, P(effect), or false, P(no effect), along the right edge of the square. This is illustrated in Figure 13.6 below. FIGURE 13.6: Representing probabilities on a square 13.4.1 No effect Likewise, we can indicate the probability of a Type I Error on the diagram as well. Specifically, \\(\\alpha = .05\\) is the distance, moving right to left, from the top right corner of the square. This is illustrated in Figure 13.7 below. FIGURE 13.7: Representing Type I Error on the square So far the probability of there being no effect, P(no effect), was indicated by a length (.600). Likewise, the probability of there being a Type I Error was indicated by a length (.05). \\[ \\begin{aligned} P(\\text{no effect}) &amp;= 0.600 \\\\ P(\\text{Type I Error}) &amp;= 0.05 \\\\ \\end{aligned} \\] We can use these together to determine the dark green area (nearly black area) on Figure 13.7 which represents the joint probability of there being no effect and obtaining a Type I Error. The \\(\\cap\\) symbol indicates “and” which implies you need to multiply the two values. You can see the probability .030 is similar to the count of 30 we had for this area in the informal explanation. \\[ \\begin{aligned} P(\\text{Type I Error} \\cap \\text{no effect} ) &amp;= (\\alpha) \\cap P(\\text{no effect}) \\\\ &amp;= (.05)(.600) \\\\ &amp;= 0.030 \\end{aligned} \\] 13.4.2 Effect So far the probability of there being an effect, P(effect), was indicated by a length (.400). Likewise, the probability of finding that effect (i.e., power) can be indicated by a length (.30) as per Figure 13.8 below. FIGURE 13.8: Representing power on the square In this figure there are two important distances that represent probabilities as indicated below. Note that we often refer to power using the notation \\(1 - \\beta\\). \\[ \\begin{aligned} P(\\text{effect}) &amp;= 0.400 \\\\ (1 - \\beta) &amp;= 0.30 \\\\ \\end{aligned} \\] We can use these together to determine the dark green area on the figure which represents the joint probability of there being an effect and obtaining a significant p-value. You can see the joint probability .030, below, is similar to the count of 30 we had for this area in the informal explanation. \\[ \\begin{aligned} P(\\text{power} \\cap \\text{effect}) &amp;= (1 - \\beta) \\cap P(\\text{effect})\\\\ &amp;= (.30)(.400) \\\\ &amp;= 0.120 \\end{aligned} \\] 13.4.3 Calculation We can then calculate positive predictive value using the same logic as the informal case - but using formal equations. You can see in the equations below that some probabilities (.030, .120, and .150) correspond to the counts (30, 120, and 150) from the informal explanation above. \\[ \\begin{aligned} PPV &amp;= \\frac{P(\\text{effect} \\cap \\text{sig. p-value})}{P(\\text{effect} \\cap \\text{sig. p-value}) + P(\\text{no effect} \\cap \\text{sig. p-value})} \\\\ &amp;= \\frac{(1 - \\beta)P(\\text{effect})}{(1 - \\beta)P(\\text{effect}) + \\alpha P(\\text{no effect})}\\\\ &amp;= \\frac{(.30)(.400)}{(.30)(.400) + (.05)(.600)}\\\\ &amp;= \\frac{0.120}{0.120 + 0.030}\\\\ &amp;= \\frac{0.120}{0.150}\\\\ &amp;= 0.80\\\\ \\end{aligned} \\] Thus, the positive predictive value is .80. This indicates that if you obtain a significant p-value in your study there is an 80% chance there really is an effect. 13.4.4 Alternative calculation You might also see positive predictive values expressed using an odds ratio (OR). This can create confusion - but it’s important to realize this alternative version of the formula is just an algebraic rearrangement. You can see this alternative version of the formula below and note that you obtain the same answer. \\[ \\begin{aligned} PPV &amp;= \\frac{(1 - \\beta)(OR)}{(1 - \\beta)(OR) + \\alpha}\\\\ &amp;= \\frac{(1 - \\beta)(\\frac{P(\\text{effect})}{P(\\text{no effect})})}{(1 - \\beta)(\\frac{P(\\text{effect})}{P(\\text{no effect})}) + \\alpha}\\\\ &amp;= \\frac{(.30)(\\frac{.400}{.600})}{(.30)(\\frac{.400}{.600}) + \\alpha}\\\\ &amp;= \\frac{(.30)(0.6666667)}{(.30)(0.6666667) + .05}\\\\ &amp;= \\frac{.20}{.20 + .05}\\\\ &amp;= 0.80\\\\ \\end{aligned} \\] 13.4.5 Touchstone You may, or may not, have noticed something familiar when we started working with the square as means of describing the numbers required for the positive predictive value calculation. Specifically, the square is very similar to the standard 2 x 2 table used to represents Type I Errors (false positives) and Type II Errors (false negatives). In Figure 13.9 I’ve placed a colour coded version of this table so that you can see the similarities with the calculation approach used above. FIGURE 13.9: Table illustrating conclusion errors. 13.5 Calculating power for your study Although we do a sample size analysis before collecting data - we are often unable to obtain the desired sample size. We can determine the consequences of failing to obtain the desired sample size by calculating positive predictive value. The first step in doing so is calculating the power for your study based on the number of participants you actually obtained. To calculate the power for your study you need to use the same estimate of the population effect size that you used in original sample size analysis. You merely enter that same smallest effect size of interest (i.e., population level effect) and your obtained sample size into the R functions and it will return the power for your study - based on the sample size you actually obtained. The examples below all illustrate a population-level effect (i.e., smallest effect size of interest from the original power analysis) and the obtained sample size. Check out the next section to see how to use this information to calculate the positive predictive value for your study. 13.5.1 Power independent groups \\(t\\)-test library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_d &lt;- .24 sample_size_per_group &lt;- 25 pwr_out &lt;- pwr.t.test(d = pop_d, n = sample_size_per_group, type = &quot;two.sample&quot;, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## Two-sample t test power calculation ## ## n = 25 ## d = 0.24 ## sig.level = 0.05 ## power = 0.2095 ## alternative = greater ## ## NOTE: n is number in *each* group This analysis reveals that a) if we assume the population effect size is .24 (\\(\\delta = .24\\)) and b) use a sample size of 25 per group, then our resulting power is 0.21. That is, if an effect exists, we have a 21% chance of finding it. 13.5.2 Power repeated measures \\(t\\)-test library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_d &lt;- .17 n &lt;- 30 pwr_out &lt;- pwr.t.test(d = pop_d, n = n, type = &quot;paired&quot;, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## Paired t test power calculation ## ## n = 30 ## d = 0.17 ## sig.level = 0.05 ## power = 0.2311 ## alternative = greater ## ## NOTE: n is number of *pairs* This analysis reveals that a) if we assume the population effect size for our repeated measures \\(t\\)-test is 0.17 (\\(\\delta = 0.17\\)) and b) we have a sample size of 30, then our resulting power is 0.23. That is, if an effect exists, we have a 23% chance of finding it. 13.5.3 Power correlations library(pwr) # Based on the previous study modify the settings below. # For alternative: use &quot;greater&quot; for one-sided # and &quot;two.sided&quot; for two-sided test alternative &lt;- &quot;greater&quot; pop_r &lt;- .18 n &lt;- 100 pwr_out &lt;- pwr.r.test(r = pop_r, n = n, alternative = alternative) Then we need to print our power / sample size analysis: print(pwr_out) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 100 ## r = 0.18 ## sig.level = 0.05 ## power = 0.5623 ## alternative = greater This analysis reveals that a) if we assume the population correlation is .18 (\\(\\rho = .18\\)) and b) we use a sample size of 100, then our resulting power is 0.56. That is, if an effect exists, we have a 56% chance of finding it. 13.5.4 Equivalence independent t-test 13.5.4.1 Standardized units library(TOSTER) n_per_group = 40 powerTOSTtwo(alpha = 0.05, N = n_per_group, low_eqbound_d = -0.40, high_eqbound_d = 0.40) ## Note: this function is defunct. Please use power_t_TOST instead ## The statistical power is 11.51 % for equivalence bounds of -0.4 and 0.4 . ## ## [1] 0.1151 13.5.4.2 Raw units library(TOSTER) n_per_group = 40 powerTOSTtwo.raw(alpha = 0.05, N = n_per_group, sdpooled = 2.50, low_eqbound = -1, high_eqbound = 1) ## Note: this function is defunct. Please use power_t_TOST instead ## The statistical power is 11.51 % for equivalence bounds of -1 and 1 . ## ## [1] 0.1151 13.5.5 Equivalence repeated t-test 13.5.5.1 Standardized units library(TOSTER) n &lt;- 10 powerTOSTpaired(alpha = 0.05, N = n, low_eqbound_dz = -0.6325, high_eqbound_dz = 0.6325) ## Note: this function is defunct. Please use power_t_TOST instead ## The statistical power is 27.79 % for equivalence bounds of -0.6325 and 0.6325 . ## ## [1] 0.2779 13.5.5.2 Raw units library(TOSTER) n &lt;- 10 powerTOSTpaired.raw(alpha = 0.05, N = n, sdif = 1.581, low_eqbound = -1, high_eqbound = 1) ## The statistical power is 27.79 % for equivalence bounds of -1 and 1 . ## ## [1] 0.2779 13.5.6 Equivalence correlation library(TOSTER) n &lt;- 75 powerTOSTr(alpha = .05, N = n, low_eqbound_r = -.20, high_eqbound_r = .20) ## The statistical power is 6.09 % for equivalence bounds of -0.2 and 0.2 . ## ## [1] 0.06086 13.6 Calculating PPV for your study To calculate PPV for your study you simply use the previously presented formula for positive predictive value. Imagine that you conducted an independents groups t-test and determined your power was .21 you would calculate positive predictive value as below. You may assume the probability of your hypothesis being true is .400. \\[ \\begin{aligned} PPV &amp;= \\frac{(1 - \\beta)P(\\text{effect})}{(1 - \\beta)P(\\text{effect}) + \\alpha P(\\text{no effect})}\\\\ &amp;= \\frac{(.21)(.400)}{(.21)(.400) + (.05)(.600)}\\\\ &amp;= 0.74\\\\ \\end{aligned} \\] Thus, if you obtained a significant p-value in this scenario there is only a 74% chance there really is an effect. References "],["regression-and-correlation.html", "Chapter 14 Regression and correlation 14.1 Population example 14.2 Consider a sample 14.3 Comparing correlations", " Chapter 14 Regression and correlation The following CRAN packages must be installed: Required CRAN Packages tidyverse apaTables cocor janitor psych 14.1 Population example Consider the following the scenario, we want to examine the extent to which IQ predicts video game scores for people who live in the City of Guelph. We want to make conclusions about people who live in the City of Guelph so we refer to Guelph citizens as our population. Because we are interested in using IQ to predict video game score we refer to IQ as the predictor. The value being predicted is video game score and we refer to that variable as the criterion (i.e., the dependent variable). Imagine, for a moment, that we are actually able to get an IQ and a video game score for everyone in the City of Guelph (N = 100,000). 14.1.1 No predictor We illustrate the range and variability of video game scores for everyone in the population below. There are 100,000 blue dots on this graph. Each blue dot represents a person in the population. The vertical position of the dot indicates each person’s video game. Notice the large number of dots (i.e., people) - there are so many that it’s hard to see individual dots/people. The dots illustrate the range of video game scores - everyone did not obtain the same score. We want to try to understand why, in this population, some people have higher vs lower video game scores. Said another way, we want to explain, for this population, the variability in video game scores. Correlation/regression can never provide evidence of causation (or explanation) but we can use those analyses to find a pattern in our data that is consistent with a causal relation. Then we conduct a second experimental study to determine if their really is a causal relation. Without a predictor variable, our best estimate of a person’s video game score is the population mean. Moreover, without a predictor, we have the same estimate for everyone in the population - the mean video game score for the population. We have no way of creating an individualized estimate of someone’s video game score. The mean video game score is illustrated in the figure below with a horizontal green line. 14.1.2 Weak relation We can plot a variable (e.g., extraversion) against video game score to see if there is a relation between the two. In this case there doesn’t appear to be a relation. Indeed, this graph illustrates a zero correlation between extraversion and video game scores - the weakest possible relation for a predictor. Effectively, there is no linear relation between extraversion and video game scores. As before, we place a horizontal green line on the graph to indicate the mean video game score. We also place a red regression line (i.e., best-fit line) on the graph; however, the red line is completely hidden by the green line representing the mean video game score. In this case, with these data, knowing a person’s extraversion score does not allow us to provide an individualized estimate of a person’s video game score. As a result, extraversion does not help us explain the variability in video game scores. That is, extraversion scores do not allow us to explain why some people have high video game score whereas other people have low video game scores. 14.1.3 Strong relation What if we were to try another variable – like IQ? The graph below illustrates a positive linear relation between IQ and video game score. As before, we place a green horizontal line on the graph to indicate the population mean. Additionally, we place a red regression line (i.e., a best-fit line) on the graph. This red regression line represents, for each person, an individualized estimate of video game score based on their IQ. The population-level regression line has the slope 8.00; which indicate that as IQ increase by 1.0 point video game score increases by 8.00 points. The equation for the regression line is: \\[ \\begin{aligned} \\widehat{score} &amp;= 8.00(IQ) + 200.31 \\end{aligned} \\] Or using the more generic X/Y notation: \\[ \\begin{aligned} \\hat{Y} &amp;= 8.00(X) + 200.31 \\\\ \\end{aligned} \\] You can see in the graph below that, for some people, the individualized estimate of video game score (i.e., the y-axis position of the red line) is higher than the population mean. That is, in some cases the red regression is line is higher than the green line for the population mean. For other people, the individualized estimate of video game score (i.e., the y-axis position of the red line) is lower than the population mean. That is, in some cases the red regression is line is lower than the green line for the population mean. It appears that individuals with a high IQ tend to have a high video game score whereas individuals with a low IQ tend of have a low video game score. The regression line provides a more nuanced estimate for individual’s video game score than you can obtain by simply using the same estimation (i.e., the population mean) for everyone. But how good is this model of the data? There are a variety of way of assessing model fit. We present two below. First, when you are only concerned about how well a single predictor performs, as is the case here, you can use a correlation. The symbol for the population correlation is \\(\\rho\\). In these data, \\(\\rho = .60\\). The correlation coefficient can range from -1 to 1. The further the correlation is from zero - the stronger the relation between the predictor and the criterion. That is, the further the correlation is from zero the better a linear model fits the data. A positive correlation indicates that as one variable increases the other variable also increases. A negative correlation indicates that as one variable increases the other variable decreases. Cohen’s benchmarks are below: Cohen (1988) Label Value Small \\(\\rho\\) = .10 Medium \\(\\rho\\) = .30 Large \\(\\rho\\) = .50 An alternative, and more general, means of assessing the quality of statistical model is to use \\(R^2\\). This indexes the proportion of video game scores that are accounted for by a statistical model. One important attribute of \\(R^2\\) is that can be used when there are multiple predictors. When there is only one predictor, \\(R^2\\) is simply the correlation squared. Thus, at the population level, when there is only one predictor: \\[ \\begin{aligned} R^2 = \\rho^2 \\end{aligned} \\] To summarize, the population-level values (i.e., parameters): Slope = 8.00 \\(\\rho = .60\\) \\(R^2 = .36\\) In the next section we use sample-level data to estimate these population-level values. 14.2 Consider a sample Unfortunately, we never/rarely have data for everyone in the population (in this example everyone in the City of Guelph). Consequently, we usually have to select a subset of the population as a sample and use sample data for our analyses. In the figure below there are 100,000 blue dots - each dot represents an individual in the population. Additionally, there are also 9 black dots. These black dots are a random subset of the population – our sample. We will use the data from our sample (i.e., the 9 black dots) to estimate the slope, correlation, and \\(R^2\\) for the population (i.e., the 100,000 blue dots). We always need to remember the values calculated from our sample (statistics) are only estimates of the population-level parameters; estimates that are likely are likely to differ from population parameters due to sampling error. Let’s look at our sample in more detail. Notice, in the figure below, that there is one data point (\\(X_i\\), \\(Y_i\\)) for each of the 9 people. We illustrate the mean video game score for the 9 people with the horizontal green line. We can calculate the variance of the video game scores using the formula below. \\[ \\begin{aligned} s^2_{score} = \\frac{\\sum{(Y_i - \\bar{Y})^2}}{N-1} \\end{aligned} \\] It’s often useful to just focus on the numerator of this equation. We call this Sum of Squares Total (SSR): \\[ \\begin{aligned} SS_{Total} = \\sum{(Y_i - \\bar{Y})^2} \\end{aligned} \\] The values used in the Sum of Squares Total calculation are illustrated in the figure below. The vertical blue line indicates the difference between \\(Y_i\\) and \\(\\bar{Y}\\). The \\(SS_{Total}\\) value indexes the variability of the actual video game scores around the sample mean. In the figure below we add the regression line in red. The regression line is a statistical model for the data (a best-fit line). The regression line will always run through the joint mean of the two variables (i.e., the [\\(\\bar{X}\\), \\(\\bar{Y}\\)] point). Recall that the regression line represents an individualized estimate of each person’s video game score derived from their IQ (via the regression equation). Later we will show you how to calculate the regression line. For now, accept that the regression equation for the sample in standard notation is: \\[ \\begin{aligned} \\hat{Y_i} &amp;= 9.44(X_i) -21.21 \\\\ \\end{aligned} \\] And contextualized to the variable names is: \\[ \\begin{aligned} \\widehat{score_i} &amp;= 9.44(IQ_i) -21.21 \\end{aligned} \\] Therefore, the predicted value for Jane is: \\[ \\begin{aligned} \\widehat{score_i} &amp;= 9.44(IQ_i) -21.21 \\\\ &amp;= 9.44(71) - 21.21 \\\\ &amp;= 649 \\text{(rounded)} \\\\ \\end{aligned} \\] You can see this calculation for everyone in the sample: In the graph below each person is represented by a blue dot. The estimate of each person’s video game score, \\(\\hat{Y_i}\\), derived from the regression equation, is indicated by a red dot on the red regression line. The vertical blue lines are used to indicate, for each person (i.e., blue dot, \\(Y_i\\)), the estimate of their video game score (i.e., red dot, \\(\\hat{Y_i}\\)). We can calculate the extent to which individualized estimates are better than the sample mean for modeling the data. That is, we can calculate the extent to which the regression line is better at modeling the data than the mean line. We do so by calculating the extent to which the individualized estimates on the regression line differ from the sample mean. This is done with the calculation below for the Sum of Squares Regression. \\[ \\begin{aligned} SS_{Regression} = \\sum{(\\hat{Y_i} - \\bar{Y})^2} \\end{aligned} \\] The values used in the Sum of Squares Regression calculation are illustrated in the figure below. The vertical red line indicates the difference between \\(\\hat{Y_i}\\) and \\(\\bar{Y}\\). The \\(SS_{Regression}\\) value indexes the variability of the estimates of video game scores around the sample mean. The longer the vertical red line (i.e., the larger the \\((\\hat{Y_i} - \\bar{Y})\\) difference) the better the model. Longer vertical red lines are associated with models that do a better job of accounting for variability in video scores. So far we have calculated two values, \\(SS_{Regression}\\) and \\(SS_{Total}\\). The \\(SS_{Total}\\) value indexes the variability of actual video game scores about the sample mean (it’s the numerator for the variance calculation). In contrast, \\(SS_{Regression}\\) indexes the variability of estimated video game scores about the sample mean. We can calculate the proportion of the variability in actual scores accounted for the statistical model (i.e., regression line) using \\(R^2\\): \\[ \\begin{aligned} R^2 = \\frac{SS_{Regression}}{SS_{Total}} \\end{aligned} \\] We can also think of this as in terms of variance (because N-1 terms cancel each other out). \\[ \\begin{aligned} R^2 &amp;= \\frac{\\text{Variance of predicted scores}}{\\text{Variance of actual scores}} \\\\ &amp;= \\frac{\\frac{\\sum{(\\hat{Y_i} - \\bar{Y})^2}}{N-1}}{\\frac{\\sum{(Y_i - \\bar{Y})^2}}{N-1}} \\\\ &amp;= \\frac{\\sum{(\\hat{Y_i} - \\bar{Y})^2}}{\\sum{(Y_i - \\bar{Y})^2}} \\\\ &amp;= \\frac{SS_{Regression}}{SS_{Total}}\\\\ \\end{aligned} \\] 14.2.1 Regression Let’s obtain the actual value for \\(R^2\\), as well as the slope, using R. We can obtain the regression model (i.e.. linear model or lm) using the command below: lm_object &lt;- lm(video_game ~ iq, data = sample_data) We display the result using apaTables: library(apaTables) apa.reg.table(lm_object, table.number = 1, filename = &quot;regression_table.doc&quot;) Which produces the output: If we examine the fit column on the far right of the output above we see \\(R^2\\) = .74, 95% CI [.17, .86]. This value indicates that in this sample 74% of the variability in video scores is accounted accounted for by the statistical model (i.e., red regression line). The confidence interval suggests a plausible range of values for the \\(R^2\\) at the population-level is .17 to .86. Notice that this range captures that population-level \\(R^2\\) of .36 that we calculated from the entire population previously. \\[ \\begin{aligned} R^2 &amp;= .736 = .74\\\\\\\\ \\end{aligned} \\] If we examine the b column in the output we can create the regression equations below: \\[ \\begin{aligned} \\hat{Y_i} &amp;= 9.44(X_i) -21.21 \\\\ \\widehat{score_i} &amp;= 9.44(IQ_i) -21.21\\\\ \\end{aligned} \\] This tells use the slope in our sample is 9.44, 95% CI [4.39, 14.50]. That is, in the sample, each IQ point is associated with an additional 9.44 points in the video game. The population regression line might have a smaller/larger slope. The 95% confidence intervals tells us that a plausible range of values for the slope of the regression line at the population-level is 4.39 to 14.50. Notice that this range captures that population-level slope of 8.00 that we calculated from the entire population previously. Additional regression details are provided with the command below. summary(lm_object) ## ## Call: ## lm(formula = video_game ~ iq, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -201.2 -80.4 58.6 79.8 151.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21.21 225.86 -0.09 0.9278 ## iq 9.44 2.14 4.42 0.0031 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130 on 7 degrees of freedom ## Multiple R-squared: 0.736, Adjusted R-squared: 0.698 ## F-statistic: 19.5 on 1 and 7 DF, p-value: 0.00309 14.2.2 Correlation As discussed previously, a correlation can be considered a fit index for a linear regression line. That is, a correlation indicates the extent to which the data fit a straight line (i.e., the extent to which the data fit a linear model). Correlation values range between -1 and +1. The further a correlation value is from 0 the more tightly points will cluster around the regression line. A positive correlation indicates that as one value increases the other value increases. For example, as height increases weight increases. A negative correlation indicates that as one value increases the other values decreases. For example, as study time increases the number of errors on an exam decreases. A few possible positive correlations are illustrated below – notice the relation between the graph and the strength of the correlation. You can obtain the correlation from our sample data with the command below: cor.test(sample_data$iq, sample_data$video_game, na.action = &quot;pairwise.complete.obs&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: sample_data$iq and sample_data$video_game ## t = 4.4, df = 7, p-value = 0.003 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4503 0.9696 ## sample estimates: ## cor ## 0.8579 From this output we extract the numbers below in APA reporting style: There was a positive relation between IQ and video game score, such that as IQ increased, so did video game score, \\(r\\) = .86, 95% CI[.45, .97], \\(p\\) = .003, N = 9. 14.2.3 Graphing A scatter plot can be made code below: my_plot &lt;- ggplot(data = sample_data, mapping = aes(x = iq, y = video_game)) + geom_point(color = &quot;blue&quot;, size = 4) + coord_cartesian(xlim = c(70, 140), ylim = c(400, 1500)) + scale_y_continuous(breaks = seq(400, 1500, by = 200)) + scale_x_continuous(breaks = seq(70, 140, by = 20)) + labs(x = &quot;IQ&quot;, y = &quot;Video Game Score&quot;) + theme_classic(24) 14.3 Comparing correlations In this part of the chapter we compare correlations within and across studies we do so with the cocor package. We begin by obtaining a data set from the psych package. Note that we do not use the library(psych) command due to conflicts with the tidyverse. # Obtain the bfi data set from the psych package bfi &lt;- psych::bfi # remove empty rows/columns and clean the variable names bfi &lt;- bfi %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() Check out the large number of columns. glimpse(bfi) ## Rows: 2,800 ## Columns: 28 ## $ a1 &lt;int&gt; 2, 2, 5, 4, 2, 6, 2, 4, 4, 2, 4, 2, 5, 5… ## $ a2 &lt;int&gt; 4, 4, 4, 4, 3, 6, 5, 3, 3, 5, 4, 5, 5, 5… ## $ a3 &lt;int&gt; 3, 5, 5, 6, 3, 5, 5, 1, 6, 6, 5, 5, 5, 5… ## $ a4 &lt;int&gt; 4, 2, 4, 5, 4, 6, 3, 5, 3, 6, 6, 5, 6, 6… ## $ a5 &lt;int&gt; 4, 5, 4, 5, 5, 5, 5, 1, 3, 5, 5, 5, 4, 6… ## $ c1 &lt;int&gt; 2, 5, 4, 4, 4, 6, 5, 3, 6, 6, 4, 5, 5, 4… ## $ c2 &lt;int&gt; 3, 4, 5, 4, 4, 6, 4, 2, 6, 5, 3, 4, 4, 4… ## $ c3 &lt;int&gt; 3, 4, 4, 3, 5, 6, 4, 4, 3, 6, 5, 5, 3, 4… ## $ c4 &lt;int&gt; 4, 3, 2, 5, 3, 1, 2, 2, 4, 2, 3, 4, 2, 2… ## $ c5 &lt;int&gt; 4, 4, 5, 5, 2, 3, 3, 4, 5, 1, 2, 5, 2, 1… ## $ e1 &lt;int&gt; 3, 1, 2, 5, 2, 2, 4, 3, 5, 2, 1, 3, 3, 2… ## $ e2 &lt;int&gt; 3, 1, 4, 3, 2, 1, 3, 6, 3, 2, 3, 3, 3, 2… ## $ e3 &lt;int&gt; 3, 6, 4, 4, 5, 6, 4, 4, NA, 4, 2, 4, 3, … ## $ e4 &lt;int&gt; 4, 4, 4, 4, 4, 5, 5, 2, 4, 5, 5, 5, 2, 6… ## $ e5 &lt;int&gt; 4, 3, 5, 4, 5, 6, 5, 1, 3, 5, 4, 4, 4, 5… ## $ n1 &lt;int&gt; 3, 3, 4, 2, 2, 3, 1, 6, 5, 5, 3, 4, 1, 1… ## $ n2 &lt;int&gt; 4, 3, 5, 5, 3, 5, 2, 3, 5, 5, 3, 5, 2, 1… ## $ n3 &lt;int&gt; 2, 3, 4, 2, 4, 2, 2, 2, 2, 5, 4, 3, 2, 1… ## $ n4 &lt;int&gt; 2, 5, 2, 4, 4, 2, 1, 6, 3, 2, 2, 2, 2, 2… ## $ n5 &lt;int&gt; 3, 5, 3, 1, 3, 3, 1, 4, 3, 4, 3, NA, 2, … ## $ o1 &lt;int&gt; 3, 4, 4, 3, 3, 4, 5, 3, 6, 5, 5, 4, 4, 5… ## $ o2 &lt;int&gt; 6, 2, 2, 3, 3, 3, 2, 2, 6, 1, 3, 6, 2, 3… ## $ o3 &lt;int&gt; 3, 4, 5, 4, 4, 5, 5, 4, 6, 5, 5, 4, 4, 4… ## $ o4 &lt;int&gt; 4, 3, 5, 3, 3, 6, 6, 5, 6, 5, 6, 5, 5, 4… ## $ o5 &lt;int&gt; 3, 3, 2, 5, 3, 1, 1, 3, 1, 2, 3, 4, 2, 4… ## $ gender &lt;int&gt; 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1… ## $ education &lt;int&gt; NA, NA, NA, NA, NA, 3, NA, 2, 1, NA, 1, … ## $ age &lt;int&gt; 16, 18, 17, 17, 17, 21, 18, 19, 19, 17, … Let’s select a small subset of the columns for our example: bfi &lt;- bfi %&gt;% select(a1, c1, e1, o1, gender) You can confirm the smaller set of columns: glimpse(bfi) ## Rows: 2,800 ## Columns: 5 ## $ a1 &lt;int&gt; 2, 2, 5, 4, 2, 6, 2, 4, 4, 2, 4, 2, 5, 5, 4… ## $ c1 &lt;int&gt; 2, 5, 4, 4, 4, 6, 5, 3, 6, 6, 4, 5, 5, 4, 5… ## $ e1 &lt;int&gt; 3, 1, 2, 5, 2, 2, 4, 3, 5, 2, 1, 3, 3, 2, 3… ## $ o1 &lt;int&gt; 3, 4, 4, 3, 3, 4, 5, 3, 6, 5, 5, 4, 4, 5, 5… ## $ gender &lt;int&gt; 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1… These columns are single items from a personality measure. a1 (Agreeableness) c1 (Conscientiousness) e1 (Extraversion) o1 (Opennness) You can obtain a condensed correlation matrix using the cor() command. You can specify use = “pairwise.complete.obs” for pairwise correlation - the documentation covers other options. The round(2) command rounds the correlations to two decimal places. cor(bfi, use = &quot;pairwise.complete.obs&quot;) %&gt;% round(2) ## a1 c1 e1 o1 gender ## a1 1.00 0.03 0.11 0.01 -0.16 ## c1 0.03 1.00 -0.02 0.17 0.01 ## e1 0.11 -0.02 1.00 -0.10 -0.13 ## o1 0.01 0.17 -0.10 1.00 -0.10 ## gender -0.16 0.01 -0.13 -0.10 1.00 Or we could use apaTable apa.cor.table() command: library(apaTables) apa.cor.table(bfi, table.number = 1, filename = &quot;table_1_bfi.doc&quot;) Inspecting the above table you see that the correlation between a1 and c1 with is r = .03, 95% CI [-.01, .07]. Likewise, the correlation between e1 and o1 is r = -.10, 95% CI [-.14, -.06]. We obtain the p-values for these relations below. 14.3.1 p-values The code below obtains the p-value for the a1/c1 relation - a value of .144. cor.test(bfi$a1, bfi$c1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi$a1 and bfi$c1 ## t = 1.5, df = 2762, p-value = 0.1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.00949 0.06502 ## sample estimates: ## cor ## 0.0278 The code below obtains the p-value for the e1/o1 relation - a value sufficiently small we report it as \\(p\\) &lt; .001 cor.test(bfi$e1, bfi$o1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi$e1 and bfi$o1 ## t = -5.3, df = 2757, p-value = 1e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.13718 -0.06329 ## sample estimates: ## cor ## -0.1004 With p-values in hand we can write this up as: Inspecting the above table you see that the correlation between a1 and c1 with is r = .03, 95% CI [-.01, .07], p = .144. Likewise, the correlation between e1 and o1 is r = -.10, 95% CI [-.14, -.06], p &lt; .001. 14.3.2 Within a data set In this section we look at comparing correlation within a single data set. 14.3.2.1 Non-overlapping correlations We will compare the correlation to between (a1, c1) to the correlation between (e1, o1) with the cocor package. In this case, because neither of the variables in the first correlation (a1, c1) are in the second correlation (e1, o1) we refer to this as a non-overlapping correlation comparison. The cocor command will provide a lot of output. We are most interested in the last part of the output corresponding to Zou (2007) which provides the confidence interval for the difference in the correlations. But we also want to examine the first part of the output which will show us the two original correlations (a1, c1) = 0.0276, and (e1, o1) = -0.1002 (these are the .03 and -.10 values prior to rounding). As well it also shows us the difference between them, Difference: r.jk - r.hm = 0.1278. library(cocor) cocor( ~ a1 + c1 | e1 + o1, data = as.data.frame(bfi)) ## ## Results of a comparison of two nonoverlapping correlations based on dependent groups ## ## Comparison between r.jk (a1, c1) = 0.0276 and r.hm (e1, o1) = -0.1002 ## Difference: r.jk - r.hm = 0.1278 ## Related correlations: r.jh = 0.1036, r.jm = 0.0125, r.kh = -0.0259, r.km = 0.1688 ## Data: as.data.frame(bfi): j = a1, k = c1, h = e1, m = o1 ## Group size: n = 2724 ## Null hypothesis: r.jk is equal to r.hm ## Alternative hypothesis: r.jk is not equal to r.hm (two-sided) ## Alpha: 0.05 ## ## pearson1898: Pearson and Filon&#39;s z (1898) ## z = 4.7832, p-value = 0.0000 ## Null hypothesis rejected ## ## dunn1969: Dunn and Clark&#39;s z (1969) ## z = 4.7676, p-value = 0.0000 ## Null hypothesis rejected ## ## steiger1980: Steiger&#39;s (1980) modification of Dunn and Clark&#39;s z (1969) using average correlations ## z = 4.7671, p-value = 0.0000 ## Null hypothesis rejected ## ## raghunathan1996: Raghunathan, Rosenthal, and Rubin&#39;s (1996) modification of Pearson and Filon&#39;s z (1898) ## z = 4.7676, p-value = 0.0000 ## Null hypothesis rejected ## ## silver2004: Silver, Hittner, and May&#39;s (2004) modification of Dunn and Clark&#39;s z (1969) using a backtransformed average Fisher&#39;s (1921) Z procedure ## z = 4.7671, p-value = 0.0000 ## Null hypothesis rejected ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r.jk - r.hm: 0.0753 0.1800 ## Null hypothesis rejected (Interval does not include 0) We could write this as: There was a negative weak relation between extraversion (e1) and openness (o1) such that as extraversion increased openness decreased, r = -.10, 95% CI [-.14, -.06], p &lt; .001. In contrast, the relation between agreeableness (a1) and conscientiousness(c1) was non-significant, r = .03, 95% CI [-.01, .07], p = .144. The extraversion/openness relation was stronger than the agreeableness/conscientiousness relation, \\(\\Delta\\)r = .13, 95% CI [.07, .18], p &lt; .001. 14.3.2.2 Overlapping correlations We will compare the correlation to between (a1, c1) to the correlation between (a1, e1) with cocor. In this case, because a1 is common to the first correlation (a1, c1) and the second correlation (e1, o1) we refer to this as an overlapping correlation comparison. We obtain the (a1, e1) correlation, below, and find: r = .11, 95% [.07, .14], p &lt; .001. cor.test(bfi$a1, bfi$e1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi$a1 and bfi$e1 ## t = 5.6, df = 2759, p-value = 2e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.06917 0.14294 ## sample estimates: ## cor ## 0.1062 We obtain the (a1, c1) correlation, below, and find: r = .03, 95% [-.00, .07], p = .14. cor.test(bfi$a1, bfi$c1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi$a1 and bfi$c1 ## t = 1.5, df = 2762, p-value = 0.1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.00949 0.06502 ## sample estimates: ## cor ## 0.0278 We use the cocor command to compare the two relations: library(cocor) cocor( ~ a1 + c1 | a1 + e1, data = as.data.frame(bfi)) ## ## Results of a comparison of two overlapping correlations based on dependent groups ## ## Comparison between r.jk (a1, c1) = 0.0263 and r.jh (a1, e1) = 0.1047 ## Difference: r.jk - r.jh = -0.0784 ## Related correlation: r.kh = -0.0248 ## Data: as.data.frame(bfi): j = a1, k = c1, h = e1 ## Group size: n = 2741 ## Null hypothesis: r.jk is equal to r.jh ## Alternative hypothesis: r.jk is not equal to r.jh (two-sided) ## Alpha: 0.05 ## ## pearson1898: Pearson and Filon&#39;s z (1898) ## z = -2.8820, p-value = 0.0040 ## Null hypothesis rejected ## ## hotelling1940: Hotelling&#39;s t (1940) ## t = -2.8827, df = 2738, p-value = 0.0040 ## Null hypothesis rejected ## ## williams1959: Williams&#39; t (1959) ## t = -2.8793, df = 2738, p-value = 0.0040 ## Null hypothesis rejected ## ## olkin1967: Olkin&#39;s z (1967) ## z = -2.8820, p-value = 0.0040 ## Null hypothesis rejected ## ## dunn1969: Dunn and Clark&#39;s z (1969) ## z = -2.8775, p-value = 0.0040 ## Null hypothesis rejected ## ## hendrickson1970: Hendrickson, Stanley, and Hills&#39; (1970) modification of Williams&#39; t (1959) ## t = -2.8827, df = 2738, p-value = 0.0040 ## Null hypothesis rejected ## ## steiger1980: Steiger&#39;s (1980) modification of Dunn and Clark&#39;s z (1969) using average correlations ## z = -2.8765, p-value = 0.0040 ## Null hypothesis rejected ## ## meng1992: Meng, Rosenthal, and Rubin&#39;s z (1992) ## z = -2.8754, p-value = 0.0040 ## Null hypothesis rejected ## 95% confidence interval for r.jk - r.jh: -0.1325 -0.0251 ## Null hypothesis rejected (Interval does not include 0) ## ## hittner2003: Hittner, May, and Silver&#39;s (2003) modification of Dunn and Clark&#39;s z (1969) using a backtransformed average Fisher&#39;s (1921) Z procedure ## z = -2.8765, p-value = 0.0040 ## Null hypothesis rejected ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r.jk - r.jh: -0.1317 -0.0250 ## Null hypothesis rejected (Interval does not include 0) The cocor command provides a lot of output. We are most interested in the last part of the output corresponding to Zou (2007) which provides the confidence interval for the difference: -.13 to -.03. But we also want to examine the first part of the output which will show us the two original correlations \\(r_{(a1, c1)}\\) = 0.0276, and \\(r_{(a1, e1)}\\) = 0.10472 As well, the output also shows us the difference between thesee two correlations, Difference: r.jk - r.jh = -0.0784 (with rounding, -.08). We can write this up as: Agreeableness and extraversion were weakly related, r = .11, 95% [.07, .14], p &lt; .001, such that as agreeableness increased so did extraversion. The relation between agreeableness and conscientiousness was non-significant, r = .03, 95% [-.00, .07], p = .14. The agreeableness/extraversion relation was significantly stronger than the agreeableness/conscientiousness relation, \\(\\Delta\\)r = .08, 95% CI [.03, .13], p = .004. 14.3.3 Between data sets In this section we look at comparing correlations from two data sets. 14.3.3.1 Create seperate data files for men and women (if needed) We begin by creating two separate data sets - one for men and one for women: bfi_men &lt;- bfi %&gt;% filter(gender == 1) %&gt;% select(-gender) bfi_women &lt;- bfi %&gt;% filter(gender == 2) %&gt;% select(-gender) Use glimpse() to check out the subgroups. Note that it also tells you the number of participants in each subgroup. glimpse(bfi_men) ## Rows: 919 ## Columns: 4 ## $ a1 &lt;int&gt; 2, 2, 2, 4, 4, 4, 2, 5, 4, 4, 5, 5, 1, 4, 1, 4,… ## $ c1 &lt;int&gt; 2, 4, 5, 3, 6, 4, 5, 4, 5, 5, 5, 4, 4, 5, 1, 4,… ## $ e1 &lt;int&gt; 3, 2, 4, 3, 5, 1, 3, 2, 3, 1, 2, 3, 2, 3, 6, 2,… ## $ o1 &lt;int&gt; 3, 3, 5, 3, 6, 5, 4, 5, 5, 6, 5, 6, 6, 6, 6, 4,… glimpse(bfi_women) ## Rows: 1,881 ## Columns: 4 ## $ a1 &lt;int&gt; 2, 5, 4, 6, 2, 5, 4, 4, 4, 1, 2, 1, 2, 2, 2, 4,… ## $ c1 &lt;int&gt; 5, 4, 4, 6, 6, 5, 4, 5, 1, 5, 3, 5, 6, 4, 5, 5,… ## $ e1 &lt;int&gt; 1, 2, 5, 2, 2, 3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1,… ## $ o1 &lt;int&gt; 4, 4, 3, 4, 5, 4, 5, 4, 4, 5, 6, 6, 5, 6, 2, 4,… 14.3.3.2 Check out the subgroup correlations For men, we can obtain the correlation between a1/e1 with the code below. From this we determine, r = .14, 95% [.07, .20], p &lt; .001. cor.test(bfi_men$a1, bfi_men$e1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi_men$a1 and bfi_men$e1 ## t = 4.2, df = 910, p-value = 3e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.07448 0.20182 ## sample estimates: ## cor ## 0.1387 For women, we can obtain the correlation between a1/e1 with the code below. From this we determine, r = .06, 95% [.02, .11], p = .007. cor.test(bfi_women$a1, bfi_women$e1) ## ## Pearson&#39;s product-moment correlation ## ## data: bfi_women$a1 and bfi_women$e1 ## t = 2.7, df = 1847, p-value = 0.007 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.01672 0.10754 ## sample estimates: ## cor ## 0.06226 14.3.3.3 Comparison r(a1, e1) We run the R commands below to compare the correlations for men and women. library(cocor) bfi_men_dataframe &lt;- as.data.frame(bfi_men) bfi_women_dataframe &lt;- as.data.frame(bfi_women) cocor( ~ a1 + e1 | a1 + e1, data = list(bfi_men_dataframe, bfi_women_dataframe)) ## ## Results of a comparison of two correlations based on independent groups ## ## Comparison between r1.jk (a1, e1) = 0.1387 and r2.hm (a1, e1) = 0.0623 ## Difference: r1.jk - r2.hm = 0.0765 ## Data: list(bfi_men_dataframe, bfi_women_dataframe): j = a1, k = e1, h = a1, m = e1 ## Group sizes: n1 = 912, n2 = 1849 ## Null hypothesis: r1.jk is equal to r2.hm ## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided) ## Alpha: 0.05 ## ## fisher1925: Fisher&#39;s z (1925) ## z = 1.9074, p-value = 0.0565 ## Null hypothesis retained ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r1.jk - r2.hm: -0.0021 0.1543 ## Null hypothesis retained (Interval includes 0) The output reveals the correlation for men, \\(r_{(a1, e1)}\\) = 0.1387, .14 rounded, and women \\(r_{(a1, e1)}\\)= 0.0623, .06 rounded, in the output. We also see the comparison: Difference: r1.jk - r2.hm = 0.0765. Finally, we see the zou2007 95% confidence interval: -.00 to .15. We write this up as: For men, there was a positive relation between agreeableness (a1) and extraversion (e1), r = .14, 95% [.07, .20], p &lt; .001, such that as agreeableness increased so did extraversion. Likewise, for women, there was a similar positive relation between agreeableness (a1) and extraversion (e1), r = .06, 95% [.02, .11], p = .007. We did not find a significant difference in the strength of these relations, \\(\\Delta\\)r = .08, 95% CI [-.00, .15], p = .057. "],["multiple-regression.html", "Chapter 15 Multiple regression 15.1 Overview 15.2 Example 15.3 Load the data 15.4 Bivariate relations 15.5 Single best predictor 15.6 Multiple regression 15.7 b-weights 15.8 \\(R^2\\) 15.9 Semi-partial (\\(sr\\)) 15.10 Beta-weights 15.11 Graphing", " Chapter 15 Multiple regression The following CRAN packages must be installed: Required CRAN Packages tidyverse apaTables janitor remotes skimr The following GitHub packages must be installed: Required GitHub Packages dstanley4/fastInteraction After the remotes package is installed, it can be used to install a package from GitHub: remotes::install_github(&quot;dstanley4/fastInteraction&quot;) 15.1 Overview Multiple regression is a means relating multiple predictor variables to a single criterion variable. We can determine the amount of variance in the criterion accounted for by the set of predictors (\\(R^2\\)), a single predictor on it’s own (\\(r^2\\)), or the extent to which a single predictor accounts for unique variance account for in the criterion that is not accounted for by any of the other predictors (\\(sr^2\\)). 15.2 Example In this chapter, we examine the extent to video game scores are predicted by age and IQ for people who live in the City of Guelph. As before, we treat citizens of Guelph as our population – though in this chapter we move straight to the sample data (without showing the population-level data). We are interested in using both IQ and age to predict video game scores so we refer to these variables as predictor variables. The value being predicted is video game score and we refer to that variable as the criterion (i.e., the dependent variable). You can think of the multiple regression problem using a venn diagram: {r. echo = FALSE} knitr::include_graphics(\"ch_multiple_regression/images/mr_venn.png\") 15.3 Load the data We use a data set called “data_mr_ex.csv” in this example. The data can be loaded with the command: library(tidyverse) library(janitor) my_data &lt;- read_csv(&quot;data_mr_ex.csv&quot;) # ensure column names match desired naming convention my_data &lt;- my_data %&gt;% clean_names() We see the structure of the data with the glimpse() command: glimpse(my_data) ## Rows: 200 ## Columns: 3 ## $ video_game &lt;dbl&gt; 122.0, 108.7, 130.4, 123.4, 121.5, 125.… ## $ iq &lt;dbl&gt; 107.6, 100.9, 89.0, 90.3, 97.1, 108.1, … ## $ age &lt;dbl&gt; 41.1, 55.1, 43.9, 46.7, 42.1, 41.2, 41.… 15.4 Bivariate relations When you conduct multiple regression analyses you should always report a correlation matrix with your predictors and criterion. library(apaTables) apa.cor.table(my_data) ## ## ## Table 0 ## ## Descriptive Statistics and Correlations ## ## ## Variable N M SD 1 2 ## 1. video_game 200 119.03 10.75 ## ## ## ## 2. iq 200 102.00 15.00 .50** ## [.39, .60] ## p &lt; .001 ## ## 3. age 200 45.00 6.00 -.30** -.20** ## [-.42, -.17] [-.33, -.07] ## p &lt; .001 p = .004 ## ## ## Note. N = number of cases. M = mean. SD = standard deviation. ## Values in square brackets indicate the 95% confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## You should always check for curvilinear relations when reporting correlations via a graph. We quickly create a graph for doing so with the code below. In this case we don’t see any curvilinear relations. library(GGally) ggpairs(my_data) You can think of each bivariate correlation as the shaded areas on the figure below. In this type of figure the degree of overlap between two circles is determined by the correlation squared (i.r., \\(r^2\\)). The value you obtain for \\(r^2\\) indicates the proportion of the criterion that is covered by the predictor. For example, if \\(r\\) = .50 then \\(r^2\\) = .25 which indicates 25% of the criterion should be covered by the circle representing that predictor. 15.5 Single best predictor What is the best predictor of video_game? Many researchers incorrectly believe you need multiple regression to answer this question - you do not. To determine the single best predictor in a set of predictors just look at the correlation matrix above - no need for regression (or beta-weights). The strongest correlation is the best predictor. In our current example, video_game_score is predicted by iq (r = .50) and age (r = -.30). The best predictor of video_game_score is iq because it has the strongest correlation (using absolute values .50 is larger than .30). 15.6 Multiple regression Multiple regression is frequently used to ask two questions: How well we can predict the criterion using a set of predictors (see \\(R^2\\))? What is the unique contribution of a single variable in a set or predictors? In other words, how much does one variable predict the criterion above and beyond another variable? For example, does study time predict exam grades above and beyond iq? Or phrased differently: Does study time predict unique variance in exam grades that is not accounted for by iq? (see \\(sr^2\\)). We want to use age and IQ to predict video game score (\\(Y\\)). More specifically, we want to combine age and IQ to create a new variable (\\(\\widehat{Y}\\)) that correlates as highly as possible with video game score. We do with the code below: lm_object &lt;- lm(video_game ~ age + iq, data = my_data) Now look at the brief output: print(lm_object) ## ## Call: ## lm(formula = video_game ~ age + iq, data = my_data) ## ## Coefficients: ## (Intercept) age iq ## 102.233 -0.371 0.328 This output shows you how we can combine age, IQ, and a constant to create (\\(\\widehat{Y}\\)). More specifically: \\(\\widehat{Y} = 102.233 - 0.0371(age) + 0.328(iq)\\) The slopes for age and iq are -0.0371 and 0.328, respectively. These are also referred to as the b-weights or unstandardized regression coefficients. The computer picks these weight so that predicted video game scores (\\(\\widehat{Y}\\)) will corresponds as closely as possible to actual video game scores. A quick way to get more comprehensive regression output is to use the apa.reg.table function in the apaTables package. library(apaTables) apa.reg.table(lm_object, filename = &quot;table_regression.doc&quot;) 15.7 b-weights An inspection of the b column in the above table reveals the b-weights we previously discussed. The b-weights are also known as the slopes or the unstandardized regression weights. The b-weights are used to create predicted/estimated video game score via the regression equation: \\[ \\begin{aligned} \\widehat{Y} &amp;= 102.233 + 0.3285(Z) - 0.3712(X) \\\\ \\widehat{\\text{video game}} &amp;= 102.233 + 0.3285(iq) - 0.3712(age) \\\\ \\end{aligned} \\] You can think of the regression equation as a receipe for making \\(\\hat{Y}\\). The variables in the regression (e.g., age and iq) are the ingredients. The b-weights (e.g., 0.3285 and 00.3712) are the amount of each ingredient you need to make \\(\\hat{Y}\\). As previously noted, the computer picks the b-weights using a process that ensures the predicted video game scores (\\(\\hat{Y}\\)) corresponds as closely as possible to actual video game scores (\\(Y\\)). Consider the regression calculation for Person 1, below, who is 41.7 years old and has an IQ of 107.6. \\[ \\begin{aligned} \\widehat{Y} &amp;= 102.233 + 0.3285(Z) - 0.3712(X) \\\\ \\widehat{\\text{video game}} &amp;= 102.233 + 0.3285(iq) - 0.3712(age) \\\\ &amp;= 102.233 + 0.3285(107.6) - 0.3712(41.1) \\\\ &amp;= 122.3 \\end{aligned} \\] The above calculation reveals an estimated video game score for Person 1 of 122.3 (i.e., \\(\\hat{Y} = 122.3\\)) – which differs only slightly from their actual video game score of 122 (i.e.. \\(Y = 122\\)). You can see the similarity between actual video game scores (\\(Y\\)) and predicted video game scores (\\(\\hat{Y}\\)) for the first several participants in the table below. You can interpret the b-weights as indicating how much the criterion changes when a predictor changes – holding the other predictors constant. In this context, a 0.3285 b-weight for IQ indicates that for each one-unit increase in IQ video game score will increase 0.3285 points – holding the effect of age constant. Don’t forget the b-weight can only be interpreting in context of that specific regression equation. If you replaced the age predictor with, say, height as a predictor, then the b-weight for IQ would change. Context matters when interpreting b-weights. 15.8 \\(R^2\\) How effective is the set of predictors? We can calculate \\(R^2\\) to determine the proportion of variance the criterion that is accounted for by the set of predictors. This value is illustrated graphically below: You will simply obtain \\(R^2\\) from computer output. But how is \\(R^2\\) calculated? Understanding how \\(R^2\\) is calculated can help you to understand how to interpret it. There are two methods for doing so - that produce the same number: 15.8.1 Method 1: Ratio Approach What does the \\(R^2\\) mean? It is the proportion variability in criterion scores (\\(Y\\)) accounted for by (\\(\\widehat{Y}\\)). In other words, it is the proportion of the variability in criterion scores that can be accounted for by (a linear combination of) iq and age. We begin by obtaining predicted video game scores (\\(\\widehat{Y}\\)) using the regression: \\(\\widehat{Y} = 102.233 + 0.3285(iq) - 0.3712(age)\\) predicted_video_game_scores \\(= 102.233 + 0.3285(iq) - 0.3712(age)\\) The code below uses the above equation to calculate predicted video game score for each person: predicted_video_game_scores &lt;- predict(lm_object) actual_video_game_scores &lt;- my_data$video_game Recall the formula for \\(R^2\\): \\[ \\begin{aligned} R^2 = \\frac{\\text{Variance of predicted scores}}{\\text{Variance of actual scores}} \\end{aligned} \\] We implement this formula using the code below: var_predicted_video_game_scores &lt;- var(predicted_video_game_scores ) var_actual_video_game_scores &lt;- var(actual_video_game_scores) R2 &lt;- var_predicted_video_game_scores / var_actual_video_game_scores print(R2) ## [1] 0.2911 Thus, 29% of the variability in video game scores is predicted by the combination of age and IQ. 15.8.2 Method 2: Correlation Approach An alternative way of thinking about \\(R^2\\) is as the squared correlation between predicted criterion scores and actual criterion scores: \\[ \\begin{aligned} R^2 &amp;= r^2_{\\hat{Y}, Y}\\\\ &amp;= r^2_{(predicted_video_game_scores, actual_video_game_scores)}\\\\ \\end{aligned} \\] We implement this formula with the code below and obtain the same value: R &lt;- cor(predicted_video_game_scores, actual_video_game_scores) R2 &lt;- R * R print(R2) ## [1] 0.2911 15.8.3 \\(R^2\\) in practice In practice we simply look at the apa.reg.table() output and obtain the \\(R^2\\) value and 95% CI from this output: From this table we determine: \\(R^2\\) = .29, 95% CI [.19, .38]. to obtain the required p-value we use the summary() command on the previously calculated lm_object: summary(lm_object) ## ## Call: ## lm(formula = video_game ~ age + iq, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.658 -5.480 0.602 6.328 20.539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 102.2333 7.3386 13.93 &lt; 2e-16 *** ## age -0.3712 0.1098 -3.38 0.00088 *** ## iq 0.3285 0.0439 7.48 2.4e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.1 on 197 degrees of freedom ## Multiple R-squared: 0.291, Adjusted R-squared: 0.284 ## F-statistic: 40.5 on 2 and 197 DF, p-value: 1.91e-15 At the bottom of this output we see that the p-value is 1.912e-15 or 0.000000000000001912. Consequently, we report the \\(R^2\\) value as: \\(R^2\\) = .29, 95% CI [.19, .38], \\(p\\) &lt; .001. Thus, 29% of the variability in video game scores is predicted by the combination of age and IQ. 15.9 Semi-partial (\\(sr\\)) A semi-partial correlation is represented by the symbol \\(sr\\) and correspondingly a squared semi-partial correlation is represented by the symbol \\(sr^2\\). What is a squared semi-partial correlation and why is it useful? Semi-partial correlations are a way of determining the unique contribution of a variable to predicting the criterion (in the context of the other predictors). The semi-partial correlation is the correlation of one predictor (with all the other predictors removed) with the criterion. The semi-partial correlation squared is the amount \\(R^2\\) would drop by if that variable was removed from the regression. It is the percentage of variability in criterion scores that is uniquely accounted for by a predictor. This is illustrated in the venn diagram below: 15.9.1 \\(sr^2\\) in theory In the text below we go “inside the black box” to show you how semi-partial correlations are computed. In practice, they are just displayed in R output - but understanding the text below where we calculate them “old school” will help with you interpret \\(sr^2\\). Overall, squared semi-partial correlations provide an index of how much that predictor contributes to the overall \\(R^2\\) (with the effect of the other predictors removed). We calculate \\(sr^2\\) for IQ (removing the effect of age) to demonstrate this fact. We do this with a regression equation in which we make IQ the criterion (\\(Y\\)). Then we predict IQ with age. This produces my_iq_regression which has inside of it a predictor version of IQ, \\(\\widehat{Y_{iq}}\\), which in this case represents a best guess of IQ based on age. my_iq_regression &lt;- lm(iq ~ age, data = my_data) print(my_iq_regression) ## ## Call: ## lm(formula = iq ~ age, data = my_data) ## ## Coefficients: ## (Intercept) age ## 124.763 -0.506 . Thus, we find: \\(\\widehat{Y_{iq}} = 124.763 - 0.506(age)\\) Consequently, when you see \\(\\widehat{Y_{iq}}\\) recognize that it is really just an estimate of IQ created entirely from age. In contrast, \\(Y_{iq}\\) is the actual IQ score we obtained from participants. We want IQ with the effect of age removed. Therefore, we want IQ (i.e., \\(Y_{iq}\\)) with the effect of age (i.e.,\\(\\widehat{Y_{iq}}\\) ) removed. Thus we want: residual = \\(Y_{iq}\\) - \\(\\widehat{Y_{iq}}\\) or another way of thinking of it is: iq_without_age = \\(Y_{iq}\\) - \\(\\widehat{Y_{iq}}\\) iq_without_age = iq - (124.763 - 0.506(age) ) We do this below: iq_without_age &lt;- resid(my_iq_regression) Then we correlate IQ without age (i.e., iq_without_age) with video game scores (i.e., video_game). This tells us how IQ correlates with video game scores when the effects of age have been removed from IQ; that is, the semi-partial correlation (i.e., \\(sr\\)). Once again refer to the venn diagram above illustrating \\(sr^2\\). # apa.reg.table does this for you - this is for learning/illustration only. sr &lt;- cor(iq_without_age, my_data$video_game) sr2 &lt;- sr * sr 15.9.2 \\(sr^2\\) in practice The \\(sr^2\\) values with confidence intervals are reported in apa.reg.table() output: From this table we determine: age: \\(sr^2\\) = .04 , 95% CI [-.01, .09] iq: \\(sr^2\\) = .20, 95% CI [.11, .30] However, to obtain the p-value for each \\(sr^2\\) value we need to use the summary() command: summary(lm_object) ## ## Call: ## lm(formula = video_game ~ age + iq, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.658 -5.480 0.602 6.328 20.539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 102.2333 7.3386 13.93 &lt; 2e-16 *** ## age -0.3712 0.1098 -3.38 0.00088 *** ## iq 0.3285 0.0439 7.48 2.4e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.1 on 197 degrees of freedom ## Multiple R-squared: 0.291, Adjusted R-squared: 0.284 ## F-statistic: 40.5 on 2 and 197 DF, p-value: 1.91e-15 The p-value for the b-weight (i.e. Estimate) is the p-value for \\(sr^2\\), Therefore, we simply look in the Pr(&gt;|t|) column to obtain the required p-value. Adding this value to our reporting, we find: age: \\(sr^2\\) = .04 , 95% CI [-.01, .09], p &lt; .001 iq: \\(sr^2\\) = .20, 95% CI [.11, .30], p &lt; .001 If a predictor is significant, this indicates that the predictor contributes unique variance to \\(\\widehat{Y_{video game}}\\) that can not be contributed by any of the other predictors. The amount of unique variance contributed by a predictor is indicated by \\(sr^2\\) (semi-partial correlation squared). 15.9.3 Blocks regression Some researchers are unfamiliar with semi-partial correlations and prefer to think in term of how the \\(R^2\\) value changes over two different regression. This approach is just an indirect way of calculating \\(sr^2\\). Consider the example below where the researcher conducts the first regression, block 1, in which age is the predictor. Then he conducts a second regression, block 2, in which both age and iq are the predictors. # apa.reg.table does this for you - this is for learning/illustration only. block1 &lt;- lm(video_game ~ age, data = my_data) block2 &lt;- lm(video_game ~ age + iq, data = my_data) The goal is to examine the \\(R^2\\) when only age is the predictor and see how much it increases when you have both age and iq as predictors. The resulting difference, \\(\\Delta R^2\\) tells you how much iq predicted video game score beyond age alone. Examine the output below. apa.reg.table(block1, block2, filename = &quot;table_mr_blocks.doc&quot;) This table illustrates that for the first regression when just age was a predictor that \\(R^2\\) = .09. When both age and iq were predictors \\(R^2\\) = .29. That indicates that \\(R^2\\) increased by .20 when we added iq as a predictor. Thus, \\(\\Delta R^2\\) = .20. This is the amount \\(R^2\\) increased by due to adding iq as predictor. Conceptually, and mathematically, this is identical to the \\(sr^2\\) value for iq. Indeed, if you look at this output in detail you see that for iq \\(sr^2\\) = .20. 15.10 Beta-weights Beta-weights are often referred to as standardized regression weights. This is a poor description that makes beta weights hard to understand. A better description of beta-weights is the regression weights for standardized variables; that is variables with a mean of 0 and a standard deviation of 1.0. 15.10.1 In practice Recall we ran a regression with the command below. This command used the original/raw form of the variables. lm_object &lt;- lm(video_game ~ age + iq, data = my_data) From lm_object created we used apa.reg.table() to obtain this output: Notice the beta column in this output. It reports beta weights of -.21 and .46 for age and iq, respectively. Where did these values come from? To answer this question, we need to start with the lm_object. The apa.reg.table() command just formats the information contained in the lm_object. We can see an unformatted version of this information with the summary() command: summary(lm_object) ## ## Call: ## lm(formula = video_game ~ age + iq, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.658 -5.480 0.602 6.328 20.539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 102.2333 7.3386 13.93 &lt; 2e-16 *** ## age -0.3712 0.1098 -3.38 0.00088 *** ## iq 0.3285 0.0439 7.48 2.4e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.1 on 197 degrees of freedom ## Multiple R-squared: 0.291, Adjusted R-squared: 0.284 ## F-statistic: 40.5 on 2 and 197 DF, p-value: 1.91e-15 Notice that the values in the Estimates column correspond to the b-weights column in the apa.reg.table() output. We will use this fact to create beta-weights. 15.10.2 Old school To obtain beta-weights there are two steps. First, we create standardized score versions of each column. Second, we run a normal regression using those columns. A set of standardized scores have a mean of 0 and a standard deviation of 1.0. To create the standardized score versions of each column in the regression we use the z-score formula: \\[ \\begin{aligned} \\text{standardized scores}=z_{X} = \\frac{X-\\bar{X}}{\\sigma_X} \\end{aligned} \\] Consider the age column. We can calculate the mean for this column, mean(age), and the standard deviation for this column, sd(age). Then for every value in the age column we subtract the column mean and then divide by the column standard deviation. We do so with the calculation: (age-mean(age))/sd(age). The code below creates standardized score versions of the iq, age, and video_game columns called z_iq, z_age, and z_video_game, respectively. my_data &lt;- my_data %&gt;% mutate(z_iq = (iq-mean(iq))/sd(iq), z_age = (age-mean(age))/sd(age), z_video_game = (video_game-mean(video_game))/sd(video_game)) We can confirm a mean of 0 and a standard deviation of 1.0 for these new columns with the skim command: library(skimr) skim(my_data) Now we conduct the regression again with standardized variables (i.e., z-score versions). lm_object_zscores &lt;- lm(z_video_game ~ z_iq + z_age, data = my_data) We can obtain the regression weights for analysis using these standardized scores with the summary() command: summary(lm_object_zscores) ## ## Call: ## lm(formula = z_video_game ~ z_iq + z_age, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.479 -0.509 0.056 0.588 1.910 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.73e-16 5.98e-02 0.00 1.00000 ## z_iq 4.58e-01 6.13e-02 7.48 2.4e-12 *** ## z_age -2.07e-01 6.13e-02 -3.38 0.00088 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.846 on 197 degrees of freedom ## Multiple R-squared: 0.291, Adjusted R-squared: 0.284 ## F-statistic: 40.5 on 2 and 197 DF, p-value: 1.91e-15 Notice that the estimates above are 4.582e-01 -2.070e-01 which are, in decimal form, .46 and -.21, respectively. These are the beta-weights from the apa.reg.table on the previous page. Thus, when you conduct a normal regression but used standardized variables in it you obtain beta-weights. Interpretation. The typically reported b-weights describe how a 1 unit change in IQ influences video_game points. Similarly, beta-weights, describe how a 1 unit change in z_iq influences z_video_game. Keep in mind, however, that 1 unit of z_iq (and z_video_game) is 1 standard deviation. As a result, a beta-weight indicates how much the criterion scores will change in SD units when a predictor increases by 1 SD – holding the effect of the other predictors constant. Similar to b-weights, beta-weights can only be interpreted in the context of the other variables in the equation. Hopefully, this description has made it clear that although beta-weights are often referred to as standardized regression weights; it would be more accurate to describe them as the weights for standardized variables. 15.11 Graphing Let’s take a minute to consider the nature of the data we have so far - examine the first few rows of the data below (that includes the predicted value for each person). You can see that each person has three measured variables associated with them: video_game_score, iq, and age. These columns are in blue to indicate the fact they are measured variables. Because we have three measured variables we can’t create a typical 2D scatter plot. That type of plot only work when there is one predictor and one criterion. Now we have two predictors and one criterion. Consequently, we need make a 3D scatter plot. Correspondingly, because we have two predictors, we can’t obtain a regression line (i.e., best-fit line). A regression line is only possible when there is one predictor. Now we have two predictors. Consequently, a regression surface is required to show the predicted values for combinations of age and iq. In this case, when there are two predictor variables, the regression surface is a plane. Let’s create the 3D scatter plot with a regression surface. You recall we previously created the lm_object when we ran our regression: lm_object &lt;- lm(video_game ~ age + iq, data = my_data) The lm_object has the three data point for each person (age, iq, video_game) embedded inside it. So we can use the lm_object to create the scatter plot with regression plane. We do so using the fastInteraction package. More specifically, we use the fast.plot command as illustrated below.Note: The fast.plot command uses the argument “moderator” which is not appropriate in our context. When you see moderator in the command below just think of it as another predictor. library(fastInteraction) surface_plot &lt;- fast.plot(lm_object, criterion = video_game, predictor = iq, moderator = age) Then just type: surface_plot You can see the graph below: All of the predicted scores fall on the regression surface. You can think of the regression surface as a best-fit plane. In some sense you can think of the plane as a collection of best-fit lines. Indeed, two illustrative best-fit lines are place on this surface. For all the best-fit lines on the plane the slopes are the same - just the intercepts differ. You can see the people relative to the predicted surface/plane by looking at the dots. Each dot represents a person. Try the interactive version of the graph below. Caveat, sometimes it doesn’t appear correctly on the web. If the graph appears incorrectly (e.g., no data points or no surface) try using the Chrome browser. Safari sometimes has problems with this web object. You can rotate the graph to see a better view of the surface. The graph illustrates that predicted video game scores (i.e., the surface) change as a function of both age and iq. surface_plot "],["sample-size-for-multiple-regression.html", "Chapter 16 Sample size for multiple regression 16.1 Sample size using \\(R^2\\) 16.2 Sample size using \\(sr^2\\) 16.3 Power obtained \\(sr^2\\)", " Chapter 16 Sample size for multiple regression When you conduct a multiple regression there are two ways to think about conducting a sample size analysis. Focus on having a sufficient power to the variance accounted for by the set of predictors (i.e. \\(R^2\\)) Focus on having a sufficient power to the unique variance account for by a single predictor (i.e. \\(sr^2\\)) 16.1 Sample size using \\(R^2\\) You want to run a multiple regression study in an existing research area using 3 predictors. A past study, \\(N\\) = 100, found an \\(R^2=.20\\). How many people do you need in your study to have 80% power for the overall regression equation (i.e., \\(R^2\\)). We need to determine two pieces of information to proceed: 1) the effect size as \\(f^2\\) instead of \\(R^2\\) and 2) the degrees of freedom for the predictors. We use these two pieces of information in the pwr command. 16.1.1 Determining \\(f^2\\) We need to represent \\(R^2\\) as \\(f^2\\). We do this with the formula: \\[ f^2=\\frac{R^2}{1-R^2} \\] So in R we type: my_f2 &lt;- .20 / (1 - .20) print(my_f2) ## [1] 0.25 Thus, \\(f^2=.25\\). 16.1.2 Determine degrees of freedom There are degrees of freedom for the predictors (\\(u\\)) and error (\\(v\\)) in the original study. The degrees of freedom for the predictors is easy to compute: Degrees of Freedom Predictors: \\(u =\\) number of predictors = 3. 16.1.3 Calculate sample size With multiple regression we send R the effect size (.25) the degrees of freedom for the predictor (3). The output provide returns the degrees of freedom for the denominator - which requires a bit of work to convert to sample size. We begin with the R code below: library(pwr) pwr.f2.test(u = 3, f2 = .25, power = .90) ## ## Multiple regression power calculation ## ## u = 3 ## v = 56.75 ## f2 = 0.25 ## sig.level = 0.05 ## power = 0.9 What does this output mean? It provides us with the degrees of freedom error (i.e., \\(v\\)) for your study (not the original study). We need to use the degrees of freedom error to calculate the \\(N\\) needed. Degrees of Freedom Error: \\(v = N - u - 1\\) therefore \\(N = u + v + 1\\). We know \\(u = 3\\) because there were three predictors. Our power analysis revealed we need \\(v = 57\\) in our study. Therefore \\(N = u + v + 1 = 3 + 57 + 1\\) N = 3 + 57 + 1 print(N) ## [1] 61 Thus we want an N of 61 in our study. 16.2 Sample size using \\(sr^2\\) You want to run a multiple regression study in an existing research area using 3 predictors (A, B, and C). A past study, \\(N\\) = 100, found an \\(R^2=.20\\). You are particularly interested in one predictor (predictor C) that you think will account for 2% of the variance in the criterion above and beyond the other two predictors. Said another way you believe .02 of the .20 will be a result of the unique contribution of one predictor. What sample size do you need to ensure you have power of .90 for detecting this increment in variance. Let’s think about this scenario in terms of Blocks to make it a bit clear. You are describing a scenario where if you put predictors A and B are in Block 1 you would obtain an overall \\(R^2 = .18\\). Then when you put Predictor A, B, and C into Block 2 you would obtain an overall \\(R^2 = .20\\), an increment of .02 so \\(sr^2\\) for predictor C is .02. We want to conduct a power analysis to determine how many people you need to ensure power of .90 for this incremental prediction effect. 16.2.1 Determine degrees of freedom We are interested in the incremental prediction of one variable so \\(u = 1\\). 16.2.2 Determine \\(f^2\\) We know \\(sr^2=.02\\) and \\(R^2 = .20\\). We can use these to compute the needed \\(f^2\\). \\[ f^2=\\frac{sr^2}{1-R^2} \\] So we type: my_f2 &lt;- .02 / (1 - .20) print(my_f2) ## [1] 0.025 16.2.3 Calculate power We can calculate the required degrees of freedom with the command below: library(pwr) pwr.f2.test(u = 1, f2 = 0.025, power = .90) ## ## Multiple regression power calculation ## ## u = 1 ## v = 420.2 ## f2 = 0.025 ## sig.level = 0.05 ## power = 0.9 Thus, degrees of freedom error (i.e., \\(v\\)) is 421 We then calculate \\(N = u + v + 1\\) In R we type: N = 1 + 421 + 1 print(N) ## [1] 423 Thus, you need an \\(N\\) of 423. 16.3 Power obtained \\(sr^2\\) Imagine your power analysis tells you that you need an \\(N\\) of 423 but you only get an \\(N\\) of 75. What is your power? Simply calculate the degrees of freedom for the predictors and error and run the command again. This time you leave out power. It will be calculated for you. You can do this before you collect your data if you know in advance you will have a low/restricted \\(N\\) - as is the case for many theses. In this example, \\(R^2=.20\\) and \\(sr^2=.02\\) are the expected effect sizes. Degrees of Freedom Predictors: \\(u = 1\\) as above (incremental prediction of one variable) In terms of error: Degrees of Freedom Error: \\(v = N - u - 1 = 75 - 1 - 1 = 73\\). In terms of effect size: \\(f^2=\\frac{sr^2}{1-R^2} = \\frac{.02}{1-.20}=.025\\). pwr.f2.test(u = 1, v = 73, f2 = 0.025) ## ## Multiple regression power calculation ## ## u = 1 ## v = 73 ## f2 = 0.025 ## sig.level = 0.05 ## power = 0.2718 In this case, power is .27. Thus, there is only a 27% chance of finding the effect if it exists. Given this, would it make sense to spend the time and energy to run the study? "],["moderated-multiple-regression.html", "Chapter 17 Moderated multiple regression 17.1 Overview 17.2 Scenario 17.3 Overview 17.4 fastInteraction 17.5 Power/sample size analysis", " Chapter 17 Moderated multiple regression The following CRAN packages must be installed: Required CRAN Packages tidyverse remotes Data data_endurance.csv Required CRAN Packages tidyverse remotes The following GitHub packages must be installed: Required GitHub Packages dstanley4/fastInteraction After the remotes package is installed, it can be used to install a package from GitHub: remotes::install_github(&quot;dstanley4/fastInteraction&quot;) 17.1 Overview In this chapter we present a brief overview of moderated multiple regression. In an ANOVA you can have two variables interact to predict a dependent variable. In this ANOVA scenario, the predictors are the categorical ANOVA variables. When our predictors are continuous variables (e.g., height, weight, etc) they can still interact to predict the dependent variable (i.e., criterion). In this chapter we primarily focus on how to obtain the required information from R to write up a continous variable interaction (also known as a moderated multiple regression). For an understanding of the underlying theory I strongly encourage you to read Chapter 7: Interactions among continuous variables in Cohen, Cohen, West, and Aiken (2003): Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2013). Applied multiple regression/correlation analysis for the behavioral sciences. Routledge. 17.2 Scenario Imagine a scenario where we are interested in predicted endurance from participant age and exercise. We can load the data: library(tidyverse) data_endurance &lt;- read_csv(&quot;data_endurance.csv&quot;) We can see the structure of the data: glimpse(data_endurance) ## Rows: 245 ## Columns: 4 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1… ## $ age &lt;dbl&gt; 60, 40, 29, 47, 48, 42, 55, 43, 39, 51, … ## $ exercise &lt;dbl&gt; 10, 9, 2, 10, 9, 6, 8, 19, 9, 14, 15, 4,… ## $ endurance &lt;dbl&gt; 18, 36, 51, 18, 23, 30, 8, 40, 28, 15, 4… 17.3 Overview 17.3.1 No interaction In a typical two variable regression we would attempt to solve this equation: \\[ \\hat{Y} = b_0 + b_1age + b_2exercise \\] We would do so using this R code: lm_no_int &lt;- lm(endurance ~ age + exercise, data = data_endurance) The result would be the a regression surface that fits the data as illustrated below. In this graph we did not assess whether there was an interaction (i.e., a moderating effect). This is just a standard two predictor regression. We illustrate the extent to which exercise and age predict endurance. You can consider the regression surface a series of best-fit lines. All of the lines on this surface have the same slope. That means in this statistical model the relation between exercise and endurance is not influenced by age – because the slopes of the lines on the surface are the same and do not change with age of participants. 17.3.2 Interaction We might wonder if the relation between exercise (a predictor) and endurance (the criterion) depends on the age of participants (a predictor). In other words, we might wonder if the relation between exercise and endurance is moderated by age. This is conceptually identical to an interaction effect in an ANOVA. We determine if there is a moderated relation by adding a product term to the regression. This is simply a new data column created by multiplying the age and exercise columns. If the b-weight (i.e., \\(b_3\\)) for for this product column (i.e., age*exercise) is significant - we say there is an interaction or a moderated relation. The regression equation we need to solve is below: \\[ \\hat{Y} = b_0 + b_1age + b_2exercise + b_3(age)(exercise)\\] We solve this equation using the code: lm_int &lt;- lm(endurance ~ age + exercise + I(age*exercise), data = data_endurance) When we examined the output for this regression, we found that that \\(b_3\\) was significant. This indicates there is a moderated relation. The relation between exercise and endurance does depend on age. We can see this moderated relation in the graph below. As before imagine the surface is composed of a series of best-fit lines. Because there is a moderated relation the slopes of the lines change as we move across this surface. You can see this clearly in the graph below. This illustrate the slope for the exercise – endurance relation changes depending on the age of participants. That is, it illustrates the nature of the moderated relation. 17.3.3 Comparison We present both graphs below so you can more easily see the difference between them. When you look at the No Interaction graph you can see the best-fit surface is composed of a series of straight lines that are parallel (i.e., all have the same slope) but all orientated a particular angle. In contrast, when you look at the Interaction graph you can see that it is also composed of a series of straight lines but the overall surface looks curves because the lines composing the best-fit surface have different slopes. Interpretation note: Notice that the difference between the No Interaction surface and the Interaction surface is the largest near the edges of the surface. Consequently, the inclusion of product terms typically makes the largest improvement in fit for the people who score at the extremes of the surface. In contrast, the inclusion of product terms typically makes a minor improvement in fit for the people who are near the middle of the surface (i.e., the majority of people). Keep that in mind as you interpret the increase in fit that results from the inclusion of the product terms. 17.4 fastInteraction If you want to actually conduct a moderated regression and solve the equation below, there is an easy way to do it. \\[ \\hat{Y} = b_0 + b_1age + b_2exercise + b_3(age)(exercise)\\] You simply use the fast.int() command in the fastInteraction package does the following: Conducts the regression (i.e., the lm command); including mean centering of predictors if desired (see Cohen, Cohen, West, and Aiken, 2003). Creates the 2D graph Creates the 3D graph Creates two APA style tables to describe the analyses To run the analysis you use the code below: library(fastInteraction) new_axis_labels &lt;- list(criterion = &quot;Endurance&quot;, predictor = &quot;Age (centered)&quot;, moderator = &quot;Exercise (centered)&quot;) mmr_output &lt;- fast.int(data = cohen_exercise, criterion = endurance, predictor = age, moderator = exercise, center.predictors = TRUE, axis.labels = new_axis_labels, path = &quot;tables_mmr.doc&quot;) ## [1] &quot;Predictor&quot; ## [1] &quot;b&quot; ## [1] &quot;b_95_CI&quot; ## [1] &quot;Unique_R2&quot; ## [1] &quot;sr2_95_CI&quot; ## [1] &quot;p&quot; ## [1] &quot;Fit&quot; ## [1] &quot;Predictor&quot; ## [1] &quot;b&quot; ## [1] &quot;b_95_CI&quot; ## [1] &quot;Unique_R2&quot; ## [1] &quot;sr2_95_CI&quot; ## [1] &quot;p&quot; ## [1] &quot;Fit&quot; ## [1] &quot;moderator&quot; ## [1] &quot;moderator.value&quot; ## [1] &quot;b1.slope&quot; ## [1] &quot;b1.LL&quot; ## [1] &quot;b1.UL&quot; ## [1] &quot;b0.intercept&quot; ## [1] &quot;b1.SE&quot; ## [1] &quot;t&quot; ## [1] &quot;p&quot; ## [1] &quot;moderator&quot; ## [1] &quot;moderator.value&quot; ## [1] &quot;b1.slope&quot; ## [1] &quot;b1.LL&quot; ## [1] &quot;b1.UL&quot; ## [1] &quot;b0.intercept&quot; ## [1] &quot;b1.SE&quot; ## [1] &quot;t&quot; ## [1] &quot;p&quot; 17.4.1 Graphing in 3D You can obtain the 3D graph (which can be rotated) using the code: mmr_output$graph3D 17.4.2 Unformatted 2D graph You can obtain the 2D graph (cross-section of the 3D surface) with the code below. We need to adjust the graph a bit before it’s presentable. But to do so we first need to inspect tables created by the fast.int() command. unformatted_ggplot_graph &lt;- mmr_output$graph2D.unformatted print(unformatted_ggplot_graph) 17.4.3 Tables We inspect MS Word file created by the fast.int() command below. The commands save the table in the file “tables_mmr.doc” – as specified. It presents the results of the various regression in tables corresponding to APA style. 17.4.4 Formatted 2D graph We inspect the Table output and find the value for -1/+1 SD of the moderator (age) in Table 2. In this case, we find the value is 4.775. So we use this to set the x-axis ticks That way the ticks on the x-axis correspond to standard deviations. custom_formatted_ggplot_graph &lt;- unformatted_ggplot_graph + coord_cartesian(ylim = c(10, 40)) + scale_y_continuous(breaks = seq(10, 40, by = 5)) + scale_x_continuous(breaks = seq(-4.775*2, 4.775*2, by = 4.775)) + labs(x = &quot;Exercise (centered)&quot;, y = &quot;Endurance&quot;, linetype = &quot;Age (centered)&quot;) + theme_classic(18) print(custom_formatted_ggplot_graph) 17.4.4.1 Saving We can save the 2D plot with the following command: ggsave(&quot;graph_2D_interaction.png&quot;, custom_formatted_ggplot_graph) 17.5 Power/sample size analysis If you examine a large number of studies involving moderated multiple regression you will see that the \\(sr^2\\) (i.e., \\(\\Delta R^2\\)) for the product term (e.g., age*exercise) typically ranges from .01 to .04 and only rarely falls out of this range - regardless of the variables involved. Consequently, to conduct a sample size analysis for a moderated mulitple regression you need to do three things: Estimate the size of the \\(sr^2\\) value (i.e., in the .01 to .04 range) for your study. Estimate the overall \\(R^2\\) value for your study. Plug both of those values into the sample size calculation provided in the multiple regression lecture. "],["open-science-preregistration.html", "Chapter 18 Open Science Preregistration 18.1 Overview 18.2 Daniel Simmons Interview 18.3 Preregistration Impact 18.4 Preregistration Details 18.5 Learn more", " Chapter 18 Open Science Preregistration 18.1 Overview It’s helpful to distinguish between preregistration and registered reports. This week we see a series of video that describe each of these approaches One of the most population options for preregistering a study is to use the Open Science Framework (OSF) which was created by the Center for Open Science (COS). In this video you obtain an overview of preregistration and the OSF. 18.2 Daniel Simmons Interview Daniel Simmons provides information about registered reports. Pay particular attention to the problems associated with finding an unexpected effect in a data set and then trying to test a hypothesis related to that effect with the same data. You can watch the video here. 18.3 Preregistration Impact Learn about the statistical impact of preregistration in this video. As in the previous video, pay particular attention to the problems associated with finding an unexpected effect in a data set and then trying to test a hypothesis related to that effect with the same data. You can watch the video here. 18.4 Preregistration Details In this video you will learn the “nuts and bolts” of preregistration using the OSF. You can watch the video here. 18.5 Learn more Learn more about preregistration at this website. "],["next-step-resources.html", "Chapter 19 Next Step Resources 19.1 OSF 19.2 Transparency checklist 19.3 Helpful web apps 19.4 Minimizing mistakes 19.5 Avoiding p-hacking 19.6 Writing articles 19.7 Writing with R 19.8 Writing with statcheck 19.9 Journal rankings via the TOP Factor 19.10 Statistics books 19.11 General R books 19.12 Retracted articles 19.13 Big data", " Chapter 19 Next Step Resources 19.1 OSF You can pre-register your study and upload your study materials via a free account: OSF Don’t forget you should also pre-register your outlier strategy. See (Leys et al. 2019) for guidance. Leys, C., Delacre, M., Mora, Y. L., Lakens, D., &amp; Ley, C. (2019). How to classify, detect, and manage univariate and multivariate outliers, with emphasis on pre-registration. International Review of Social Psychology, 32(1). Also don’t forget to pre-register any analyses involving control variables. Control variables should be pre-registered with very specific predictions. See (Breaugh 2008) and (Spector and Brannick 2011) for guidance. Breaugh, J. A. (2008). Important considerations in using statistical procedures to control for nuisance variables in non-experimental studies. Human Resource Management Review, 18(4), 282-293. Spector, P. E., &amp; Brannick, M. T. (2011). Methodological urban legends: The misuse of statistical control variables. Organizational Research Methods, 14(2), 287-305. 19.2 Transparency checklist Be sure to check out the Transparency checklist. This is a great tool to help you ensure your process is transparent. 19.3 Helpful web apps Daniel Lakens has a number of very helpful web apps to help you with sample size planning and other issues. I encourage you to check them out here. As well, check out Designing Experiments for some other helpful tools. Sampling size planning, effect size calculations, and more! 19.4 Minimizing mistakes I strongly encourage you to check out (Rouder, Haaf, and Snyder 2019) and (Strand 2021) to learn how to minimize mistakes in your research. Rouder, J. N., Haaf, J. M., &amp; Snyder, H. K. (2019). Minimizing mistakes in psychological science. Advances in Methods and Practices in Psychological Science, 2(1), 3-11. Strand, J. (2021). Error Tight: Exercises for Lab Groups to Prevent Research Mistakes. 19.5 Avoiding p-hacking See both of these articles (Wicherts et al. 2016; Stratton and Neil 2005) to obtain advice on how to avoid p-hacking and other problems. Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., &amp; Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in psychology, 7, 1832. Stratton, I. M., &amp; Neil, A. (2005). How to ensure your paper is rejected by the statistical reviewer. Diabetic medicine, 22(4), 371-373. Obviously, the advice in this article is “reverse-keyed”. Also don’t forget about the Guelph Psychology Department Thesis Guidelines Website. The website is largely based on the advice of (Wicherts et al. 2016) but is slightly broader in focus and provides Department specific advice. 19.6 Writing articles I suggest you check out (Gernsbacher 2018) “Writing empirical articles: Transparency, reproducibility, clarity, and memorability. Advances in methods and practices in psychological science” for excellent advice on writing articles (pictured below). Conversely, I suggest you AVOID “Writing the Empirical Journal Article” by Daryl Bem because this article has been described by some as a “how to guide” for p-hacking (i.e., finding the prettiest path in the garden of forking analysis paths). Gernsbacher, M. A. (2018). Writing empirical articles: Transparency, reproducibility, clarity, and memorability. Advances in methods and practices in psychological science, 1(3), 403-414. 19.7 Writing with R The packages described below are very helpful for learning to write papers within RStudio. 19.7.1 rmarkdown / bookdown One approach to avoiding errors in your article/thesis is to create a dynamic document. In this type of document you do not type the numbers into the document. Rather the document contains your analysis script (hidden from readers) and it inserts the calculated values into the text of the document. The exciting part of this type of document is that a single rmarkdown document and produce a number output formats such as PDF, Word, Power Point, HTML - as illustrated in the diagram below. You can learn more about rmarkdown in this video. I suggest you read the official documentation to get started. Incidently, the PDF course assignments are made with rmardown - as well as this website! Some other great resources: pagedreport. If you are writing a consulting report this package could be very useful. posterdown Instructions in case you want to make an academic poster using rmarkdown. R Markdown: The Definitive Guide by Xie, Allaire, and Grolemund. Free. R Markdown Cookbook by Xie, Dervieux, and Riederer. Free. bookdown: Authoring Books and Technical Documents with R Markdow by Xie. Free. Karl Broman’s website has many helpful tips. Especially this page. 19.7.2 LaTex As you learn more about creating PDF document using rmarkdown - you will eventually want to learn about using LaTex. You can insert LaTex codes into your rmarkdown document to adjust the formatting (e.g., font size, etc.). Here are a few LaTex resources. Cheatsheet Intro video Font size commands 19.7.3 papaja You may also find the rmarkdown template papaja package by Frederik Aust helpful. It’s an easy way to use rmarkdown. It is a based on the rmarkdown extension called bookdown. This package is specifically designed to make it easy to use rmarkdown/bookdown to make an APA style paper. Indeed that’s the basis for the odd package name: Preparing APA Journal Articles (papaja). I suggest you read the extensive papaja (documentation)[https://crsh.github.io/papaja_man/introduction.html]. It will be worth your while! The only slight complication with papaja is the fact it is not on the CRAN and can’t be installed in the usual way. But it’s still straight forward. You can install papaja with the commands below - taken from the papaja website. # Install devtools package if necessary if(!&quot;devtools&quot; %in% rownames(installed.packages())) install.packages(&quot;devtools&quot;) # Install the stable development verions from GitHub devtools::install_github(&quot;crsh/papaja&quot;) Indeed, once papaja is installed - you simply have to select the APA template before you enter your rmarkdown, as illustrated below: 19.7.4 Quarto The rmarkdown language for document creation has evolved into Quarto. Quarto is more or less the same but is cross-platform statistically (i.e., Python, Julia, etc.). It represents the next step in this approach to document creation. I suggest you check it out - but Quarto is still in the early days. There are not nearly as many blogs, posts, or YouTube videos on Quarto as there are on rmarkdown - yet. So right now, I suggest you still learn rmarkdown, but realize the future is a slightly tweaked version of rmarkdown called Quarto. 19.7.5 apaTables If you don’t want to learn rmarkdown you may find the apaTables package useful - it can easily create the most commonly used APA tables formatted for Microsoft Word. The documentation has extensive examples. You can also see the published guide by Stanley and Spence (2018). 19.8 Writing with statcheck One concern associated with the replicability crisis is that the numbers reported in published articles are simply wrong. The numbers could be wrong due to typos or due to deliberate alteration (to ensure p &lt; .05). Interestingly, one study decided to check if the p-values published in articles were correct (Nuijten et al. 2016). The authors checked the articles using the software statcheck. You can think of statcheck as a statistical spell checker that independently recreates the p-values in an article and checks if the reported p-value is correct. The authors used this process on over 250,000 p-values reported in eight major journals between 1985 and 2013. They found that roughly 50% of journal articles had a least one reporting error. Moreover, one in eight journal articles had a reporting error sufficiently large that it likely altered the conclusions of the paper. Note that incorrect p-values reported were typically smaller than they should been such that the incorrectly reported p-value was less than .05. That’s quite a large number of studies with incorrect p-values! 19.8.1 statcheck software Fortunately, you can use statcheck on your own work before submitting it to an adviser or a journal. The statcheck software is available, as a website, a plug-in for Microsoft Word, and as an R package. You can see the GitHub page for statcheck here. 19.8.2 statcheck website The statcheck website is easy to use. Just upload your PDF or Word document and it will perform the statcheck scan to determine if the numbers in your papers are correct / internally consistent. You can try it out with the PDF of a published article. You can see the first few rows of the statcheck output for an article below: 19.8.3 statcheck and Word Interestingly, statcheck will soon be available as plug-in for Word – as illustrated below. As you type it will perform the statcheck scan to determine if the numbers in your papers are correct / internally consistent. You can see the GitHub page for statcheck Word plug-in here. 19.8.4 statcheck process Exactly how does statcheck work? Statcheck is based on the fact that authors report redundant information in their papers. For example, an author might report the statistics: t(46) = 2.40, p = .0102 (one-sided). Or in the past report this information using a p-value threshold: t(46) = 2.40, p &lt; .05 (one-sided). The first part of this reporting, t(46) = 2.40, can be used to independently generate the p-value, as illustrated below. The software does so and then simply compares the independently generated p-value with the reported p-value (e.g., p = 0102) or p-value threshold (p &lt; .05). You would think the independently generated p-value and the reported p-value would always match. But as illustrated by (Nuijten et al. 2016) at least 50% of papers of a problem with the p-values reported matching the correct p-value. 19.8.5 statcheck validity Although there were some initial concerns about the validity of statcheck, subsequent research on the package indicates an impressive validity level of roughly 90% (or a little higher/lower depending on the settings used). Indeed, in July of 2016, the journal Psychological Science started using statcheck on all submitted manuscripts - once they passed an initial screen. Journal editor, Stephen Lindsey, reports there has been little resistance to doing so “Reaction has been almost non-existent.” 19.9 Journal rankings via the TOP Factor When you’re done writing - you need to decide upon a journal. You can see journal rankings based on the Transparency and Openness Promotion Guidelines at the Top Factor website. 19.10 Statistics books If you want to learn more about statistics I suggest (Maxwell, Delaney, and Kelley 2017), (Cohen et al. 2014), and (Baguley 2012). Maxwell, S. E., Delaney, H. D., &amp; Kelley, K. (2017). Designing experiments and analyzing data: A model comparison perspective. Routledge. by Maxwell, Delaney, and Kelley. This book has a fantastic website (https://designingexperiments.com) with online tools and R scripts. Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2014). Applied multiple regression/correlation analysis for the behavioral sciences. Psychology press. by Cohen, Cohen, West, and Aiken. This is the “go to” resource if you are conducting moderated multiple regression. A great book on regression beyond just moderation though. Baguley, T. (2012). Serious stats: A guide to advanced statistics for the behavioral sciences. Macmillan International Higher Education. by Baguley. The book title tells you everything you need to know about this one. Hayes, A. F. (2017). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications.. Many people use the approach in this book when they have multiple mediators and moderators. The PROCESS [website]((https://processmacro.org/index.html) has the R-script for the PROCESS macro. 19.11 General R books There are many R books out there. I believe that you will find these most helpful: Data Visualization by Healy. Free. Fundamentals of Data Visualization by Wilke. Free. R Graphics Cookbook by Chang. Free. ggplot2 by Wickham. Free. R for Data Science by Wickham and Grolemund. Free. Hands-On Programming with R by Grolemund. Free. Advanced R by Wickham. Free. Art of R Programming by Matloff. Then some books that are great but less likely to by used by psychology folks: Mastering Shiny by Wickham. Free. R packages by Wickham Free. Efficient R Programming by Lovelace. Free. Text Mining with R by Silge and Robinson. Free. Javascript for R by Coene. Free. Practical Data Science with R by Zumel and Mount. Functional Programming in R by Mailund. Deep learning with R Chollet and Allaire. Extending R by Chambers. 19.12 Retracted articles As you write-up your research you need to be concerned with the problem of citing research papers that have been retracted. This problem is substantially larger than you might first expect; indeed, one group of researchers found that retracted papers often received the majority their citations after retraction (Madlock-Brown and Eichmann 2015). Therefore, take the extra time to confirm the papers you cite have not been retracted! Moreover, don’t assume because an article was published in a high-impact journal that it is a high quality article - and not likely to be retracted. The truth is the opposite. Retraction rates correlate positively with journal impact factor (how often articles in that journal are cited). Specifically, journals with high impact factors have the higest retraction rate (Fang and Casadevall 2011). 19.12.1 DOI But how do you go about determining if a paper has been retracted? There are websites you can check like retraction watch. It can, however, be time consuming to check for every article in this website. There is an easier approach but it requires you know the DOI number for each article you cite. What is a DOI number? All modern journal articles have a DOI (digital object identifier) number associated with them. This is a unique number that identifies the article and can be used to access the document. You can see a DOI number on a PDF: Or you can see a DOI number on the website: Retraction search with DOI You can enter the DOI up on the search site as illustrated below. Then click the Search button. You will get the search output. Notice the yellow box in the lower left which indicates this article has NOT been retracted. 19.12.2 retractiondatabase.org If you don’t have the DOI number for an article you can search for by article title or author at http://retractiondatabase.org as you can see from the interface below. 19.12.3 openretractions.com However, if you have the DOI number for an article, an easier approach is use the http://openretractions.com website. At this website you type in the DOI number for an article and it checks if that article has been retracted. 19.12.4 retractcheck Even better, you can use the retractcheck R package. With this package you can check large batches of DOI numbers with openretractions.com to see if the corresponding articles have been retracted. You can use this package by the command line or via the website illustrated below. 19.13 Big data Occasionally psychology researchers deal with big data. File sizes can be quite large with big data. Check out the arrow package, specifically, the write_parquet() command as means of using smaller file sizes. This approach can make sharing a file on GitHub, or emailing it to a colleague, substantially easier. References "],["cookbook.html", "Chapter 20 Cookbook 20.1 Required Packages 20.2 Following the examples 20.3 Entering data into spreadsheets 20.4 Experiment: Between 20.5 Within Data Entry 20.6 Experiment: Within one-way 20.7 Experiment: Within N-way 20.8 Surveys: Single Occassion 20.9 Surveys: Multiple Occasions 20.10 Basic descriptive statistics", " Chapter 20 Cookbook The chapter provide a series of “Cookbook” receipes for starting different types of analysis scripts. 20.1 Required Packages The data files below are used in this chapter. Required Data data_ex_between.csv data_ex_within.csv data_food.csv data_item_scoring.csv data_item_time.csv The following CRAN packages must be installed: Required CRAN Packages apaTables Hmisc janitor psych skimr tidyverse Important Note: You should NOT use library(psych) at any point! There are major conflicts between the psych package and the tidyverse. We will access the psych package commands by preceding each command with psych:: instead of using library(psych). 20.2 Following the examples Below we present example scripts transforming raw data to analytic data for various study designs (experimental and survey). These examples illustrate the value of using the naming conventions outlined previously. Don’t just read the example - follow along with the projects by creating a separate script for each example. Resist the urge to cut and paste from this document - type the script yourself. When first learning iPhone/Mac software development, I did so by taking a course at Big Nerd Ranch - yes, that’s a real place. They advised in their material (and now book) the following: “We have learned that “going through the motions” is much more important than it sounds. Many times we will ask you to start typing in code before you understand it. We realize that you may feel like a trained monkey typing in a bunch of code that you do not fully grasp. But the best way to learn coding is to find and fix your typos. Far from being a drag, this basic debugging is where you really learn the ins and outs of the code. That is why we encourage you to type in the code yourself. You could just download it, but copying and pasting is not programming. We want better for you and your skills.”, p. xiv, (Keur and Hillegass 2020). This is excellent advice for a beginning statistician or data scientist as well. And as an aside: if you want to learn iPhone programming you can’t go wrong with the Big Nerd Ranch guide! As you work through this chapter, create your own new script for each example. In light of the above advice, avoid copying and pasting code - type it out; you will be the better for it. Getting started: The Class: R Studio in the Cloud Assignment The data should be in the assignment project automatically. Just start the assignment. For everyone in the class, that’s it. For those of you not in the class, and reading this work, see the two options below: R Studio Cloud, custom project Create a new Project using the web interface Upload all the example data files into the project. The data files needed are listed at the beginning of this chapter. The upload button can be found on the Files tab. R Studio Computer, custom project Create a folder on your computer for the example Place all the example data files in that folder. The data files needed are listed at the beginning of this chapter. Use the menu item File &gt; New Project… to start the project On the window that appears select “Existing Directory” On the next screen, press the “Browse” button and find/select the folder with your data Press the Create Project Button Regardless of whether your are working from the cloud, or locally, you should now have an R Studio project with your data files in it. We anticipate that many people will doubtless want to refer back to an encapsulated set of instructions for each design. Therefore the example for each design is written in a way that it stands alone. A consequence of this approach is that there is some redundancy in the code across examples. We see this a strength - because readers will see the commonalities across differ types of designs. As you make a script for each example: Recall the instruction from Chapter 1 about putting the date and your name in the script via comments. Recall the instruction from Chapter 1 about running library(tidyverse) before you type the rest of each script - this provides you with tidyverse autocomplete for the script. After you type each new block of code in an example, save your script. After you type each new block of code in an example, do two additional things: 1) Session Restart R, 2) Run your script using Source with Echo. 20.3 Entering data into spreadsheets The first example uses a data file data_ex_between.csv that corresponds to a fictitious example where we recorded the run times for a number of male and female participants. How did we create this data file? We used a spreadsheet to enter the data, as illustrated in Figure 20.1. Programs like Microsoft Excel and Google Sheets are good options for entering data. FIGURE 20.1: Spreadsheet entry of running data The key to using these types of programs is to save the data as a .csv file when you are done. CSV is short for Comma Separated Values. After entering the data in Figure 20.1 we saved it as data_ex_between.csv. There is no need to do so, but if you were to open this file in a text editor (such as TextEdit on a Mac or Notepad on Windows) you would see the information displayed in Figure 20.2. You can see there is one row per person and the columns are created by separating each values by a comma; hence, comma separated values. FIGURE 20.2: Text view of CSV data There are many ways to save data, but the CSV data is one of the better ones because it is a non-proprietary format. Some software, such as SPSS, uses a proprietary format (e.g., .sav for SPSS) this makes it challenging to access that data if you don’t have that (often expensive) software. One of our goals as scientists is to make it easy for others to audit our work - that allows science to be self-correcting. Therefore, choose an open format for your data like .csv. 20.4 Experiment: Between This section outlines a workflow appropriate for when you plan to a conduct independent-groups t-test or a between-participants ANOVA. To Begin: Use the Files tab to confirm you have the data: data_ex_between.csv Start a new script for this example. Don’t forget to start the script name with “script_”. As noted previously, these data correspond to a design where the researcher is interested in comparing run times (elapsed_time) based on sex (male/female). # Date: YYYY-MM-DD # Name: your name here # Example: Between-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_between &lt;- read_csv(file = &quot;data_ex_between.csv&quot;, na = my_missing_value_codes) We load the initial data into a raw_data_between data set but immediately make a copy that we will work with called analytic_data_between. It’s good to keep a copy of the raw data for reference in the event that you encounter problems. analytic_data_between &lt;- raw_data_between After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_between &lt;- analytic_data_between %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names follow our naming convention with the glimpse() command. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;… ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 20.4.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. We have one variable, sex, that is a categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;… ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 You can quickly convert all character columns to factors using the code below. In this case, the code just converts the sex column to a factor. Because there is only one column (sex) being converted to a factor, we could have treated it the same way as the id column below. However, we use this code because of its broad applicability to many scripts. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so it was not converted by the above code. The id column is converted to a factor with the code below. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(id = as_factor(id)) You can ensure both the sex and id columns are now factors using the glimpse() command. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, fem… ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 This example is so small it’s clear you didn’t miss converting any columns to factors. In general, however, at this point you should inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. 20.4.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_between %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 The order of the levels influences how graphs are generated. In these data, the sex column has two levels: male and female in that order. The code below adjusts the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_between %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 20.4.3 Numeric screening For numeric variables, you should search for impossible values. For example, in the context of this example you want to ensure that none of the elapsed_times are impossible, or so large they appear to be data entry errors. One option for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. analytic_data_between %&gt;% select(where(is.numeric)) %&gt;% summary() ## elapsed_time ## Min. :33.0 ## 1st Qu.:35.2 ## Median :37.0 ## Mean :37.3 ## 3rd Qu.:39.5 ## Max. :42.0 Scan the min and max values to ensure there are not any impossible values. If necessary, go back to the original data source and fix these impossible values. Alternatively, you might need to change them to missing values (i.e., NA values). In this example all the values are reasonable values. However, if we discovered an out of range value (or values) for elapsed time, we could convert those values to missing values with the code below. This code changes (i.e., mutates) a value in the elapsed_time column to become NA (not available or missing) if that value is less than zero. If the value is greater than, or equal to zero, it stays the same. Note that when using this command we have to be very specific in terms of specifying our missing value. It usually needs to be one of NA_real_ or NA_character_. For numeric columns use NA_real_ and for character columns use NA_character_. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(elapsed_time = case_when( elapsed_time &lt; 0 ~ NA_real_, elapsed_time &gt;= 0 ~ elapsed_time)) Once you are done numeric screening, the data is ready for analysis. 20.5 Within Data Entry If you have a study that involves within-participant predictors naming conventions can become examples. When you have a single repeated measures predictor like occasion in the previous running example, it is often necessary to spread the level of that variable over multiple columns (e.g., march, may, july). When you have multiple repeated-measures predictors the situation is even more complicated. In this case, each column name needs to represent the levels of multiple repeated measures predictors at the time of data entry. For example, imagine you are a food researcher interested in taste ratings (the dependent variable) for various foods and contexts. You have a predictor, food type (i.e., food_type), with three levels (pizza, steak, burger). You have a second predictor, temperature, with two levels (hot, cold). All participants taste all foods at all temperatures. Thus, six columns are required to record taste rating for each participant: pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold. Notice how each name contains one level of each predictor variable. The levels by the two predictor variables are separated by a single underscore. This should be the only underscore in the variable name because that underscore will be used by the computer when changing the data to the tidy format. If you had two underscores in a name like “italian_pizza_hot” you would confuse the pivot_longer() command when it attempts to create a tidy version of the data. The computer would think there were three repeated-measures predictors instead of two. Thus, when dealing with repeated measures predictors, only use underscores to separate levels of predictor variables in column names. 20.6 Experiment: Within one-way This section outlines a workflow appropriate for when you have a repeated measures design with a single repeated measures predictor. The data corresponds to a design where the researcher is interested in comparing run times (elapsed_time) across three different occasions (march/may/july). To Begin: Use the Files tab to confirm you have the data: data_ex_within.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: Within-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_within &lt;- read_csv(file = &quot;data_ex_within.csv&quot;, na = my_missing_value_codes) ## Rows: 6 Columns: 5 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sex ## dbl (4): id, march, may, july ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We load the initial data into raw_data_within but immediately make a copy that we will work with called analytic_data_within. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_within &lt;- raw_data_within After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_within &lt;- analytic_data_within %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, … ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 20.6.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, … ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 We have one variable, sex, that is a categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. You can quickly convert all character columns to factors using the code below. In this case, this just converts the sex column to a factor. Because there is only one column (sex) being converted to a factor, we could have treated it the same was as the id column below. However, we use this code because of its broad applicability to many scripts. analytic_data_within &lt;- analytic_data_within %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so it was not converted by the above code. The id column is converted to a factor with the code below. analytic_data_within &lt;- analytic_data_within %&gt;% mutate(id = as_factor(id)) You can ensure both the sex and id columns are now factors using the glimpse() command. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, female ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 This example is so small it’s clear you didn’t miss converting any columns to factors. In general, however, at this point you should inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. 20.6.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_within %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 The order of the levels influences how graphs are generated. In these data, the sex column has two levels: male and female in that order. The code below adjusts the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_within &lt;- analytic_data_within %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_within %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 20.6.3 Numeric screening For numeric variables, it is important to find and remove impossible values. For example, in the context of this example you want to ensure none of the elapsed_times are impossible (i.e., negative) or clearly data entry errors. Because we have several numeric columns that we are screening, we use the skim() command from the skimr package. The skim() command quickly provides basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. library(skimr) analytic_data_within %&gt;% select(where(is.numeric)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 march 0 37.33 3.33 33 37 42 ## 2 may 0 34.33 3.33 30 34 39 ## 3 july 0 32.33 3.33 28 32 37 Scan the minimum and maximum values (p0 and p100) to ensure there are not any impossible values. If necessary, go back to the original data source and fix these impossible values. Alternatively, you might need to change them to missing values (i.e., NA values). In this example all the values are reasonable values. However, if we discovered an out of range value (or values) for elapsed time we could convert those values to missing values with the code below. This code changes (i.e., mutates) a value in the march column to become NA (not available or missing) if that value is less than zero. If the value is greater than or equal to zero, it stays the same. Note that when using this command we have to be very specific in terms of specifying our missing value. It usually needs to be one of NA_real_ or NA_character_. For numeric columns use NA_real_ and for character columns use NA_character_. analytic_data_within &lt;- analytic_data_within %&gt;% mutate(march = case_when( march &lt; 0 ~ NA_real_, march &gt;= 0 ~ march)) 20.6.4 Pivot to tidy data The analytic data in it’s current form does not conform to the tidy data specification. Inspect the data with the print() command. Notice thatthere is not a column for occasion (with levels march/may/july). Instead, there are three columns each of which represents a level of occasion. The levels of occasion are spread across three columns called march, may, and july. Each of these columns contains elapsed time for participants in that month. print(analytic_data_within) ## # A tibble: 6 × 5 ## id sex march may july ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 male 40 37 35 ## 2 2 female 35 32 30 ## 3 3 male 38 35 33 ## 4 4 female 33 30 28 ## 5 5 male 42 39 37 ## 6 6 female 36 33 31 The pivot_longer() command below coverts our data to the tidy data format. In this command we specify the columns march, may, and july are all levels of a single variable called occasion. We specify the columns involved with the cols argument. The code march:july after the cols argument selects the march column, the july column, and all the columns in-between. Each column contains elapsed times at level of the variable occasion. The names_to argument is used to indicate that a new column called occasion should be created to hold the different months. The value_to argument is used to indicate that a new column called elapsed_time should created to hold all the values from the march, may, and july columns. analytic_data_within_tidy &lt;- analytic_data_within %&gt;% pivot_longer(cols = march:july, names_to = &quot;occasion&quot;, values_to = &quot;elapsed_time&quot; ) You can see the data in the new format below. print(analytic_data_within_tidy) ## # A tibble: 18 × 4 ## id sex occasion elapsed_time ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male march 40 ## 2 1 male may 37 ## 3 1 male july 35 ## 4 2 female march 35 ## 5 2 female may 32 ## 6 2 female july 30 ## 7 3 male march 38 ## 8 3 male may 35 ## 9 3 male july 33 ## 10 4 female march 33 ## 11 4 female may 30 ## 12 4 female july 28 ## 13 5 male march 42 ## 14 5 male may 39 ## 15 5 male july 37 ## 16 6 female march 36 ## 17 6 female may 33 ## 18 6 female july 31 Notice that the new column occasion is of the type character. We need it to be a factor. So use the code below to do so: analytic_data_within_tidy &lt;- analytic_data_within_tidy %&gt;% mutate(occasion = as_factor(occasion)) You can confirm that occasion is now a factor with the glimpse() command. Once this is complete, you are done preparing your one-way within participant analytic data. glimpse(analytic_data_within_tidy) ## Rows: 18 ## Columns: 4 ## $ id &lt;fct&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5… ## $ sex &lt;fct&gt; male, male, male, female, female, fem… ## $ occasion &lt;fct&gt; march, may, july, march, may, july, m… ## $ elapsed_time &lt;dbl&gt; 40, 37, 35, 35, 32, 30, 38, 35, 33, 3… You now have two data sets analytic_data_within and analytic_data_within_tidy. You can calculate descriptive statistics, correlations and general cross-sectional analyses using the analytic_data_within data set. If you want to conduct a repeated measures ANOVA you use the analytic_data_within_tidy data set. Both data sets are now ready for analysis. 20.7 Experiment: Within N-way This section outlines a workflow appropriate for when you have a repeated measures design with multiple repeated measures predictors. The data corresponds to a design where the researcher is interested in assessing the taste of food as a function of food type (pizza/steak/burger) and temperature (hot/cold). To Begin: Use the Files tab to confirm you have the data: data_food.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: 2-way within-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_within_nway &lt;- read_csv(file = &quot;data_food.csv&quot;, na = my_missing_value_codes) We load the initial data into raw_data_within_nway but immediately make a copy that we will work with called analytic_data_within_nway. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_within_nway &lt;- raw_data_within_nway After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;dbl&gt; 1, 2, 1, 2, 1, 2 ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 20.7.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. In this example, there are two categorical variables id and sex, but both are represented numerically. As revealed by the glimpse() output. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;dbl&gt; 1, 2, 1, 2, 1, 2 ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 We convert both the sex and id columns to factors with the mutate() command below: analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(id = as_factor(id), sex = as_factor(sex)) The sex column is a factor but we have to tell the computer that 1 indicates male and 2 indicates female. analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(sex = fct_recode(sex, male = &quot;1&quot;, female = &quot;2&quot;)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, fema… ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 Inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. As noted in the previous examples, its common to have additional columns that are categorical predictors but appear in the glimpse() output as being of the type character. That is not the case in these data, but if it were the command below would turn them into factors: analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) 20.7.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_within_nway %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 The order of the levels influences how graphs are generated. In these data, the sex column has two levels: male and female in that order. The code below adjusts the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_within_nway %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 20.7.3 Numeric screening For numeric variables, it’s important find and remove impossible values. For example, in the context of this example you want to ensure none of the taste ratings the six columns (pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold) are outside the range of the 1 to 10 rating scale. Because we have several numeric columns that we are screening, we use the skim() command from the skimr package. The skim() command quickly provides basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. library(skimr) analytic_data_within_nway %&gt;% select(where(is.numeric)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 pizza_hot 0 7.67 0.82 7 7.5 9 ## 2 pizza_cold 0 6.33 0.82 5 6.5 7 ## 3 steak_hot 0 6.83 0.75 6 7.0 8 ## 4 steak_cold 0 5.00 2.10 3 4.5 8 ## 5 burger_hot 0 7.50 0.55 7 7.5 8 ## 6 burger_cold 0 3.33 1.03 2 3.0 5 Scan the minimum and maximum values (p0 and p100) to ensure there are not any impossible values. That is, ensure all values are inside the 1 to 10 range for the dependent variable. If necessary, get back to the original data source and fix these impossible values. Alternatively, you might need to change them to missing values (i.e., NA values). In this example all the values are reasonable values. However, if we discovered an out-of-range value (or values) for elapsed time we could convert those values to missing values with the code below. This code changes (i.e., mutates) a value in the pizza_hot column to become NA (not available or missing) if that value is outside the 1 to 10 range of the rating scale. Note that when using this command we have to be very specific in terms of specifying our missing value. It usually needs to be one of NA_real_ or NA_character_. For numeric columns use NA_real_ and for character columns use NA_character_. # Values lower than 1 are converted to missing values analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(pizza_hot = case_when( pizza_hot &lt; 1 ~ NA_real_, pizza_hot &gt;= 1 ~ pizza_hot)) # Values greater than 10 are converted to missing values analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(pizza_hot = case_when( pizza_hot &gt; 10 ~ NA_real_, pizza_hot &lt;= 10 ~ pizza_hot)) 20.7.4 Pivot to tidy data The analytic data in its current form does not conform to the tidy data specification. Inspect the data with the print() command. Notice that columns do not exist for temperature (with levels hot/cold) or food type (with levels pizza/steak/burger). Instead, there are six columns that are combinations of the levels of these variables (i.e., pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold). Each of these columns contains taste ratings on a 1 to 10 point scale. print(analytic_data_within_nway) ## # A tibble: 6 × 8 ## id sex pizza_hot pizza_cold steak_hot steak_cold ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 male 7 6 6 3 ## 2 2 female 8 7 6 3 ## 3 3 male 7 5 7 4 ## 4 4 female 8 7 7 5 ## 5 5 male 7 6 8 7 ## 6 6 female 9 7 7 8 ## # ℹ 2 more variables: burger_hot &lt;dbl&gt;, burger_cold &lt;dbl&gt; We need to restructure the data into the tidy data format so that we have a food_type column and a temperature column to properly represent these predictors. As well, we need a column that contains all of the taste ratings that is clearly labeled taste. Doing all of these things will ensure we have one variable per column and one observation per row - consistent with the requirements of tidy data. The pivot_longer() command below coverts our data to the tidy data format. In this command we specify the columns march, may, and july are all levels of a single variable called occasion. We specify the columns involved with the cols argument. The code pizza_hot:burger_cold after the cols argument selects the pizza_hot column, the burger_cold column, and all the columns in between. Each column contains taste ratings at a combination of the levels for the variables food_type and temperature. The names_to argument is used to indicate that two new columns should be created to represent food and temperature. Notice the order food then temperature. This is consistent with our naming convention; in the column name pizza_hot the food_type is specified before temperature. The value_to argument is used to indicate that a new column called taste should created to hold all the values from the pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold columns. analytic_data_nway_tidy &lt;- analytic_data_within_nway %&gt;% pivot_longer(cols = pizza_hot:burger_cold, names_to = c(&quot;food_type&quot;, &quot;temperature&quot;), names_sep = &quot;_&quot;, values_to = &quot;taste&quot; ) You can see the data in the new format below. print(analytic_data_nway_tidy) ## # A tibble: 36 × 5 ## id sex food_type temperature taste ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male pizza hot 7 ## 2 1 male pizza cold 6 ## 3 1 male steak hot 6 ## 4 1 male steak cold 3 ## 5 1 male burger hot 7 ## 6 1 male burger cold 4 ## 7 2 female pizza hot 8 ## 8 2 female pizza cold 7 ## 9 2 female steak hot 6 ## 10 2 female steak cold 3 ## # ℹ 26 more rows Notice that the new column occasion is of the type character. We need it to be a factor. Use the code below to do so: analytic_data_nway_tidy &lt;- analytic_data_nway_tidy %&gt;% mutate(food = as_factor(food_type), temperature = as_factor(temperature)) You can confirm that occasion is now a factor with the glimpse() command. Once this is complete, you are done preparing your one-way within participant analytic data. glimpse(analytic_data_nway_tidy) ## Rows: 36 ## Columns: 6 ## $ id &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3,… ## $ sex &lt;fct&gt; male, male, male, male, male, male, fe… ## $ food_type &lt;chr&gt; &quot;pizza&quot;, &quot;pizza&quot;, &quot;steak&quot;, &quot;steak&quot;, &quot;b… ## $ temperature &lt;fct&gt; hot, cold, hot, cold, hot, cold, hot, … ## $ taste &lt;dbl&gt; 7, 6, 6, 3, 7, 4, 8, 7, 6, 3, 8, 3, 7,… ## $ food &lt;fct&gt; pizza, pizza, steak, steak, burger, bu… You now have two data sets analytic_data_within_nway and analytic_data_nway_tidy. You can calculate descriptive statistics, correlations and general cross-sectional analyses using the analytic_data_within_nway data set. If you want to conduct a repeated measures ANOVA you use the analytic_data_nway_tidy data set. Both data sets are now ready for analysis. 20.8 Surveys: Single Occassion This section outlines a workflow appropriate for when you have cross-sectional single occasion survey data. The data corresponds to a design where the researcher has measured, age, sex, eye color, self-esteem, and job satisfaction. Two of these, self-esteem and job satisfaction, were measured with multi-item scales with reverse-keyed items. To Begin: Use the Files tab to confirm you have the data: data_item_scoring.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: Single occasion survey # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_survey &lt;- read_csv(file = &quot;data_item_scoring.csv&quot;, na = my_missing_value_codes) ## Rows: 300 Columns: 14 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): sex, eye_color ## dbl (12): id, age, esteem1_likert5, esteem2_likert5, est... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We load the initial data into a raw_data_survey but immediately make a copy we will work with called analytic_data_survey. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_survey &lt;- raw_data_survey Remove empty row and columns from your data using the remove_empty_cols() and remove_empty_rows(), respectively. As well, clean the names of your columns to ensure they conform to tidyverse naming conventions. library(janitor) # Initial cleaning analytic_data_survey &lt;- analytic_data_survey %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fema… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … 20.8.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fema… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … We have two variables, sex and eye_color, that are categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. You can quickly convert all character columns to factors using the code below: analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so we have to handle that column on its own. analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22,… ## $ sex &lt;fct&gt; male, female, male, female, mal… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, h… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4,… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, … Inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. Note: f you have factors like sex that have numeric data in the column (e.g, 1 and 2) instead of male/female you need to handle the situation differently. The preceding section, Experiment: Within N-way, illustrates how to handle this scenario. 20.8.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 male :147 blue : 99 ## 2 : 1 female :149 brown: 98 ## 3 : 1 intersex: 2 hazel:100 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = fct_relevel(sex, &quot;intersex&quot;, &quot;female&quot;, &quot;male&quot;)) For eye color, we want a future graph to have the most common eye colors on the left so we reorder the factor levels: analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(eye_color = fct_infreq(eye_color)) You can see the new order of the factor levels with summary(): analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 intersex: 2 hazel:100 ## 2 : 1 female :149 blue : 99 ## 3 : 1 male :147 brown: 98 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 20.8.3 Numeric screening For numeric variables, it’s important to find and remove impossible values. For example, in the context of this example you want to ensure none of the Likert responses are impossible (e.g., outside the 1- to 5-point rating scale) or clearly data entry errors. Because we have several numeric columns that we are screening, we use the skim() command from the skimr package. The skim() command quickly provides basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. Start by examining the range of non-scale items. In this case it’s only age. Examine the output to see if any of the age values are unreasonable. As noted, in the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the minimum and maximum values for the variable. Check to make sure none of the age values are unreasonably low or high. If they are, you may need to check the original data source or replace them with missing values. library(skimr) analytic_data_survey %&gt;% select(age) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17 20 24 With respect to the multi-item scales, it makes sense to look at sets of items rather than all of the items at once. This is because sometimes items from different scales use different response ranges. For example, one measure might use a response scale with a range from 1 to 5; whereas another measure might use a response scale with a range from 1 to 7. This is undesirable from a psychometric point of view, as discussed previously, but if it happens in your data - look at the scale items separately to make it easy to see out of range values. We begin by looking at the items in the first scale, self-esteem. Possible items responses for this scale range from 1 to 5; make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 esteem1_likert5 24 3.39 0.54 3 3 5 ## 2 esteem2_likert5 28 2.35 0.48 2 2 3 ## 3 esteem3_likert5 31 3.96 0.37 3 4 5 ## 4 esteem4_likert5 15 3.54 0.50 3 4 4 ## 5 esteem5_likert5rev 35 2.22 0.47 1 2 3 Follow the same process for the job satisfaction items. Write that code on your own now. Possible item responses for the job satisfaction scale range from 1 to 5, make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 jobsat1_likert5 25 3.34 0.51 3 3 5 ## 2 jobsat2_likert5rev 27 1.51 0.61 1 1 3 ## 3 jobsat3_likert5 28 2.84 0.37 2 3 3 ## 4 jobsat4_likert5 35 4.29 0.70 3 4 5 ## 5 jobsat5_likert5 24 4.57 0.61 3 5 5 20.8.4 Scale scores For each person, scale scores involve averaging scores from several items to create an overall score. The first step in the creation of scales is correcting the values of any reverse-keyed items. 20.8.4.1 Reverse-key items The way you deal with reverse-keyed items depends on how you scored them. Imagine you had a 5-point scale. You could have scored the scale with the values 1, 2, 3, 4, and 5. Alternatively, you could have scored the scale with the values 0, 1, 2, 3, and 4. The mathematical approach you use to correcting reverse-keyed items depends upon whether the scale starts with 1 or 0. In this example, we scored the data using the value 1 to 5; so that is the approach illustrated here. See the extra information box for details on how to fixed reverse-keyed items when the scale begins with zero. In this data file all the reverse-keyed items were identified with the suffix “_likert5rev” in the column names. This suffix indicates the item was reverse keyed and that the original scale used the response points 1 to 5. We can see those items with the glimpse() command below. Notice that there are two reverse-keyed items - each on difference scales. analytic_data_survey %&gt;% select(ends_with(&quot;_likert5rev&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 2 ## $ esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2,… ## $ jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, … To correct a reverse-keyed item where the lowest possible rating is 1 (i.e, 1 on a 1 to 5 scale), we simply subtract all the scores from a value one more than the highest point possible on the scale (i.e., one more than 5). For example, if a 1 to 5 response scale was used we subtract each response from 6 to obtain the recoded value. Original value Math Recoded value 1 6 - 1 5 2 6 - 2 4 3 6 - 3 3 4 6 - 4 2 5 6 - 5 1 The code below: selects columns that end with “_likert5rev” (i.e., both esteem and jobsat scales) subtracts the values in those columns from 6 renames the columns by removing “_likert5rev” from the name because the reverse coding is complete analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(6 - across(.cols = ends_with(&quot;_likert5rev&quot;)) ) %&gt;% rename_with(.fn = str_replace, .cols = ends_with(&quot;_likert5rev&quot;), pattern = &quot;_likert5rev&quot;, replacement = &quot;_likert5&quot;) You can use the glimpse() command to see the result of your work. If you compare these new values to those obtained from the previous glimpse() command you can see they have changed. Also notice the column names no longer indicate the items are reverse keyed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17… ## $ sex &lt;fct&gt; male, female, male, female, male, … ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, haze… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, … ## $ esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4… ## $ jobsat2_likert5 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, … If your scale had used response options numbered 0 to 4 the math is different. For each item you would use subtract values from the highest possible point (i.e, 4) instead of one larger than the highest possible point. Original value Math Recoded value 0 4 - 0 4 1 4 - 1 3 2 4 - 2 2 3 4 - 3 1 4 4 - 4 0 Thus, the mutate command would instead be: mutate(4 - across(.cols = ends_with(“_likert5rev”)) ) 20.8.4.2 Creating scores The process we use for creating scale scores deletes item-level data from analytic_data_survey. This is a desirable aspect of the process because it removes information that we are no longer interested in from our analytic data. That said, before we create scale score, we create a backup on the item-level data called analytic_data_survey_items. We will need to use this backup later to compute the reliability of the scales we are creating. analytic_data_survey_items &lt;- analytic_data_survey We want to make a self_esteem scale and plan to select items using starts_with(“esteem”). But prior to doing this we make sure the start_with() command only gives us the items we want - and not additional unwanted items. The output below confirms there are not problems associated with using starts_with(“esteem”). analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, … ## $ esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3,… Likewise, we want to make a job_sat scale and plan to select items using starts_with(“jobsat”). The code and output below using starts_with(“jobsat”) only returns the items we are interested in. analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4… ## $ jobsat2_likert5 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, … We calculate the scale scores using the rowwise() command. The mean() command provides the mean of columns by default - not people. We use the rowwise() command in the code below to make the mean() command work across columns (within participants) rather than within columns. The mutate command calculates the scale score for each person. The c_across() command combined with the starts_with() command ensures the items we want averaged together are the items that are averaged together. Notice there is a separate mutate line for each scale. The ungroup() command turns off the rowwise() command. We end the code block by removing the item-level data from the data set. Important: Take note of how we name the scale variables (e.g., self_esteem, job_sat). We use a slightly different convention than our items. That is, these scale labels were picked so that they would not be selected by a starts_with(“esteem”) or starts_with(“jobsat”). Why - because we later use those commands to remove the item-level data. We would want the command designed to remove the item-level data to also remove the scale we just calculated! This example illustrates how carefully you need to think about your naming conventions. analytic_data_survey &lt;- analytic_data_survey %&gt;% rowwise() %&gt;% mutate(self_esteem = mean(c_across(starts_with(&quot;esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(job_sat = mean(c_across(starts_with(&quot;jobsat&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;esteem&quot;)) %&gt;% select(-starts_with(&quot;jobsat&quot;)) We can see our data now has the self_esteem column, a job_sat column, and that all of the item-level data has been removed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 6 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA… ## $ sex &lt;fct&gt; male, female, male, female, male, fema… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, b… ## $ self_esteem &lt;dbl&gt; 3.200, 3.800, 3.800, 3.000, 3.400, 3.5… ## $ job_sat &lt;dbl&gt; 4.00, 5.00, 4.40, 3.50, 4.00, 3.80, 3.… You now have two data sets analytic_data_survey and analytic_data_survey_items. You can calculate descriptive statistics, correlations and most analyses using the analytic_data_survey. To obtain the reliability of the scales you just created though you will need to use the analytic_data_survey_items. Both sets of data are ready for analysis. 20.9 Surveys: Multiple Occasions This section outlines a workflow appropriate for when you have multiple occasion survey data. The data corresponds to a design where the researcher has measured, age, sex, eye color, self-esteem, and job satisfaction at each of two times points. Self-esteem and job satisfaction were measured with multi-item scales with reverse-keyed items. To Begin: Use the Files tab to confirm you have the data: data_item_time.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: Multiple occasion survey # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_occasions &lt;- read_csv(file = &quot;data_item_time.csv&quot;, na = my_missing_value_codes) ## Rows: 300 Columns: 24 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): sex, eye_color ## dbl (22): id, age, t1_esteem1_likert5, t1_esteem2_likert... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We load the initial data into a raw_data_occasions but immediately make a copy we will work with called analytic_data_occasions. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_occasions &lt;- raw_data_occasions Remove empty rows and columns from your data using the remove_empty(“rows”) and remove_empty(“cols”), respectively. As well, clean the names of your columns to ensure they conform to tidyverse naming conventions. library(janitor) # Initial cleaning analytic_data_occasions &lt;- analytic_data_occasions %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;… ## $ t1_esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, … ## $ t1_esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, … ## $ t1_esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4,… ## $ t1_esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, … ## $ t1_esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2… ## $ t1_jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, … ## $ t1_jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2,… ## $ t1_jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3,… ## $ t1_jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA… ## $ t1_jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5,… ## $ t2_esteem1_likert5 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5,… ## $ t2_esteem2_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, … ## $ t2_esteem3_likert5 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, … ## $ t2_esteem4_likert5 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, … ## $ t2_esteem5_likert5rev &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ t2_jobsat1_likert5 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, … ## $ t2_jobsat2_likert5rev5 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, … ## $ t2_jobsat3_likert5 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, … ## $ t2_jobsat4_likert5 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, … ## $ t2_jobsat5_likert5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6,… 20.9.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23,… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;… ## $ t1_esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, … ## $ t1_esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, … ## $ t1_esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4,… ## $ t1_esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, … ## $ t1_esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2… ## $ t1_jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, … ## $ t1_jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2,… ## $ t1_jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3,… ## $ t1_jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA… ## $ t1_jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5,… ## $ t2_esteem1_likert5 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5,… ## $ t2_esteem2_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, … ## $ t2_esteem3_likert5 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, … ## $ t2_esteem4_likert5 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, … ## $ t2_esteem5_likert5rev &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ t2_jobsat1_likert5 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, … ## $ t2_jobsat2_likert5rev5 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, … ## $ t2_jobsat3_likert5 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, … ## $ t2_jobsat4_likert5 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, … ## $ t2_jobsat5_likert5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6,… We have two variables, sex and eye_color, that are categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. You can quickly convert all character columns to factors using the code below: analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is numeric, so we have to handle that column on its own. analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23,… ## $ sex &lt;fct&gt; male, female, male, female,… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, N… ## $ t1_esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, … ## $ t1_esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, … ## $ t1_esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4,… ## $ t1_esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, … ## $ t1_esteem5_likert5rev &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2… ## $ t1_jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, … ## $ t1_jobsat2_likert5rev &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2,… ## $ t1_jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3,… ## $ t1_jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA… ## $ t1_jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5,… ## $ t2_esteem1_likert5 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5,… ## $ t2_esteem2_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, … ## $ t2_esteem3_likert5 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, … ## $ t2_esteem4_likert5 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, … ## $ t2_esteem5_likert5rev &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ t2_jobsat1_likert5 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, … ## $ t2_jobsat2_likert5rev5 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, … ## $ t2_jobsat3_likert5 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, … ## $ t2_jobsat4_likert5 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, … ## $ t2_jobsat5_likert5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6,… Inspect the output of the glimpse() command and make sure you have converted all categorical variables to factors - especially those you will use as predictors. Note: If you have factors like sex that have numeric data in the column (e.g, 1 and 2) instead of male/female you need to handle the situation differently. The preceding section, Experiment: Within N-way, illustrates how to handle this scenario. 20.9.2 Factor screening Inspect the levels of each factor carefully. Make sure the factor levels of each variable are correct. Examine spelling and look for additional unwanted levels. For example, you wouldn’t want to have the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for erroneous factor levels. The code below displays the factor levels: analytic_data_occasions %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 male :147 blue : 99 ## 2 : 1 female :149 brown: 98 ## 3 : 1 intersex: 2 hazel:100 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex variable because we want the x-axis of a future graph to display columns in the left to right order: female, male. analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(sex = fct_relevel(sex, &quot;intersex&quot;, &quot;female&quot;, &quot;male&quot;)) For eye color, we want a future graph to have the most common eye colors on the left so we reorder the factor levels: analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(eye_color = fct_infreq(eye_color)) You can see the new order of the factor levels with summary(): analytic_data_occasions %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 intersex: 2 hazel:100 ## 2 : 1 female :149 blue : 99 ## 3 : 1 male :147 brown: 98 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 20.9.3 Numeric screening For numeric variables, it’s important to find and remove impossible values. For example, in the context of this example you want to ensure none of the Likert responses are impossible (e.g., outside the 1- to 5-point rating scale) or clearly data entry errors. Because we have several numeric columns that we are screening, we use the skim() command from the skimr package. The skim() command quickly provides basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 are omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. Start by examining the range of non-scale items. In this case it’s only age. Examine the output to see if any of the age values are unreasonable. As noted, p0 and p100 in the output indicate the 0th percentile and the 100th percentile; that is the minimum and maximum values for the variable. Check to make sure none of the age values are unreasonably low or high. If they are, you may need to check the original data source or replace them with missing values. library(skimr) analytic_data_occasions %&gt;% select(age) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17 20 24 With respect to the multi-item scales, it makes sense to look at sets of items rather than all of the items at once. This is because sometimes items from different scales use different response ranges. For example, one measure might use a response scale with a range from 1 to 5; whereas another measure might use a response scale with a range from 1 to 7. This is undesirable from a psychometric point of view, as discussed previously, but if it happens in your data - look at the scale items separately to make it easy to see out of range values. We begin by looking at the items in the first scale, self-esteem. Possible item responses for this scale range from 1 to 5. Make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. Because we want to select the self-esteem items from both time 1 and time 2 we cannot use the starts_with() command. Instead we use the contains() command in the code below. analytic_data_occasions %&gt;% select(contains(&quot;esteem&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 t1_esteem1_likert5 24 3.39 0.54 3 3 5 ## 2 t1_esteem2_likert5 28 2.35 0.48 2 2 3 ## 3 t1_esteem3_likert5 31 3.96 0.37 3 4 5 ## 4 t1_esteem4_likert5 15 3.54 0.50 3 4 4 ## 5 t1_esteem5_likert5rev 35 2.22 0.47 1 2 3 ## 6 t2_esteem1_likert5 5 4.27 0.64 3 4 6 ## 7 t2_esteem2_likert5 5 3.33 0.47 3 3 4 ## 8 t2_esteem3_likert5 6 4.77 0.69 3 5 6 ## 9 t2_esteem4_likert5 3 4.46 0.59 3 5 5 ## 10 t2_esteem5_likert5rev 4 3.19 0.45 2 3 4 Follow the same process for the job satisfaction items. Possible item responses for the job satisfaction scale range from 1 to 5, make sure all responses are in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_occasions %&gt;% select(contains(&quot;jobsat&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 t1_jobsat1_likert5 25 3.34 0.51 3 3 5 ## 2 t1_jobsat2_likert5rev 27 1.51 0.61 1 1 3 ## 3 t1_jobsat3_likert5 28 2.84 0.37 2 3 3 ## 4 t1_jobsat4_likert5 35 4.29 0.70 3 4 5 ## 5 t1_jobsat5_likert5 24 4.57 0.61 3 5 5 ## 6 t2_jobsat1_likert5 2 4.23 0.62 3 4 6 ## 7 t2_jobsat2_likert5rev5 3 2.54 0.59 2 2 4 ## 8 t2_jobsat3_likert5 5 3.76 0.43 3 4 4 ## 9 t2_jobsat4_likert5 3 5.03 0.99 3 5 6 ## 10 t2_jobsat5_likert5 3 5.36 0.92 3 6 6 20.9.4 Scale scores For each person, scale scores involve averaging scores from several items to create an overall score. The first step in the creation of scales is correcting the values of any reverse-keyed items. 20.9.4.1 Reverse-key items The way you deal with reverse-keyed items depends on how you scored them. Imagine you had a 5-point scale. You could have scored the scale with the values 1, 2, 3, 4, and 5. Alternatively, you could have scored the scale with the values 0, 1, 2, 3, and 4. The mathematical approach you use to correcting reverse-keyed items depends upon whether the scale starts with 1 or 0. In this example, we scored the Likert items using the values 1 to 5. Therefore, we use the reverse keying approach for scales that being with 1. The preceding section, “Surveys: Single occasion”, describes how the math differs when the response scale starts with 0. We encourage you to read that section before going further if you have not done so already. In this data file all the reverse-keyed items were identified with the suffix “_likert5rev” in the column names. This suffix indicates the item was reverse keyed and that the original scale used the response points 1 to 5. We can see those items with the glimpse() command below. Notice that there are two reverse-keyed items - each on difference scales. analytic_data_survey %&gt;% select(ends_with(&quot;_likert5rev&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 0 To correct reverse-keyed items where the lowest possible rating is 1 (i.e, 1 on a 1 to 5 scale), we simply subtract all the scores from a value one more than the highest possible rating (i.e., 6). The code below: selects columns that end with “_likert5rev” (i.e., both esteem and jobsat scales) subtracts the values in those columns from 6 renames the columns by removing “_likert5rev” from the name because the reverse coding is complete analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(6 - across(.cols = ends_with(&quot;_likert5rev&quot;)) ) %&gt;% rename_with(.fn = str_replace, .cols = ends_with(&quot;_likert5rev&quot;), pattern = &quot;_likert5rev&quot;, replacement = &quot;_likert5&quot;) You can use the glimpse() command to see the result of your work. If you compare these new values to those obtained from the previous glimpse() command you can see they have changed. Also notice the column names no longer indicate the items are reverse keyed. 20.9.4.2 Creating scores The process we use for creating scale scores deletes item-level data from analytic_data_survey. This is a desirable aspect of the process because it removes information we no longer need. That said, before we create scale scores, we create a backup on the item-level data in analytic_data_survey called analytic_data_survey_items. We will need to use this backup later to compute the reliability of the scales we are creating. analytic_data_occasions_items &lt;- analytic_data_occasions We want to make a self_esteem scale and plan to select items using starts_with(“t1_esteem”). Prior to doing this we make sure the start_with() command only gives us the items we want - and not additional unwanted items. The output below confirms there are not problems associated with using starts_with(“t1_esteem”). analytic_data_occasions %&gt;% select(starts_with(&quot;t1_esteem&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ t1_esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3… ## $ t1_esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2… ## $ t1_esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, … ## $ t1_esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, N… ## $ t1_esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4,… Repeat this set of commands using t2_esteem, t1_jobsat, and t2_jobsat. Make sure that those start_with() terms select only the relevant items and not others. We calculate the scale scores using the rowwise() command. The mean() command provides the mean of columns by default - not people. We use the rowwise() command in the code below to make the mean() command work across columns (within participants) rather than within columns. The mutate command calculates the scale score for each person. The c_across() command combined with the starts_with() command ensures the items we want averaged together are the items that are averaged together. Notice that there is a separate mutate line for each scale. The ungroup() command turns off the rowwise() command. We end the code block by removing the item-level data from the data set. Important: Take note of how the names of the scale variables (e.g., esteem_t1, jobsat_t1) use a slightly different convention than our items. That is, these scale labels were picked so that they would not be selected by a starts_with(“t1_esteem”) or starts_with(“t1_jobsat”). Why - because we later use those commands to remove the item-level data. We would want the command designed to remove the item-level data to also remove the scale we just calculated! This example illustrates how carefully you need to think about your naming conventions. analytic_data_occasions &lt;- analytic_data_occasions %&gt;% rowwise() %&gt;% mutate(esteem_t1 = mean(c_across(starts_with(&quot;t1_esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(esteem_t2 = mean(c_across(starts_with(&quot;t2_esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(jobsat_t1 = mean(c_across(starts_with(&quot;t1_jobsat&quot;)), na.rm = TRUE)) %&gt;% mutate(jobsat_t2 = mean(c_across(starts_with(&quot;t2_jobsat&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;t1_esteem&quot;)) %&gt;% select(-starts_with(&quot;t2_esteem&quot;)) %&gt;% select(-starts_with(&quot;t1_jobsat&quot;)) %&gt;% select(-starts_with(&quot;t2_jobsat&quot;)) We can see our data now has the columns t1_esteem, t2_esteem, t1_jobsat, and t2_jobsat. As well, we can see that all of the item-level data has been removed from the data set. glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 8 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, … ## $ sex &lt;fct&gt; male, female, male, female, male, female… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blu… ## $ esteem_t1 &lt;dbl&gt; 3.200, 3.800, 3.800, 3.000, 3.400, 3.500… ## $ esteem_t2 &lt;dbl&gt; 3.8, 4.4, 4.4, 3.6, 4.0, 4.2, 3.6, 4.4, … ## $ jobsat_t1 &lt;dbl&gt; 4.00, 5.00, 4.40, 3.50, 4.00, 3.80, 3.60… ## $ jobsat_t2 &lt;dbl&gt; 3.80, 4.00, 4.60, 4.20, 3.75, 4.00, 4.20… 20.9.5 Pivot to tidy data The analytic data in its current form does not conform to the tidy data specification. Inspect the data with the print() command. Notice that a single column is not used to represent esteem, jobsat, or time. Rather, there are four columns that are a mix of these variables. The consequence of this is that there are two esteem ratings/observations on each row and two jobsat ratings/observations on each row. Tidy data is structured so that each variable is represented in a single column and each observation has its own row. print(analytic_data_occasions) ## # A tibble: 300 × 8 ## id age sex eye_color esteem_t1 esteem_t2 jobsat_t1 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 23 male blue 3.2 3.8 4 ## 2 2 22 fema… brown 3.8 4.4 5 ## 3 3 18 male hazel 3.8 4.4 4.4 ## 4 4 23 fema… blue 3 3.6 3.5 ## 5 5 22 male &lt;NA&gt; 3.4 4 4 ## 6 6 17 fema… hazel 3.5 4.2 3.8 ## 7 7 23 male blue 3 3.6 3.6 ## 8 8 22 fema… brown 3.8 4.4 4.6 ## 9 9 17 male hazel 3.6 4.2 3.75 ## 10 10 NA fema… blue 3.6 4.2 3.8 ## # ℹ 290 more rows ## # ℹ 1 more variable: jobsat_t2 &lt;dbl&gt; The pivot_longer() command below coverts our data to the tidy data format. In this command we specify the columns with data by using esteem_t1:jobsat_t2; this selects these two columns and all of the columns between them. Each of these columns represents a dependent variable at a particular time in the format “variable_time” (e.g., esteem_t1). The code names_to = c(“.value”, “time”) explains this format to R. It indicates that the first part of the column name (e.g., esteem) contains the name of the variable (expressed in the code as “.value”). It also indicates that the second part of the column name represents time. The line names_sep = “_” tells the R that the underscore character is used to separate the first part of the name from the second part of the name. When this code is executed it creates a tidy version of data set stored in analytic_survey_tidy. analytic_occasion_tidy &lt;- analytic_data_occasions %&gt;% pivot_longer(esteem_t1:jobsat_t2, names_to = c(&quot;.value&quot;, &quot;time&quot;), names_sep = &quot;_&quot;) You can see the new data with the print() command. Notice that each participant has multiple rows associated with them. print(analytic_occasion_tidy) ## # A tibble: 600 × 7 ## id age sex eye_color time esteem jobsat ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 23 male blue t1 3.2 4 ## 2 1 23 male blue t2 3.8 3.8 ## 3 2 22 female brown t1 3.8 5 ## 4 2 22 female brown t2 4.4 4 ## 5 3 18 male hazel t1 3.8 4.4 ## 6 3 18 male hazel t2 4.4 4.6 ## 7 4 23 female blue t1 3 3.5 ## 8 4 23 female blue t2 3.6 4.2 ## 9 5 22 male &lt;NA&gt; t1 3.4 4 ## 10 5 22 male &lt;NA&gt; t2 4 3.75 ## # ℹ 590 more rows You now have three data sets The data analytic_occasion_tidy is appropriate for conducting a repeated measures ANOVA or more complicated analyses. The data analytic_data_occasions is appropriate for calculating descriptive statistics and correlations. The data analytic_occasions_items is appropriate for calculating the reliability of the scales you constructed. These data are ready for analysis. 20.10 Basic descriptive statistics Regardless of the design of the study, most researchers want to see descriptive statistics for the variables in their study. We offer three approaches for obtaining descriptive statistics below. For convenience we use the recent data set analytic_data_occasions. But recognize the commands below can be used with all the analytic data sets we created for the various designs. 20.10.1 skim() One approach is the skim() command from the skimr package. The skim() command quickly provides the basic descriptive statistics. In the output for this command there are also several columns that begin with p: p0, p25, p50, p75, and p100 (p25 and p75 are omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The minimum and maximum values for the data column are indicated under the p0 and p100 labels. The median is the 50th percentile (p50). The interquartile range is the range between p25 and p75. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(skimr) skim(analytic_data_occasions) ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17.0 20.0 24.00 ## 2 esteem_t1 0 3.40 0.32 2.5 3.4 4.25 ## 3 esteem_t2 0 3.93 0.34 3.2 4.0 4.80 ## 4 jobsat_t1 0 3.91 0.43 2.0 4.0 5.00 ## 5 jobsat_t2 0 4.18 0.34 3.0 4.2 4.80 20.10.2 apa.cor.table() Another approach is the apa.cor.table() command from the apaTables package. This quickly provides the basic descriptive statistics as well as correlations among variable. As well, it will even create a Word document with this information, see Figure 20.3. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(apaTables) analytic_data_survey %&gt;% select(where(is.numeric)) %&gt;% apa.cor.table(filename = &quot;apa_descriptives.doc&quot;) FIGURE 20.3: Word document created by apa.cor.table 20.10.3 tidyverse A final approach uses tidyverse commands. This approach is oddly long - and we won’t describe how it works in detail. But, based on the information in the previous chapter you should be able to work out how this code works. Even though this code is long - it provide the ultimate in flexibility. If a new statistic is developed that you want to use, you can simply include the command for it in the desired_descriptives list and it will be included in your table. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(tidyverse) # HMisc package must be installed. # Library command not needed for HMisc package. desired_descriptives &lt;- list( mean = ~mean(.x, na.rm = TRUE), CI95_LL = ~Hmisc::smean.cl.normal(.x)[2], CI95_UL = ~Hmisc::smean.cl.normal(.x)[3], sd = ~sd(.x, na.rm = TRUE), min = ~min(.x, na.rm = TRUE), max = ~max(.x, na.rm = TRUE), n = ~sum(!is.na(.x)) ) row_sum &lt;- analytic_data_occasions %&gt;% summarise(across(.cols = where(is.numeric), .fns = desired_descriptives, .names = &quot;{col}___{fn}&quot;)) long_summary &lt;- row_sum %&gt;% pivot_longer(cols = everything(), names_to = c(&quot;var&quot;, &quot;stat&quot;), names_sep = c(&quot;___&quot;), values_to = &quot;value&quot;) summary_table &lt;- long_summary %&gt;% pivot_wider(names_from = stat, values_from = value) # round to 3 decimals summary_table_rounded &lt;- summary_table %&gt;% mutate(across(.cols = where(is.numeric), .fns= round, digits = 3)) %&gt;% as.data.frame() print(summary_table_rounded) ## var mean CI95_LL CI95_UL sd min max n ## 1 age 20.522 20.288 20.756 2.048 17.0 24.00 297 ## 2 esteem_t1 3.403 3.366 3.440 0.324 2.5 4.25 300 ## 3 esteem_t2 3.927 3.889 3.966 0.337 3.2 4.80 300 ## 4 jobsat_t1 3.905 3.856 3.955 0.435 2.0 5.00 300 ## 5 jobsat_t2 4.184 4.146 4.223 0.338 3.0 4.80 300 20.10.4 Cronbach’s alpha If you want Cronbach’s alpha to estimate the reliability of the scale, you can use the alpha command from the psych package with the code below. Note we have to use the item-level data we previously created a copy of called analytic_data_survey_items. The glimpse() command illustrates this data set has all the original items (after reverse-key coding has been fixed). analytic_data_survey_items %&gt;% glimpse() ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17… ## $ sex &lt;fct&gt; male, female, male, female, male, … ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, haze… ## $ esteem1_likert5 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4… ## $ esteem2_likert5 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2… ## $ esteem3_likert5 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, … ## $ esteem4_likert5 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, … ## $ esteem5_likert5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3,… ## $ jobsat1_likert5 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4… ## $ jobsat2_likert5 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, … ## $ jobsat3_likert5 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ jobsat4_likert5 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA… ## $ jobsat5_likert5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, … We calculated reliability using psych::alpha() command. Cronbach’s alpha is labeled “raw alpha” in the output. Cronbach’s alpha is an estimate of the proportion of variability in observed scores that is due to actual differences among participants (rather than measurement error). Remember, never use library(psych), it will break the tidyverse packages. Instead, precede all psych package commands with psych:: as we do below with psych::alpha(). rxx_alpha &lt;- analytic_data_occasions_items %&gt;% select(starts_with(&quot;t1_esteem&quot;)) %&gt;% psych::alpha() print(rxx_alpha$total) ## raw_alpha std.alpha G6(smc) average_r S/N ase mean ## 0.6622 0.6634 0.6173 0.2827 1.97 0.03035 3.403 ## sd median_r ## 0.3239 0.2927 References "],["deep-dive-sampling-and-anova.html", "Chapter 21 Deep dive: Sampling and ANOVA 21.1 Equal population means 21.2 A tale of two formulas 21.3 A student population 21.4 Random sampling process 21.5 Population: What’s in a name 21.6 Sample mean distribution variance 21.7 Estimating sample mean distribution variance 21.8 Frame of reference: Random sampling only 21.9 Central \\(F\\)-distributions 21.10 Summary table 21.11 Walk away points", " Chapter 21 Deep dive: Sampling and ANOVA 21.1 Equal population means In this chapter we build from a one population scenario to a three population scenario where the three populations have the same mean. 21.2 A tale of two formulas In this chapter, we focus on describing the distribution of sample means. Our goal is to understand the logic behind two different formulas for estimating the variance of distribution of sample means. A thorough understanding of both of these two estimation approaches lays the foundation for understanding Analysis of Variance (ANOVA). Approach Estimation Formula Goal Approach 1: Total \\(s_{\\bar{x}}^2=\\frac{\\sum (\\bar{x}-\\bar{\\bar{x}})^2}{a-1}\\) Estimate the variance of the sampling distribution. That is, estimate variance of sample mean due to all sources (population differences, random sampling error, etc.). Approach 2: CLT \\(s_{\\bar{x}}^2=\\frac{s_{people}^2}{n}\\) Estimate the variance in sample mean due ONLY to random sampling error. Understanding the logic behind both approaches for estimating the variance of the distribution of sample means is critical. In this chapter, where there is one population of people, the two approaches estimate the same thing - variabilty in sample means due to random sampling. However, in future chapters, where there are multiple populations of people with different means, the two approaches estimate different things. Consequently, to prepare you for those more complex future chapters, it’s important to understand the logic behind both approaches in the simple case - where there is a single population of people. What we learn in this chapter (and the pattern of results we obtain) will serve as frame-of-reference for interpreting more complex situations in future chapters. 21.3 A student population Consider a scenario where there is a large university with 100,000 students and we have the heights of all 100,000 students. We think of this as a population of student heights. The mean height of all 100,000 students is 172.50 cm and the variance is 156.3236 cm\\(^2\\). The heights of all the university students are illustrated in Figure 21.1A. This figure is shaded with stick figures wearing trousers that remind us this distribution represents people. FIGURE 21.1: A distributionof people (A.) and a corresponding distribution of sample means (B.) can both be considered populations in some sense. The mean of a population of 100,000 student heights is calculated using Equation (21.1) below. \\[\\begin{equation} \\mu_{people} = \\frac{\\sum_{i=1}^{N=100000}{X_i}}{N} \\tag{21.1} \\end{equation}\\] The variance of a population of 100,000 student heights is calculated using Equation (21.2) below. \\[\\begin{equation} \\sigma_{people}^2 = \\frac{\\sum_{i=1}^{N=100000}{(X_i - \\mu_{people})^2}}{N} \\tag{21.2} \\end{equation}\\] For these 100,000 people the mean is \\(\\mu_{people}=172.50\\) and the variance is \\(\\sigma_{people}^2=156.3236\\). 21.4 Random sampling process Now imagine that we are interested in studying the random sampling process. Specifically we are interested in examining the variability in sample means that occurs when taking random samples of students heights from the population of people (see Figure 21.1). Indeed, imagine that we obtained a sample of \\(n\\) = 5 students and measured their heights. Then we calculated a sample mean (\\(\\bar{x}\\)) based on those heights. Then we repeated this sampling process an infinite number of times such that we have an infinite number of sample means each based on the heights of 5 students. This set of an infinite number of sample means is conceptually illustrated in Figure 21.1B and we refer to it as the sampling distribution of the mean. Moreover, because we sampled from a single population, whose variance is 156.3236 (i.e., \\(\\sigma_{people}^2 =156.3236\\)), the variability in sample means, when each sample size is 5 (\\(n=5\\)), is due to the random sampling error only. 21.5 Population: What’s in a name Typically, in psychology, when we use the word “population” we are referring to a population of people with respect to some dependent variable - such as the heights illustrated in Figure 21.1A. However, in statistics, a population is simply a set of elements (often people) about which we wish to draw conclusions. In this chapter, we shift our focus from thinking of the distribution of people in Figure 21.1A as a population to also thinking of the distribution of sample means in Figure 21.1B as a population. Doing so may seem counter intuitive but it the logical foundation for Analysis of Variance (ANOVA). 21.5.1 Sample means in context Thinking of the distribution of sample means as a population can be a bit tricky - since there are two populations that we could be referring to when we use the word “population”. Notation helps to keep things straight. In this book, when we are talking about a distribution of people (see Figure 21.1A), we use \\(N\\) to indicate the number people in the population and \\(n\\) to refer to the number of people in a subset of the population (what we typically refer to as a sample). Likewise, when we are talking about a distribution of sample means (see Figure 21.1B), we use \\(K\\) to indicate the number of sample means in the population. Consider a concrete example. When we obtain a sample of five people’s heights (\\(n\\) = 5) we can think of the people in the sample as a subset of the population of people (\\(N\\)=100,000), see Figure 21.1A. But we can also think of that single sample mean as mean a subset of an infinity large set of sample means, see Figure 21.1B. We use (\\(K = \\infty\\)) to refer to the total number of sample mean in the population of sample means. When we have one sample mean from this population of same mean we say \\(a = 1\\). If we had three sample means from this population of sample means, 21.1B, we would say \\(a = 3\\). 21.5.2 Distribution of sample means as a population Because we are thinking of the distribution of sample means as a population we can calculate the mean and variance for the distribution of sample means. The mean of the population of sample means is calculated as below in Equation (21.3). Notice that we used the symbol, \\(\\mu_{\\bar{x}}\\), to represented this value. Further notice, the little \\(\\bar{x}\\) beside the \\(\\mu\\) in \\(\\mu_{\\bar{x}}\\) which indicates it is sample means that are being averaged. As noted previously, we use \\(K\\) instead of \\(N\\) to represent the number of sample means (where \\(K\\) = infinity, because there are an infinite number of sample means in the distribution)1. Notice the similarity between the formula for the mean of a distribution of people, Equation (21.1) above, and the formula for the mean of a distribution of sample means, Equation (21.3). \\[\\begin{equation} \\mu_{\\bar{x}} = \\frac{\\sum_{i=1}^{K=\\infty}{\\bar{x}_i}}{K} \\tag{21.3} \\end{equation}\\] Using Equation (21.3) we find that the mean of the sample means is 172.50 cm (i.e., \\(\\mu_{\\bar{x}} = 172.50\\)). A key point to remember is that the mean of the sample means, \\(\\mu_{\\bar{x}}\\), will always be the same as the mean of people’s heights, \\(\\mu_{people}\\). Consequently, any conclusions we make the mean of sample means, \\(\\mu_{\\bar{x}}\\) see Figure 21.1B, will also apply to the mean of people, \\(\\mu_{people}\\) see Figure 21.1A. This is a crucial point to keep in mind as we proceed through this chapter. Although we are ultimately interested in making conclusion about the mean of the people (i.e., \\(\\mu_{people}\\)) - all our work in this chapter (and in ANOVA) is based on the mean of the distribution of sample means in Figure 21.1B (i.e., \\(\\mu_{\\bar{x}}\\)). 21.6 Sample mean distribution variance When there is one population of people (see Figure 21.1A) there is a single distribution of sample means that corresponds to that population (see Figure 21.1B). In this scenario, the sample means differ only because of random sampling. There are two approaches for calculating the variance of this distribution of sample means. 21.6.1 Approach 1: Total: Calculate the variance of infinite sample means In this first method, we use an infinite number of sample means and simply calculate the variance, see Equation (21.4) below. \\[\\begin{equation} \\sigma_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{K=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{K} \\tag{21.4} \\end{equation}\\] Notice how Equation (21.4) for the variance of sample means in Figure 21.1B, is similar to the formula we use for the variance of people, Equation (21.2), in Figure 21.1A. The formula for the variance of sample means, Equation (21.4), differs because we are using sample means (\\(\\bar{x}\\)) instead of attributes of people (\\(X\\)). As well, we use \\(K\\) to indicate the number of elements used (i.e., an infinite number of sample means) in the calculation rather than \\(N\\). Distribution Attribute Parameter Calculation Estimate of Parameter People Variance \\(\\sigma_{people}^2 = \\frac{\\sum_{i=1}^{N}{(X_i - \\mu_{people})^2}}{N}\\) \\(s_{people}^2 = VAR = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x}_{people})^2}}{n-1}\\) Sample Means Approach 1: Variance \\(\\sigma_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{K=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{K}\\) \\(s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}\\) Obviously, using Equation (21.4) to calculate the variance of an infinite set of sample means is an impractical solution because we could never obtain the infinite number of samples and then calculate the variance for this set. Conceptually, however, it’s good to keep this calculation method in mind - since it reminds us of principles we’ve already learned about the variance of populations 2 But imagine if we could obtain the full infinite set of sample means - then we could use the formula as below: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sum_{i=1}^{K=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{K}\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] Thus, the “population” variance of the distribution of sample means (\\(\\sigma_{\\bar{x}}^2\\), see Figure 21.1B) is calculated, based an infinite number of sample means (\\(K = \\infty\\)) using Approach 1, to be \\(\\sigma_{\\bar{x}}^2 = 31.26472\\). When there is only one population of people this formula will reflect variability in sample means due to random sampling. When there is more than one population of people - it becomes more complicated. We’ll discuss that situation in a future chapter. 21.6.2 Approach 2: CLT: Calculation of variance due to sampling error Fortunately, when sampling error is the ONLY source of variability in sample means there is a shortcut. We don’t need to obtain an infinite number of sample means to determine the variance of the distribution of an infinite number of sample means, Figure 21.1B. There is a “shortcut formula” we can use based on the Central Limit Theorem (CLT), see the formula below. \\[ \\sigma_{\\bar{x}}^2 = \\frac{\\sigma_{people}^2}{\\text{sample size}} \\] Be we more often see it represented completely symbolically as per Equation (21.5) below. \\[\\begin{equation} \\sigma_{\\bar{x}}^2 = \\frac{\\sigma_{people}^2}{n}\\\\ \\tag{21.5} \\end{equation}\\] An inspection of Equation (21.5) reveals the variance of the distribution of sample means DUE TO RANDOM SAMPLING ERROR ONLY(see Figure 21.1B) depends on both the number of people in each sample (\\(n\\)) and the variability in the people’s heights in the population (see Figure 21.1B, \\(\\sigma_{people}^2\\)). The reason this formula works should not be obvious - at all. If you are looking at this formula trying to figure out why it works - I suggest you stop and just accept that it does work. If you want to understand why it works, I suggest you consult (W. L. Hays 1994) who provides a statistical proof for the Central Limit Theorem. It’s helpful, moving forward, to think of this formula (\\(\\frac{\\sigma_{people}^2}{n}\\)) as representing the variability in means due to random sampling. The formula was devired from teh Central Limit Theorem so we refer to it as the CLT approach. No matter the situation - this formula will always reflect the variability in sample means due ONLY to random sampling. Previously, we learned that the variance of the distribution of people in Figure 21.1A is \\(\\sigma_{people}^2=156.3236\\). Consequently, we can use Equation (21.5) to calculate the variance of the distribution of sample means in Figure 21.1B: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ &amp;= \\frac{156.3236}{5}\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] Thus, the “population” variance of the distribution of sample means (\\(\\sigma_{\\bar{x}}^2\\), see Figure 21.1B) is calculated, using Approach 2 the CLT shortcut, to be \\(\\sigma_{\\bar{x}}^2 = 31.26472\\). 21.6.3 Ratio comparison of two approaches To begin, we are looking at a distribution of sampling means where the means differed only due to random sampling error. In this situation, we have calculated the variance of the distribution of sample means using two different approaches. Approach 1: Total variance of sample means calculated using sample means and the variance formula Approach 2: Variance of sample means, due to random sampling error, calculated using CLT shortcut formula Although we can just look at the numbers resulting from these two approaches and compare them, that’s not how statisticians typically handle such comparisons. The approach used by statisticians to compare two numbers is to create a ratio. That is, take one number and divide it by the other number. If the two numbers are the same the ratio will be 1.00. If the ratio is larger than 1.00 it means the number on the top was bigger than the number on the bottom. \\[ \\begin{aligned} \\text{ratio comparing approaches} &amp;= \\frac{\\text{total variance of sample means }}{\\text{variance of sample means due to random sampling error}}\\\\ &amp;=\\frac{\\frac{\\sum_{i=1}^{A=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{A}}{\\frac{\\sigma_{people}^2}{n}}\\\\ &amp;=\\frac{31.26472}{31.26472}\\\\ &amp;= 1.00\\\\ \\end{aligned} \\] You can see the ratio comparing these two approaches below is 1.00 - so we conclude two approaches produced the same result. If the number on the top of the ratio was the larger of the two, then the value of the ratio would be greater than 1.00. In contrast, if the number on the top of the ratio was the smaller of the two, then the value of the ratio would be less than 1.00. Be sure you understand how to interpet the value of a ratio using these rules before continuing. CRITICAL: Think about what this means. Both approaches produced the same number. The total variance approach resulted in the same number as the random samplingly only approach. So we can conclude the only reason the sample means differed from each other was random sampling. Go back and re-read this paragraph a few times until you are sure you understand this crucial point. CALCULATION VS ESTIMATION. In this section, we focused on actually calculating the variance of the distribution of sample means using two approaches. But to actually calculate this variance we needed an infinite number of sample means (Approach 1) or knowledge of the population of people variance (Approach 2). Consequently, it’s impractical to calculate either quantity. In the next section, below, we recognize that we can likely never know or calculate the variance of the distribution of sample means. Instead, we focus on estimating the variance of the distribution of sample means. An estimated value is an approximation of the actual value that will differ due to the effects of random sampling. 21.7 Estimating sample mean distribution variance In this section we focus on estimating the variance of the distribution of sample means using sample data. Imagine that we obtained three samples (i.e., \\(a = 3\\)) and each sample has 5 people (i.e., \\(n = 5\\)) from the population of people illustrated in Figure 21.1A. The statistics for those three samples are presented below. Sample Number Sample size (\\(n\\)) Mean (\\(\\bar{x} = \\frac{\\Sigma x_i}{n}\\)) Variance (\\(s_{people}^2=\\frac{\\Sigma (x_i - \\bar{x})^2}{n-1}\\)) 1 5 185 168 2 5 169.2 146.7 3 5 170 346 21.7.1 Approach 1: Estimate the total variance of infinite sample means using 3 sample means We can use the sample means presented in the table above to estimate the variance of the distribution of sample means as illustrated in the formula below. Recall, we use a lower case character, \\(a\\), to represent the number of sample means (in the case because we have 3 sample means \\(a = 3\\)). We think of these three sample means as a subset (\\(a = 3\\)) of the distribution of an infinite number of sample means (i.e., a population of sample means, \\(K=\\infty\\)) illustrated in Figure 21.1B. Consequently, we use the standard approach of estimating the variance of a larger set of elements from a (i.e., a population of sample means) subset of those elements (i.e., a sample of sample means) by including “-1” in the denominator of the variance formula, see Equation (21.6) below. \\[\\begin{equation} s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1} \\tag{21.6} \\end{equation}\\] First, we calculate the mean of the three sample means (\\(\\bar{\\bar{x}}\\)) because we will need that in the variance formula. \\[ \\begin{aligned} \\bar{\\bar{x}} &amp;= \\frac{\\sum_{i=1}^{a=3}{\\bar{x}_i}}{a}\\\\ &amp;= \\frac{\\bar{x}_1+\\bar{x}_2+\\bar{x}_3}{a}\\\\ &amp;= \\frac{185 + 169.2 + 170 }{3}\\\\ &amp;= 174.733 \\end{aligned} \\] Second, we estimate the variance of the population of sample means (\\(s_{\\bar{x}}^2\\), see Figure 21.1B) using the set of sample means. Again, notice the use of “-1” in the denominator - which we discussed previously in the sample accuracy chapter. The use of the “-1” in the denominator means we are not calculating the variance of these three sample means. Rather, when we use “-1” in the denominator we are estimating the variance of the population of sample means from which they were obtained (i.e., the distribution of sample means depicted in Figure 21.1B). \\[ \\begin{aligned} s_{\\bar{x}}^2 &amp;= \\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}\\\\ &amp;= \\frac{(\\bar{x_1}- \\bar{\\bar{x}})^2+(\\bar{x_2}- \\bar{\\bar{x}})^2+(\\bar{x_3}- \\bar{\\bar{x}})^2}{a-1}\\\\ &amp;= \\frac{(185 - 174.733)^2+(169.2 - 174.733)^2+(170 - 174.733)^2}{3-1}\\\\ &amp;= 79.213 \\end{aligned} \\] Thus, the “population” variance of the distribution of sample means (\\(\\sigma_{\\bar{x}}^2\\), see Figure 21.1B) is estimated, based on three sample means (\\(a = 3\\)) using Approach 1, to be \\(s_{\\bar{x}}^2 = 79.213\\). Remember though 79.213 is just an estimate of the population value (\\(\\sigma_{\\bar{x}}^2\\)) based on three sample means. IMPORTANT: The denominator of this variance calculation is \\(2\\) (i.e., \\(3-1\\)). Therefore, we say there are 2 degrees of freedom associated with this estimate of the variance of the infinite number of sample means. 21.7.2 Approach 2: CLT shortcut: Estimation of variance due to random sampling error We can use the information presented in the table above in another way to estimate the variance of the distribution of sample means. Indeed, recall that than another approach to calculating the variance of the distribution of sample means is to use the shortcut formula based on the Central Limit Theorem (CLT), Equation (21.5) above, repeated below for clarity. \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ \\end{aligned} \\] 21.7.2.1 Estimating \\(\\sigma_{people}^2\\) with \\(s_{people}^2\\) The problem with the formula above is that you need to know the variance of the all the people in the population (\\(\\sigma_{people}^2\\)) - and we don’t know the variance of all the people in the population. But, we do have three samples from that population of people. Because the three samples are from the same population of people - we know each sample provides an estimate of the same (single) variance of people in the population. Therefore, we can combine those variance estimates into a better (or average) estimate of the variance of the people (\\(s_{people}^2\\)) in the population using the formula below. \\[ \\begin{aligned} s_{people}^2 &amp;= MSE_{people}\\\\ &amp;= \\frac{\\sum_{i=1}^{a=3}(n_i-1)s_i^2}{\\sum_{i=1}^{a}(n_i-1)} \\\\ &amp;= \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + (n_3-1)s_3^2}{(n_1-1)+(n_2-1)+(n_3-1)} \\\\ &amp;= \\frac{(5-1)s_1^2 + (5-1)s_2^2 + (5-1)s_3^2}{(5-1)+(5-1)+(5-1)} \\\\ &amp;= \\frac{(5-1)168 + (5-1)146.7 +(5-1)346}{(5-1) + (5-1) + (5-1)} \\\\ &amp;= \\frac{672 + 586.8 + 1384}{15 -3} \\\\ &amp;= \\frac{2642.8}{12} \\\\ &amp;= 220.2333 \\\\ \\end{aligned} \\] IMPORTANT: The denominator of this variance calculation is \\(12\\) (i.e., \\(a(n-1)=3(5-1)=3(4)=12\\)). Therefore, we say there are 12 degrees of freedom associated with this estimate of the variance of the people in the population. When the three sample sizes are the same the above formula just boils down to an average: \\[ \\begin{aligned} s_{people}^2 &amp;= MSE_{people}\\\\ &amp;= \\frac{s_1^2 + s_2^2 + s_3^2}{3} \\\\ &amp;= \\frac{168 + 146.7 + 346 }{3} \\\\ &amp;= \\frac{660.7}{3} \\\\ &amp;= 220.2333 \\\\ \\end{aligned} \\] The number we just calculated, \\(s_{people}^2 =220.2333\\), is an estimate of the variance of the population of people illustrated in Figure 21.1A. You could think of this number as an average variance estimate. We created the average variance estimate,\\(s_{people}^2 =220.2333\\), by combining three sample estimates of the variance of the population of people. Each sample estimate was based on 5 people (i.e., 4 degrees of freedom). But the average variance estimate we just calculated ( \\(s_{people}^2 =220.2333\\)) is based on 15 people - people from all three samples (i.e., 12 degrees of freedom). Because we combined people from all three samples the average variance estimate is a better estimate of the variance of the population of people than any single sample estimate. In technical terms, we say the variance estimate we just calculated is a better estimate because of the higher degrees of freedom (12) compared to an individual sample estimate (with only 4 degrees of freedom). We use different names to refer to the resulting average variance estimate - we could just call it a better estimate of the population variance (\\(s_{people}^2\\)). But, statisticians also use the term pooled variance (\\(s_{pooled}^2\\)) to refer it. Other times, statisticians use the term Mean Square Error (\\(MSE\\)) to refer to it. You should realize that these are all just synonyms for variance of the population of people illustrated in Figure 21.1A: \\(s_{people}^2 = s_{pooled}^2 = MSE_{people} = MSE\\). 21.7.2.2 Estimating \\(\\sigma_{\\bar{x}}^2\\) with CLT If we assume that random sampling error is the only source of variance in the distribution of sample means we can estimate \\(\\sigma_{\\bar{x}}^2\\) using the formula we derived from the Central Limit Theorem. More specifically, armed with the estimate of the variance of the people in the population (\\(s_{people}^2\\)) we can use the CLT shortcut formula to obtain an estimate of the variance of the distribution of sample means using Equation (21.7) below. \\[\\begin{equation} s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n} \\tag{21.7} \\end{equation}\\] Putting in values: \\[ \\begin{aligned} s_{\\bar{x}}^2 &amp;= \\frac{s_{people}^2}{n}\\\\ &amp;= \\frac{220.2333 }{5}\\\\ &amp;= 44.0467 \\end{aligned} \\] Thus, using Approach 2: CLT shortcut, our estimate of the variance of the distribution of sample means, due to random sampling error is \\(s_{\\bar{x}}^2 = 44.0467\\). IMPORTANT: This estimate of the variance of the distribution of sample means was based on using an estimate of the variance of the people in the population (i.e., \\(s_{people}^2 =220.2333\\)). There were 12 degrees of freedom associated with \\(s_{people}^2\\) which \\(s_{\\bar{X}}^2\\) depends upon. So we say there are 12 degrees of freedom associated with \\(s_{\\bar{X}}^2\\). 21.7.3 Ratio comparison of two approaches The table below summarizes our work so far. Attribute Unknown Parameter Estimate of Parameter \\(s_{\\bar{x}}^2\\) degrees of freedom Approach 1: Total variance \\(\\sigma_{\\bar{x}}^2 =31.26472\\) \\(s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}= 79.213\\) \\(a-1=3-1=2\\) Approach 2: CLT Random sampling variance \\(\\sigma_{\\bar{x}}^2 =31.26472\\) \\(s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n}=44.0467\\) \\(a(n-1)=3(5-1)=12\\) In the table above, Approach 1, estimates the total variance (from all sources) in sample means. In contrast, Approach 2, estimates the variance in sample means due strictly to random sampling error. In this particular scenario we know the only source of variability in sample means is random sampling error. So…we do the two numbers differ? Recall, that in Approach 1 we didn’t have all of the sample means in the infinity large distribution of sample means. We based our estimate on only three of those means. So our estimate using, Approach 1, will differ from the truth due to sampling error – because we only have 3 sample means - not the full infinite set of sample means. Likewise, recall that in Approach 2 we didn’t know the variance of the population of people (\\(\\sigma_{people}^2\\)). We had to rely on an esimate of that value (\\(s_{people}^2\\)), based on 15 people (three sets of 5). Consequently, Approach 2 will differ from the truth as well due to random sampling error. Because both Approach 1 and 2 will differ from the true due to random sampling error, when we construct a ratio, it too will differ from the truth due to random sampling error. That is, when we construct a ratio based on those variance estimate (\\(s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}= 79.213\\) and \\(s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n}=44.0467\\)) the ratio will differ from “the truth” (1.00) due to sampling error. \\[ \\begin{aligned} \\text{ratio comparing approaches} &amp;= \\frac{\\text{variance of sample means ESTIMATED using sample means}}{\\text{variance of sample means ESTIMATED using CLT shortcut formula}}\\\\ &amp;=\\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}}\\\\ &amp;=\\frac{79.213}{44.0467}\\\\ &amp;= 1.7984\\\\ \\end{aligned} \\] 21.8 Frame of reference: Random sampling only In this section we focus on using simulations to establish a frame of reference for interpreting the variance ratio in a scenario where random sampling is the only reason sample means differ from each other. Because random sampling is the only reasons the sample means differ both formulas are estimating the same thing. However, due to random sampling error, the the formulas may differ. 21.8.1 Simulation of 1 population FIGURE 21.2: Sampling from one population The simplest scenario to understand the estimation formulas we have discussed is the one illustrated in Figure 21.2 above. In this scenario, we obtain 3 samples from the population of people and then used that data to estimate the variance of the distribution of sample means using Approach 1 and Approach 2. Then we create a ratio of those two values (Approach 1/Approach 2). Each additional row of the table shows a case where we did this same thing with three new samples. We begin by creating the population in R: library(learnSampling) pop1 &lt;- make_population(mean = 172.50, variance = 156.3236) Then we run a simulation where we estimate the variance using the distribution of sample means using the information in the three sample via Approach 1 and Approach 2. We repeat this estimation process 100,000 times to see if it is accurate in the long run. data_ratios &lt;- get_mean_samples_ratio(pop1, a = 3, n = 5, number.of.trials = 100000) The results of the 100,000 simulations are placed in the data set called data_ratios. We can see the first few rows of these data below. head(data_ratios) ## trial app_1_total_var_est app_2_clt_est var_ratio ## 1 1 38.89 54.17 0.7180 ## 2 2 20.57 18.77 1.0963 ## 3 3 44.41 10.91 4.0696 ## 4 4 64.05 18.19 3.5220 ## 5 5 19.85 25.55 0.7771 ## 6 6 21.33 19.64 1.0862 When you inspect the table above, recognize that for the first row of the table we obtained 3 random samples from the population of people and then used that data to estimate the variance of the sampling distribution of means using Approach 1 and Approach 2 (approach_1_total_var_est and approach_2_clt_est, respectively). Then we created a ratio of those two values (approach_1_var_est/approach_2_clt_est) and placed that in the table (var_ratio). Each additional row of table shows a case were were did this same thing with three new samples. There are a total of 100,000 rows in this table. We obtain the summary statistics with the skim command: library(skimr) skim(data_ratios) Recall that previously, we used Equation (21.5) to calculate \\(\\sigma_{\\bar{x}}^2 = 31.26472\\). Each row in the table above presents two estimates (i.e., \\(s_{\\bar{x}}^2\\)) of \\(\\sigma_{\\bar{x}}^2\\). You can see that the Approach 1 approach for obtaining \\(s_{\\bar{x}}^2\\) was on average correct becase of the 31.3 value in the blue box. Likewise, you can see that the Approach 2 approach for obtaining \\(s_{\\bar{x}}^2\\) was on average correct because of the 31.3 value in the blue box. Graphing the ratios. Although each estimate was on average correct, the two estimates were not always the same. Recall we compared the two estimates by making a ratio. Due to random sampling error on some trials the numerator of the ratio was larger and on other trials the denominator of the ratio was larger. As a result, on some trials the ratio was larger than 1.0 and on other trials the ratio was smaller than 1.0. Below I present a graph of these ratios, Figure 21.3. You can see there is a wide range of ratio that occur. Note that this is shape of the distributions when we have: A single population - so sampling error is the only reason means differ Total variance estimate is based on 3 sample means (i.e., \\(a-1\\) degrees) Sampling error variance estimate created using a population of people variance estimate dervied from 15 people (i.e., \\(a(n-1)=3(5-1)=12\\) degrees of freedom) This type of graph is helpful, because it allows us to understand, when these conditions are true, the range of ratio values that can occur. FYI - as we’ll discuss more later, this is an F-ratio. FIGURE 21.3: Variance ratio when there is one population of people This graph illustrates the range of ratio values that can occur when there is a single population when we use 3 samples, \\(a =3\\), and a sample size of 5, \\(n = 5\\). More formally, there are 2, \\((a -1)\\), and 12, \\(a(n-1)\\), degrees of freedom. 21.8.2 Simulation of 3 identical populations FIGURE 21.4: Sampling from three populations with the same mean (and variance) Population Mean Variance 1 \\(\\mu_{A1_{people}}= 172.50\\) \\(\\sigma_{A1_{people}}^2= 156.3236\\) 2 \\(\\mu_{A2_{people}}= 172.50\\) \\(\\sigma_{A2_{people}}^2= 156.3236\\) 3 \\(\\mu_{A3_{people}}= 172.50\\) \\(\\sigma_{A3_{people}}^2= 156.3236\\) In this section, instead of using one population, we use three identical populations. That is, \\(a = 3\\). We use \\(A1\\), \\(A2\\), and \\(A3\\) to refer to these three identical populations. You will see in this section that when you have three identical populations of people, see Figure 21.4, that the results are the same as when you have one population of people. We simulate the situation illustrated in the figure above. We start by creating three identical populations of people. library(learnSampling) pop1 &lt;- make_population(mean = 172.50, variance = 156.3236) pop2 &lt;- pop1 pop3 &lt;- pop1 Then, we obtain a sample from each population of people and calculate a sample mean. When we use Approach 1: Total variance we think of these three sample means as being a random sample (\\(a = 3\\)) of the infinite distribution of sample means (\\(K = \\infty\\)). The variance of the three sample means is an estimate of the variance of the distribution of sample means. When we use Approach 2: CLT Shortcut we use the people from the three sample to estimate the variance of the population of people. We use our estimate of the variance of the population of people in the Approach 2: CLT Shortcut formula to estimate the variance of the distribution of sample means. Then we calculate the ratio of Approach 1: Total variance divided by Approach 2: CLT Shortcut. Then we repeat this process 100,000 times to see how well this process works on average - via the R commands below. data_ratios3 &lt;- get_mean_samples_ratio(pop1, pop2, pop3, n = 5, number.of.trials = 100000) The results of the 100,000 simulations are placed in the data set called data_ratios3. We can see the first few rows of these data below. head(data_ratios3) ## trial app_1_total_var_est app_2_clt_est var_ratio ## 1 1 117.973 34.20 3.44951 ## 2 2 45.853 44.61 1.02795 ## 3 3 2.573 48.56 0.05299 ## 4 4 8.920 22.07 0.40411 ## 5 5 9.693 35.68 0.27167 ## 6 6 84.853 21.42 3.96141 In the first row of the above table we obtained 3 samples (one from each population) and then used that data to estimate the variance of the sampling distribution of means using Approach 1 and Approach 2. Then we create a ratio of those two values (Approach 1/Approach 2). Each additional row of table shows a case where were did this same thing with three new samples. There are a total of 100,000 rows in this table. We obtain the summary statistics with the skim command: library(skimr) skim(data_ratios3) Recall that previously, we used Equation (21.5) to calculate \\(\\sigma_{\\bar{x}}^2 = 31.26472\\). Each row in table above presents two estimates (i.e., \\(s_{\\bar{x}}^2\\)) of \\(\\sigma_{\\bar{x}}^2\\). You can see that the Approach 1 approach for obtaining \\(s_{\\bar{x}}^2\\) was on average correct becase of the 31.3 value in the green box. Likewise, you can see that the Approach 2 approach for obtaining \\(s_{\\bar{x}}^2\\) was on average correct because of the 31.3 value in the green box. Graphing the ratios. Although each estimate was on average correct, the two estimates were not always the same. Recall we compared the two estimates by making a ratio. Due to random sampling error on some trials the numerator of the ratio was larger and on other trials the denominator of the ratio was larger. As a result, on some trials the ratio was larger than 1.0 and on other trials the ratio was smaller than 1.0. Below I present a graph of these ratios, Figure 21.5. You can see there is a wide range of ratio that occur. Note that this is shape of the distributions when we have: Three identical population - so sampling error is the only reason means differ Total variance estimate is based on 3 sample means (i.e., \\(a-1\\) degrees) Sampling error variance estimate created using a population of people variance estimate dervied from 15 people (i.e., \\(a(n-1)=3(5-1)=12\\) degrees of freedom) This type of graph is helpful, because it allows us to understand, when these conditions are true, the range of ratio values that can occur. Again remember, these are called F-ratios. FIGURE 21.5: Variance ratio when there are three identical populations of people 21.8.2.1 Extreme ratio values Keep in mind that this is the range of ratios that can occur when all three populations have the same mean. In the above graph we use a vertical blue line to distinguish between the lower 95% of ratio values and the upper 5% of ratios values. How did we know where to place this line? We simply sorted the ratios from smallest to largest. Then when to the 95% percentile spot in that list. To obtain the 95% percentile in of the sorted list of 100,000 ratios we just mutiple the total number of ratios (100,000) by .95. We 95000 (i.e., \\(.95*100000=95000\\)) which indicates that 95% of ratios are below the value in this spot. Likewise, 5% of ratios are above the value at this spot. The ratio value in the spot 95,000 of the sorted list of 100,000 ratio values is 3.88. How do you interpret this value? In the situation where: the variance estimate the numerator has 2 degrees of freedom the variance estimate the denominator has 12 degrees of freedom the population variances are the same the population means are the same When the above situation is the case, we will obtain an variance ratio above 3.88 only 5% of the time. We obtain this 3.88 value with the R-code below. # sort the variance ratios smallest to largest var_ratio3_sorted &lt;- sort(data_ratios3$var_ratio) # in order list, obtain the 95th percentile # values beyond this point are in the upper 5% of variances ratios # that occur when the population means are equal extreme_cut_off3 &lt;- var_ratio3_sorted[100000*.95] # round the result and print it extreme_cut_off3 &lt;- round(extreme_cut_off3, 2) print(extreme_cut_off3) ## [1] 3.88 21.8.2.2 It’s really an \\(F\\)-value Most importantly, we don’t usually refer to the ratio we have been examining as a variance ratio. We refer to it as an \\(F\\)-ratio or \\(F\\)-value. We used this simulation to see the range of \\(F\\)-ratios that occur when we KNOW all the population means are equal. From the graph, you can see that this range of \\(F\\)-ratio values is quite wide. But, we estimated that when all three populations means are equal (and n = 5) that only 5% of \\(F\\)-ratios will fall above a value we estimated to be 3.88. This value is what we refer to as \\(F_{critical}\\) when \\(\\alpha = .05\\). Thus, \\(F_{critical}\\)(2, 12) = 3.88. Compare the \\(F\\)-ratio we obtained (3.88) to the values in \\(F\\)-table at 2 and 12 degrees of freedom for \\(\\alpha=.05\\). An \\(F\\)-table is a table of \\(F_{critical}\\) values. You can see that the value in the table is 3.89 - this is the correct value. Our estimate is 3.88 which is very close to the correct value of 3.89. Our value of 3.88 is off a bit from the 3.89 in the \\(F\\)-table because we only use 100,000 samples and not an infinite number of samples. 21.8.2.3 Making your own \\(F\\)-table Congratuations you have just learned how to create the values in the \\(F\\)-table from scratch! To ensure you understand everything you have learned in this exercise think through how you would do the following: Obtain \\(F_{critical}\\)(2, 12) for \\(\\alpha=.01\\) Obtain \\(F_{critical}\\)(3, 36) for \\(\\alpha=.05\\). Look at the degrees of freedom and alpha. How many populations would you use (i.e., what would value would you use for \\(a\\))? How many people would you use in each sample (i.e., what value would you use for \\(n\\))? What position would you look at in the list of sorted variance ratios? Obtain \\(F_{critical}\\)(3, 36) for \\(\\alpha=.01\\) Look at the degrees of freedom and alpha. How many populations would you use (i.e., what would value would you use for \\(a\\))? How many people would you use in each sample (i.e., what value would you use for \\(n\\))? What position would you look at in the list of sorted variance ratios? 21.9 Central \\(F\\)-distributions In this chapter we focused on creating an \\(F\\)-distribution when the population means were the same. When you create an \\(F\\)-distribution based on a scenario where the population means are all the same we refer to that as a Central \\(F\\)-distribution. We created a Central \\(F\\)-distribution with 2 and 12 degrees of freedom (i.e., \\(a=3\\) and \\(n=5\\)). But we could have used a different number of populations (i.e., a different value for \\(a\\)) or a different sample size (i.e., a different value for \\(n\\)). For example, if we had used 5 populations (i.e., \\(a = 5\\)) and a sample size of 20 (i.e., \\(n = 20\\)) then we would have created a Central \\(F\\)-distribution with 4 and 95 degrees of freedom. Recall that \\(df_{numerator}= a - 1 = 5 -1 = 4\\) and that \\(df_{denominator} = a(n-1) = 5 (20-1) = 5(19) = 95\\). Thus, because there are many possible Central \\(F\\)-distributions, we say there is a Family of Central \\(F\\)-distributions. For this Family of Central \\(F\\)-distributions, the family resemblance is that they are all based on a scenario where the population means are equal. You may, or may not, have realized that \\(F\\)-tables are just way of summarizing the Family of Central \\(F\\)-distributions. That is, when you look up a value at 2 and 12 degrees of freedom in an \\(F\\)-table are obtaining information about a specific Central \\(F\\)-distribution. Specifically, you are looking up a “cut” point for extreme values in that particular Central \\(F\\)-distribution. Also note, that when we do significance testing we use the Null Hypothesis that the population means are all equal. This is the same assumption we use when creating a Central \\(F\\)-distribution. Consquently, some people refer to a Central \\(F\\)distribution (or Central \\(t\\) distribution) as the Sampling Distribution for the Null Hypothesis. 21.10 Summary table Much of what we have learned is summarized in the table below. Distribution Attribute Parameter Calculation Estimate of Parameter People Mean \\(\\mu_{people} = \\frac{\\sum_{i=1}^{N}{X_i}}{N}\\) \\(\\bar{x}_{people} = M = \\frac{\\sum_{i=1}^{n}{x_i}}{n}\\) People Variance \\(\\sigma_{people}^2 = \\frac{\\sum_{i=1}^{N}{(X_i - \\mu_{people})^2}}{N}\\) \\(s_{people}^2 = VAR = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x}_{people})^2}}{n-1}\\) People Standard Deviation \\(\\sigma_{people} = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i - \\mu_{people})^2}}{N}}\\) \\(s_{people} = SD = \\sqrt{\\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x}_{people})^2}}{n-1}}\\) Sample Means Mean \\(\\mu_{\\bar{x}} = \\frac{\\sum_{i=1}^{K=\\infty}{\\bar{x}_i}}{K}\\) \\(\\bar{\\bar{x}} = \\frac{\\sum_{i=1}^{a}{\\bar{x}_i}}{a}\\) Sample Means Approach 1: TotalVariance \\(\\sigma_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{K=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{K}\\) \\(s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}\\) Sample Means Approach 2 CLT: Random sampling variance \\(\\sigma_{\\bar{x}}^2 = \\frac{\\sigma_{people}^2}{n}\\) \\(s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n}\\) Sample Means Approach 2 CLT: Standard Error \\(\\sigma_{\\bar{x}} = \\sqrt{\\frac{\\sigma_{people}^2}{n}}=\\frac{\\sigma_{people}}{\\sqrt{n}}\\) \\(s_{\\bar{x}} = SE=\\sqrt{\\frac{s_{people}^2}{n}}=\\frac{s_{people}}{\\sqrt{n}}\\) 21.11 Walk away points Sometimes we need to shift our thinking and conceptualize the distribution of sample means as the population to which we are trying to generalize. Think of \\(s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n}\\) as estimating the variability in the distribution of sample means due to random sampling. It only estimates the variability due to random sampling and not variability due to other sources. The findings that we illustrate here only hold when a) there is one population of people or b) multiple identical populations of people. We are interested in the variance of the distribution of sample means (Figure 21.1B). If we were “all knowing” we would know that the variance of the distribution is 31.26472. But we are not “all knowing”, and so need to rely on sample data to estimate the variance of this distribution. We used two different approaches to estimating the variance of the distribution of sample means. With our “all knowing cap” on we can see, in the table above, that both approaches produce values that differ from the actual variance of the distribution. That’s because the best we can do with sample data is obtain an estimate of the variance of the distribution of means - we can never know the actual variance of the distribution of sample means. Each variance estimate (Approach 1 or Approach 2) will likely produce values that differ from the actual variance of sample means. The first estimate, Approach 1, differs from 31.26472 due to sampling error. That is, we used 3 sample means (\\(a=3\\)) to estimate the variance of an infinite set of sample means (\\(A = \\infty\\)). Because we used such a small subset of sample means (3 of \\(\\infty\\)) our estimate will differ from 31.26472 due to sampling error. Likewise, the second estimate, Approach 2 CLT, used a formula that relies on knowledge of variance of the population of people (\\(\\sigma_{people}^2\\)) to determine the variance of the distribution of sample means. We had to estimate \\(\\sigma_{people}^2\\) using \\(s_{people}^2\\). Because we used such a small subset of people (\\(n = 15\\), 5 from each sample) from the distribution of people (\\(N = 100000\\)) for our estimate, \\(s_{people}^2\\), it will differ from \\(\\sigma_{people}^2\\) due to sampling error. Consequently, our estimate of the variance of sample means, which relies on this value, differs fron 31.26472 due to sampling error. References "],["deeper-dive-sampling-and-anova.html", "Chapter 22 Deeper dive: Sampling and ANOVA 22.1 Now: Different Population Means 22.2 Theory sample mean variance 22.3 Practice sample mean variance 22.4 Back to ANOVA 22.5 Calculation 22.6 A confusing rearrangement 22.7 Worked example 22.8 p-value 22.9 Take a moment 22.10 Summary", " Chapter 22 Deeper dive: Sampling and ANOVA 22.1 Now: Different Population Means In the last chapter, we examined a scenario with multiple populations where the population means were identical. Consequently, sample means differed for one reason: random sampling error. This meant the formulas for a) total variance and b) random sampling variance were estimates of the same quantity. In this chapter, we examine a scenario with multiple populations where the population means are different. Consequently, sample means will differ for two reasons. More specifically, sample means will differ due to differences among the population mean and random sampling error. This means the formulas for a) total variance and b) random sampling variance are estimates of different quantities. 22.2 Theory sample mean variance 22.2.1 Single population (theory) Although the focus of this chapter is multiple populations, we begin with a single population. Recall that when there is a single population we express the variance in sample means using the formula below. This formula is conceptual one that calculates the variance in sample means due to sampling error for a given population variance and sample size. \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error}\\\\ &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ \\end{aligned} \\] 22.2.2 Multiple populations (theory) When there are multiple populations with identical variances, but different population means, that same formula applies for sampling error. That is, because the populations of people all of have the same variance, we can use the old formula to calculate the variability in sample means due to sampling error. \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ \\end{aligned} \\] However, when there are multiple populations we also have to take into account the variability in population means when determining the variance of sample means. We use \\(a\\) to indicate the number of populations. If there are three populations then \\(a = 3\\). We use the formula below to calculate the variance of the population means: \\[ \\begin{aligned} \\text{variance in population means} &amp;= \\sigma_{\\mu_A}^2 \\\\ &amp;= \\frac{\\sum (\\mu_{A_i}-\\mu_{\\mu_A})^2}{a} \\\\ \\end{aligned} \\] where \\[ \\begin{aligned} \\mu_{\\mu_A} &amp;= \\frac{\\sum{\\mu_A}}{a} \\end{aligned} \\] So when there are multiple populations the variability in sample means is due to both sampling error and the variability in population means. \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error} + \\text{variance in population means} \\\\ &amp;= \\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2\\\\ \\end{aligned} \\] Importantly, if the populations all have the same mean then \\(\\sigma_{\\mu_A}^2=0\\). When this happens, sample means differ only due to sampling error. Goal Formula Expected value degrees of feedom Estimated total variance \\(\\frac{\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}\\) \\(\\frac{\\sigma_{people}^2}{n} + \\sigma_{A}^2\\) \\(a-1\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{n}\\) \\(\\frac{\\sigma_{people}^2}{n}\\) \\(n-1\\) The table above summarizes the different estimate goals of the two formulas. Consider the “Expected value” (or Expected Mean Square) column. When the populations means are the same then \\(\\sigma_{A}^2 = 0\\). When the population means are different, then \\(\\sigma_{A}^2&gt;0\\). Note that when the population mean are the same, and \\(\\sigma_{A}^2\\) becomes \\(0\\), the “Expected value” is becomes the same for both formulas. That is, they are both estimates of the same quantity, namely, \\(\\frac{\\sigma_{people}^2}{n}\\). We examined that scenario in the last chapter. In this chapter, our final goal is to examine the multiple population scenario where \\(\\sigma_{A}^2&gt;0\\). That is, that situation where the two formulas are estimates of different quantities. But we begin with a review of the single population scenario. 22.3 Practice sample mean variance 22.3.1 Single population (practice) Let’s quickly revisit the scenario where there is a single population with a mean of 172.50 and a variance of 156.3236. Sample means (n = 5) will only differ due to sampling error. We can calculate the variance due to sampling error with the equation below. \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ &amp;= \\frac{156.3236}{5}\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] Thus, where there is a single population (with variance 156.3236) and the sample size is n = 5, sample means will differ only due to sampling error. The variance in the sample means will be 31.26472. We can confirm this with the R code below. We create a population and then take 9,000 sample means. library(tidyverse) library(learnSampling) set.seed(1) # set random number seed pop1 &lt;- make_population(mean = 172.50, variance = 156.3236) set.seed(1) sample_means &lt;- get_mean_samples(pop1, n = 5, number.of.trials = 9000) ## Number of populations: 1 ## Using a = 1 The first few rows of the data set with 9,000 sample mean is below: head(sample_means) ## trial source_population sample_mean ## 1 1 1 176.2 ## 2 2 1 167.4 ## 3 3 1 168.8 ## 4 4 1 175.8 ## 5 5 1 174.0 ## 6 6 1 157.4 The trial column indicate which trial (of the 10,000) is represented in that row. The source population column indicate the population we obtain the sample from (1 since we only had 1 population). The sample mean column indicate the sample mean in that trial. sample_means %&gt;% summarise(var_means = var(sample_mean)) ## var_means ## 1 30.8 We take the variance of all of those sample means and get a value of 30.80005 which closely approximately the actual value of 31.26472 (which we get when using an infinite number of sample means.) 22.3.2 Multiples population, equal \\(\\mu\\) (practice) Now let’s see what happens when we have three populations with different means - as illustrated in the table and figure below. Notice the mean for all three populations is 172.50 in the table below. This means, \\(\\sigma_{A}^2 = 0\\). Population Mean Variance 1 \\(\\mu_{A1_{people}}= 172.50\\) \\(\\sigma_{1_{people}}^2= 156.3236\\) 2 \\(\\mu_{A2_{people}}= 172.50\\) \\(\\sigma_{2_{people}}^2= 156.3236\\) 3 \\(\\mu_{A3_{people}}= 172.50\\) \\(\\sigma_{3_{people}}^2= 156.3236\\) We created a scenario where the sample means differed to random sampling only when we created three populations with identical means (and variances). This situation is depicted in the scenario below. FIGURE 22.1: Sampling from three populations with the same mean (and variance) Because the populations have equal variances we calculate the variance due to sampling error the way we did previously: \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ &amp;= \\frac{156.3236}{5}\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] But we also have to calculate the variance in population means. But to calculate the variance of the population means we need the mean of the population means. This value is calculated below - it’s simply the average of 172.50 repeated three times: \\[ \\begin{aligned} \\mu_{\\mu_A} &amp;= \\frac{\\sum{\\mu_A}}{a} \\\\ &amp;= \\frac{\\mu_{A1}+\\mu_{A2}+\\mu_{A3}}{a} \\\\ &amp;= \\frac{172.50 + 172.50 + 172.50}{3} \\\\ &amp;= \\frac{517.50}{3}\\\\ &amp;= 172.50 \\\\ \\end{aligned} \\] We use this value to calculate the variance of the population means. Which we find to be 0 because the population means are all the same. \\[ \\begin{aligned} \\sigma_{\\mu_A}^2 &amp;= \\frac{\\sum (\\mu_{A_i}-\\mu_{\\mu_A})^2}{a} \\\\ &amp;= \\frac{(\\mu_{A_1} - \\mu_{\\mu_A})^2 + (\\mu_{A_2} - \\mu_{\\mu_A})^2 + (\\mu_{A_3} - \\mu_{\\mu_A})^2}{3} \\\\ &amp;= \\frac{(172.50 - 172.50)^2 + (172.50 - 172.50)^2 + (172.50 - 172.50)^2}{3} \\\\ &amp;= \\frac{0}{3} \\\\ &amp;= 0\\\\ \\end{aligned} \\] Thus, as indicated above: \\(\\sigma_{A}^2 = 0\\). So the total variance is sample means is: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error} + \\text{variance in population means} \\\\ &amp;= \\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2\\\\ &amp;= 31.26472 + 0\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] We can confirm this with the simulation below. In this simulation we still take 9000 sample means but its 3000 from each of the three populations. library(tidyverse) library(learnSampling) set.seed(1) pop1 &lt;- make_population(mean = 172.50, variance = 156.3236) pop2 &lt;- pop1 # populations are identical pop3 &lt;- pop1 # populations are identical set.seed(1) sample_means &lt;- get_mean_samples(pop1, pop2, pop3, n = 5, number.of.trials = 9000) ## Number of populations: 3 ## Using a = 3 The first few rows of the data set with 9,000 sample mean is below: head(sample_means) ## trial source_population sample_mean ## 1 1 1 176.2 ## 2 2 2 167.4 ## 3 3 3 168.8 ## 4 4 1 175.8 ## 5 5 2 174.0 ## 6 6 3 157.4 The trial column indicate which trial (of the 9,000) is represented in that row. The source population column indicate the population we obtain the sample from (1 since we only had 1 population). The sample mean column indicate the sample mean in that trial. sample_means %&gt;% summarise(var_means = var(sample_mean)) ## var_means ## 1 30.81 We take the variance of all of those sample means and get a value of 30.80005 which closely approximately the actual value of 31.26472 (which we get when using an infinite number of sample means.) That is, when multiple populations have identical means, the variance of sample means from those populations is due entirely to sampling error. That variance of sample means is represented by the formula below: \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ \\end{aligned} \\] 22.3.3 Multiples population, different \\(\\mu\\) (practice) Now let’s see what happens when we have three populations with different means - as illustrated in the table and figure below. The different populations means indicate that \\(\\sigma_{A}^2&gt;0\\). Population Mean Variance 1 \\(\\mu_{A1_{people}}= 152.50\\) \\(\\sigma_{1_{people}}^2= 156.3236\\) 2 \\(\\mu_{A2_{people}}= 172.50\\) \\(\\sigma_{2_{people}}^2= 156.3236\\) 3 \\(\\mu_{A3_{people}}= 192.50\\) \\(\\sigma_{3_{people}}^2= 156.3236\\) FIGURE 22.2: Sampling from three populations with the same mean (and variance) Because the populations have equal variances (even if the means are different) we calculate the variance due to sampling error the way we did previously: \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ &amp;= \\frac{156.3236}{5}\\\\ &amp;= 31.26472\\\\ \\end{aligned} \\] But we also have to calculate the variance in population means. But to calculate the variance of the population means we need the mean of the population means. This value is calculated below - notice that we are simply averaging the three population means: 152.5, 172.5, and 192.5. \\[ \\begin{aligned} \\mu_{\\mu_A} &amp;= \\frac{\\sum{\\mu_A}}{a} \\\\ &amp;= \\frac{\\mu_{A1}+\\mu_{A2}+\\mu_{A3}}{a} \\\\ &amp;= \\frac{152.50 + 172.50 + 192.50}{3} \\\\ &amp;= \\frac{517.50}{3}\\\\ &amp;= 172.50 \\\\ \\end{aligned} \\] We use this value to calculate the variance of the population means. Which we find to be 0 because the population means are all the same. \\[ \\begin{aligned} \\sigma_{\\mu_A}^2 &amp;= \\frac{\\sum (\\mu_{A_i}-\\mu_{\\mu_A})^2}{a} \\\\ &amp;= \\frac{(\\mu_{A_1} - \\mu_{\\mu_A})^2 + (\\mu_{A_2} - \\mu_{\\mu_A})^2 + (\\mu_{A_3} - \\mu_{\\mu_A})^2}{3} \\\\ &amp;= \\frac{(152.50 - 172.50)^2 + (172.50 - 172.50)^2 + (192.50 - 172.50)^2}{3} \\\\ &amp;= \\frac{(-20)^2 + (0)^2 + (20)^2}{3} \\\\ &amp;= \\frac{400 + 0 + 400}{3} \\\\ &amp;= \\frac{800}{3} \\\\ &amp;= 266.6667\\\\ \\end{aligned} \\] Thus, as indicated above, \\(\\sigma_{A}^2&gt;0\\). So the total variance is sample means is: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error} + \\text{variance in population means} \\\\ &amp;= \\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2\\\\ &amp;= 31.26472 + 266.6667\\\\ &amp;= 297.9314\\\\ \\end{aligned} \\] We can confirm this with the simulation below. In this simulation we still take 9000 sample means but its 3000 from each of the three populations. library(tidyverse) library(learnSampling) set.seed(1) pop1 &lt;- make_population(mean = 172.50, variance = 156.3236) pop2 &lt;- make_population(mean = 152.50, variance = 156.3236) pop3 &lt;- make_population(mean = 192.50, variance = 156.3236) set.seed(1) sample_means &lt;- get_mean_samples(pop1, pop2, pop3, n = 5, number.of.trials = 9000) ## Number of populations: 3 ## Using a = 3 The first few rows of the data set with 9,000 sample mean is below: head(sample_means) ## trial source_population sample_mean ## 1 1 1 176.2 ## 2 2 2 147.4 ## 3 3 3 188.8 ## 4 4 1 175.8 ## 5 5 2 154.0 ## 6 6 3 177.4 The trial column indicate which trial (of the 9,000) is represented in that row. The source population column indicate the population we obtain the sample from (1 since we only had 1 population). The sample mean column indicate the sample mean in that trial. sample_means %&gt;% summarise(var_means = var(sample_mean)) ## var_means ## 1 297.8 We take the variance of all of those sample means and get a value of 297.7683 which closely approximately the actual value of 297.9314 (which we get when using an infinite number of sample means.) That is, when multiple populations have different means, the variance of sample means from those populations is due to both sampling error and the variance of the population means. That variance of sample means is represented by the formula below: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2\\\\ \\end{aligned} \\] 22.4 Back to ANOVA 22.4.1 Conceptual/Expected \\(F\\)-values In the preceding simulations we saw how the variance of the distribution of sample means was different in the a) population means the same scenario and b) population means different scenario. That is, when the population means were different the variance of the distribution of sample means was larger. Think about this finding in terms of an experiment. Consider a scenario where you are interested in examining the effect of crowding on aggressive behavior. Your dependent variable is aggressive behavior. Your independent variable is Crowding with three levels: low, medium, and high. Your study has 5 people in each of these three conditions. The 5 people’s aggression scores in the low crowding condition represent a sample aggression scores from a low crowding population. Likewise, the 5 people’s aggression scores in the medium crowding condition represent a sample aggression scores from a medium crowding population. Finally, the 5 people people’s aggression scores in the high crowding condition represent a sample aggression scores from a high crowding population. You calculate a sample mean for each of the three conditions. You are asking if there is an effect of crowding on aggression. To answer this question we need to think of the distribution of sample means that would occur if you repeats the study thousands of times. If you repeated the study thousand of times, and crowding had now effect on aggression the distribution of sample means differ only due to sampling error and be described by the formula below. We think of this formula as describing the distribution of sample means under the Null Hypothesis (i.e., the scenario where the population means are equal.) \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error} \\\\ &amp;= \\frac{\\sigma_{people}^2}{n} \\\\ \\end{aligned} \\] In contrast, if the crowding manipulation did influence aggressive behavior, this means there are different population means for the three crowding conditions. If the population means are different, then means the variance for the distribution of sample means would correspond to: \\[ \\begin{aligned} \\sigma_{\\bar{x}}^2 &amp;= \\text{variance due to sampling error} + \\text{variance in population means} \\\\ &amp;= \\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2\\\\ \\end{aligned} \\] Conceptually, an \\(F\\)-value represents the comparison of these two approaches: \\[ \\begin{aligned} F &amp;= \\frac{\\text{total variance}}{\\text{variance due to sampling error}}\\\\ &amp;= \\frac{\\text{variance due to sampling error} + \\text{variance in population means}}{\\text{variance due to sampling error}}\\\\ &amp;= \\frac{\\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2}{\\frac{\\sigma_{people}^2}{n}}\\\\ \\end{aligned} \\] 22.4.2 Two scenarios Consider what happens to the above \\(F\\)-value when the population means are all the same. In this case the variance in population means is zero, \\(\\sigma_{\\mu_A}^2=0\\). When this happens, the numerator and denominator are the same - so the expected \\(F\\)-value is 1.00. Now consider what happens to the above \\(F\\)-value when the population means are different. In this case, the variance of population means is larger than zero, \\(\\sigma_{\\mu_A}^2 &gt; 0\\). When this happens, the numerator is larger than the denominator - so the expected \\(F\\)-value is greater than 1.00. 22.5 Calculation The \\(F\\)-value formulas presented above are for the expected values. How do we calculate \\(F\\)-values in practice? The formula below estimates the variance in sample means due to all sources (sampling error and the variance in population means): \\[ \\begin{aligned} s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1} \\end{aligned} \\] In contrast, the formula below estimates the variance due to sampling error alone: \\[ \\begin{aligned} s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n} \\end{aligned} \\] Consequently, in practice we calculate \\(F\\)-values using the equation below. \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}} \\end{aligned} \\] FIGURE 22.3: Linking conceptual and calculation formulas for the F-value. 22.6 A confusing rearrangement Unfortunately, most textbooks don’t present \\(F\\)-values in the conceptually easy to follow way presented above. Instead, they use the algebraic rearrangement presented below. The rearrangement is important, however, because it is the basis for how information is presented in an ANOVA table. Unfortunately, when the rearrangement is used - it’s hard to understand the equation. Below we show the conceptual rearrangement of the formula (i.e., the rearrangement of the expected values). \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2}{\\frac{\\sigma_{people}^2}{n}}*1\\\\ &amp;= \\frac{\\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2}{\\frac{\\sigma_{people}^2}{n}}*\\frac{n}{n}\\\\ &amp;= \\frac{n(\\frac{\\sigma_{people}^2}{n} + \\sigma_{\\mu_A}^2)}{n(\\frac{\\sigma_{people}^2}{n})}\\\\ &amp;= \\frac{\\sigma_{people}^2 + n \\sigma_{\\mu_A}^2}{ \\sigma_{people}^2}\\\\ &amp;= \\frac{\\sigma_{error}^2 + n \\sigma_{\\mu_A}^2}{ \\sigma_{error}^2}\\\\ \\end{aligned} \\] Likewise, the calculation version is often presented in a different way - which is also harder to understand: \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}}*1\\\\ &amp;= \\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}}*\\frac{n}{n}\\\\ &amp;= \\frac{n \\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{n \\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{\\frac{n \\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{s_{people}^2}\\\\ \\end{aligned} \\] 22.7 Worked example A researcher was interested in people’s reactions to video game feedback. Participants were randomly assigned to each of three conditions: negative feedback (subjects were insulted for their lack of ability to learn the game by a person over a loudspeaker who was reported to be in a control booth monitoring their performance), positive feedback (subjects were encouraged at how well they were doing), and a control condition in which subjects did not receive feedback. The dependent measure was video game score. Feedback Mean Variance n negative \\(\\bar{x}_{negative}= 17498.97959\\) \\(s_{negative}^2= 41157141\\) n = 196 none \\(\\bar{x}_{none}= 20196.90306\\) \\(s_{none}^2= 37084531\\) n=196 positive \\(\\bar{x}_{positive}= 25003.62245\\) \\(s_{positive}^2= 31086669\\) n=196 First, we estimate the population variance: \\[ \\begin{aligned} \\text{variance due to sampling error} &amp;= \\frac{s_{negative}^2+s_{none}^2+s_{positive}^2}{3}\\\\ &amp;= \\frac{41157141+37084531+31086669}{3}\\\\ &amp;= 36442780\\\\ \\end{aligned} \\] Second, we estimate the variance in sample means as if sampling error was the only source of variability: \\[ \\begin{aligned} s_{\\bar{x}}^2 &amp;= \\frac{s_{people}^2}{n}\\\\ &amp;= \\frac{36442780}{196}\\\\ &amp;= 185932.6\\\\ \\end{aligned} \\] Third, we estimate the variance in the distribution of sample means due to all sources: \\[ \\begin{aligned} s_{\\bar{x}}^2 &amp;= \\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}\\\\ &amp;= \\frac{(\\bar{x}_{negative}-\\bar{\\bar{x}})^2+(\\bar{x}_{none}-\\bar{\\bar{x}})^2+(\\bar{x}_{positive}-\\bar{\\bar{x}})^2}{3-1}\\\\ &amp;= \\frac{(17498.97959-20899.83503)^2+(20196.90306-20899.83503)^2+(25003.62245-20899.83503)^2}{3-1}\\\\ &amp;= 14450501.13 \\end{aligned} \\] Fourth, we compare those two estimates in a ratio: \\[ \\begin{aligned} F &amp;= \\frac{\\text{variance due to sampling error} + \\text{variance in population means}}{\\text{variance due to sampling error}}\\\\ &amp;= \\frac{\\frac{\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{\\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{14450501.13}{\\frac{36442780}{196}}\\\\ &amp;= 77.72\\\\ \\end{aligned} \\] The rearrangement of the \\(F\\)-value equation is below. This can be used to understand the other components of the \\(F\\)-table. \\[ \\begin{aligned} F &amp;= \\frac{ \\frac{(n)\\sum_{i=1}^{a=3}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}}{s_{people}^2}\\\\ &amp;= \\frac{(196)14450501.13}{36442780}\\\\ &amp;= 77.72\\\\ \\end{aligned} \\] Below we see this formula mapped to the \\(F\\)-table. Pay attention to the feedback line of this table. FIGURE 22.4: Mapping formulas to the F-table. 22.8 p-value Notice on the feedback line of the output that the \\(p-value\\) is very small \\(p&lt;.001\\). How did we obtain this value? The F-value is obtained by creating a simulation where there is no effect. We do as we did in the last chapter. First, we assume the population means were identical. Second, we assume the population variances are identical. Third, we imagine taking three samples (\\(a = 3\\)), one for each population, each \\(n = 5\\). Fourth, we imagine creating an F-value from the data. Fifth, we imagine repeating this process a large number of times. The results is the distribution of F-values that you would obtain if there was no effect. Then we see how the F-value from our study (\\(F = 77.72\\)) fits into this distribution. In the previous chapter, we created this exact distribution. See it presented again below. Notice the F-value from our study (\\(F = 77.72\\)) can’t fit on this axis range. FIGURE 22.5: Variance ratio when there are three identical populations of people So we make the graph again, with a wider x-axis. Now we can see the F-value from our study (\\(F = 77.72\\)) is quite unlikely when the means and variances for the three populations are all the same. The F-value from our study is indicated by the vertical red line. FIGURE 22.6: Next axes: variance ratio when there are three identical populations of people Because the red line is so extreme, our \\(p-value\\) is very low \\(p&lt;.0001\\). This is the probability of our data (or more extreme data), assuming the population means are all the same. Consequently, we say the ANOVA is significant. We began by assuming the population means were all the same. Under this assumption, our study result is incredibly unlikely. Consequently, we reject the assumption, that the population means are all the same, and conclude the population means may be different. 22.9 Take a moment Take a moment and think about all of the math and logic we’ve used to come to this point. In particular, take note of the fact that we are making conclusions at the population-level not the sample-level. 22.10 Summary Attribute Parameter Calculation Estimate of Parameter Reflects Method 1: Total Variance \\(\\sigma_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{K=\\infty}{(\\bar{x}_i - \\mu_{\\bar{x}})^2}}{K}\\) \\(s_{\\bar{x}}^2 = \\frac{\\sum_{i=1}^{a}{(\\bar{x}_i - \\bar{\\bar{x}})^2}}{a-1}\\) Sampling Error &amp; Variance of Population Means Method 2 CLT: Variance \\(\\sigma_{\\bar{x}}^2 = \\frac{\\sigma_{people}^2}{n}\\) \\(s_{\\bar{x}}^2 = \\frac{s_{people}^2}{n}\\) Just Sampling Error "],["sampling-and-n-way-anova.html", "Chapter 23 Sampling and N-way ANOVA 23.1 Previously: One-way ANOVA 23.2 Now: N-way ANOVA 23.3 Output 23.4 Multiple frames of reference 23.5 Frame 1: Cell Populations 23.6 Frame 2: \\(A\\) Populations (Juggling) 23.7 Frame 3: \\(B\\) Populations (Preparation) 23.8 ANOVA by another name 23.9 Homogeneity of variance 23.10 Estimating population variance 23.11 Random Sampling Variance 23.12 Main Effect \\(A\\): Juggling 23.13 Main Effect \\(B\\): Preparation 23.14 Interaction Preparation 23.15 Interaction (\\(AB\\))", " Chapter 23 Sampling and N-way ANOVA 23.1 Previously: One-way ANOVA In the last chapter, we considered a scenario where there was a single independent variable with multiple levels. For example, imagine that we are interested measuring the heights of students (a dependent variable). We might examine how those heights vary across three populations. That is, we could think of university as the independent variable and Waterloo, Western, and Guelph as the levels of that single independent variable, university. In this example, the independent variable (i.e., university) represents a SET of populations and the each level of the independent variable is a specific population (e.g., Waterloo). 23.2 Now: N-way ANOVA In this chapter, we consider a scenario where there are multiple independent variables. We call it N-way because the N- refers to the number of independent variables. I must stress, however, that you should probably limit the number of independent variables in you study to at most three independent variables. Cognitive psychology research has demonstrated that human beings have a very hard time wrapping their minds around the complexity associated with four independent variables and interpreting five independent variables is practically impossible. My understanding is that, somewhat ironically, 4-way interactions are not uncommon in Cognitive psychology. Be sure to read the article below though so that you can avoid the problems associated with having too many independent variables. Halford, G. S., Baker, R., McCredden, J. E., &amp; Bain, J. D. (2005). How many variables can humans process?. Psychological science, 16(1), 70-76. Chicago. N-way ANOVA differs from one-way ANOVA because you can look how the multiple independent variables combine to influence a dependent variable. When we look at these combined effects we are looking at an interaction (or moderation) effect. Additionally, because there are multiple independent variables there are multiple sets of populations to consider. For simplicity, we will focus on only two independent variables for this chapter (i.e., a 2-way ANOVA). Specifically, consider a fictitious example where we examine how two independent variables combine to influence how calm students are before an exam. Calmness before the exam is the dependent variable. The first independent variable is juggling (yes/no) during study breaks. This independent variable has two levels: yes and no. So we think of there being two populations of calmness scores obtained before the exam: 1) a population of calmness scores for students who learned to juggling during study breaks and 2) a population of calmness scores for students who did not learn to juggle during study breaks. We refer to this set of two populations (yes and no juggling) using the label juggling. Juggling is an independent variable or factor in our design. We are interested in how juggling level (yes/no) influences the mean of these two populations of calmness scores. We consider the participants in our study as samples from these two populations. The second independent variable is preparation (low/medium/high) time before the exam. That is, how much students studied before the exam. This independent variable has three levels: low, medium, and high. So we think of there being three populations of calmness scores obtained before the exam: 1) a population of calmness scores where students prepared a low amount of time, 2) a population of calmness scores where students prepared a medium amount of time, and 3) a population of calmness scores where students prepared a high amount of time. We refer to this set of three populations (low, medium, and high preparation) using the label preparation. Preparation is an independent variable or factor in our design. We are interested in how preparation level (low/medium/high) influences the mean of these three populations of calmness scores. We consider the participants in our study as samples from these three populations. As mentioned previously, an advantage of conducting a 2-way ANOVA over two 1-way ANOVAs is that we get to look at how calmness scores are influenced by the combined effect of the two independent variables. You can see this in the diagram below - there are six cells each of which is created by the combination of one level of juggling and one level of preparation. We call this a fully-crossed design because there are people at every combination of juggling and preparation levels. When we examine cells we are looking at the combined effect of independent variables on the dependent variable. When we do this, we typically say we are examining how the independent variables interact to influence the dependent variable. You can this of the six cells are another set of populations. We will see people that you can think of each set of populations as a frame of reference. 23.3 Output We begin by creating the output for this design in R. First we activate the required packages: library(tidyverse) library(apaTables) nway_data = read_csv(&quot;data_crf.csv&quot;) # Turn character columns into factors nway_data &lt;- nway_data %&gt;% mutate_if(is.character, as.factor) Next we conduct the analysis: # set contrasts to values match SPSS options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # Conduct analysis lm_nway &lt;- lm(calm ~ preparation*juggling, data = nway_data) Then we create the output using: apa.aov.table(lm_nway, filename = &quot;my_table.doc&quot;) Notice the preparation, juggling, and preparation x juggling rows on this table. In particular, the \\(F\\)-value for each of these rows. In the remainder of the chapter we will review how these values are obtained - and their conceptual meaning. 23.4 Multiple frames of reference A complication many students experience with understanding the different frames of reference that can be used with N-way ANOVA is the notation. Particularly when it comes the different sample sizes involved in this design. Therefore, prior to continuing we’ll review some key notation. The notation is general in nature because each experiment will have different independent variables. A common way to refer to the first independent variable using symbolic notation is by using an uppercase \\(A\\). Likewise, we refer to the number of levels in that independent variable using lowercase \\(a\\). Thus, for the first independent variable \\(A\\) = “Juggling” and \\(a = 2\\) because there are two levels of juggling (yes or no). So instead of saying we are examining the effect of juggling on calmness we might say we are examining the effect of Independent Variable \\(A\\) on the dependent variable. For the second independent variable we use uppercase \\(B\\) and lowercase \\(b\\), and so on. The table below illustrates how we represent both independent variables in this experiment. Effect Letter designation Number of levels Levels Independent variable 1 \\(A\\) = Juggling \\(a = 2\\) yes, no Independent variable 2 \\(B\\) = Preparation \\(b = 3\\) low, medium, high There are three frames of reference we will use for this 2-way ANOVA. The key to understanding these frames of reference is to think in terms of populations that are represented in your experiment. These population were mentioned previously, but we’ll review them in more detail below. 23.5 Frame 1: Cell Populations With this first frame of reference we look at combinations of the levels of the \\(A\\) and \\(B\\) independent variables (i.e., juggling and preparation). Recall we refer to each combination of levels as a cell. For example, individuals that both juggled during breaks (i.e., yes level of juggling) and prepared for a low amount of time (i.e., low level of preparation), would be in one cell. As mentioned previously, be sure to think in terms of populations. In this study, for this frame of reference, we can calculate the total number of populations by multiplying the number of levels for the independent variables. Here, \\(a = 2\\) (two levels of juggling) and \\(b =3\\) (three levels of preparation). Thus, \\(ab=(2)(3)=6\\), so there are six cell populations. The participants in each cell are a sample from a corresponding cell population. The six cell populations are illustrated below. For example, there is a single population (Population 1) that is composed of people who juggled during breaks and prepared for a low amount of time. The participants in Cell 1 are a sample (\\(n = 5\\) ) from the No Juggling Low Preparation Population. We look at the means of these six cells when we are examining an interaction among the independent variables. 23.6 Frame 2: \\(A\\) Populations (Juggling) The second frame of reference we examine is that of the first independent variable (i.e., \\(A\\)). We examine the effect of juggling (i.e., \\(A\\)) on calmness ignoring the effect of preparation (i.e., \\(B\\)). Within this frame of reference we imagine that there are two populations of calmness scores; one for each level of juggling. Note that the sample size is different for this frame of reference. For each level of juggling, the sample size is 15. Thus, if you were comparing the yes juggling population to the no juggling population each sample mean would would be based on 15 people. This should be clear when you look at the diagram below. Unfortunately, students often struggle when presented with the sample size as a calculation with symbolic notation. Recall that we we use \\(n = 5\\) to refer the number of people in each cell, \\(a = 2\\) to refer to the number of juggling levels, and \\(b = 3\\) to refer to the number of preparation levels. When we want to know the sample size for an \\(A\\) mean we simply multiply the cell size (\\(n = 5\\)) by the number of levels of B (i.e., \\(b = 3\\)) to obtain 15. This is confusing for some people because even though you want to calculate the sample size of the means for \\(A\\) you use \\(b\\) in the calculation. But if you look at the diagram above, it should be clear why this is the case. \\[ \\begin{aligned} \\text{Sample Size A} &amp;= nb\\\\ &amp;= 5\\times3\\\\ &amp;= 15\\\\ \\end{aligned} \\] Note when you examine the means for Juggling we refer to this using a few different phrases: Examining the Main effect of \\(A\\) Examining the Margin means for \\(A\\) Examining the Main effect of Juggling Examining the Margin means for Juggling But again remember we are using sample means to make inferences about population means. Never forget the conclusions we make are at the population level - not the sample level. 23.7 Frame 3: \\(B\\) Populations (Preparation) The third frame of reference we examine is that of the second independent variable (i.e., \\(B\\)). That is, we examine the effect of preparation (i.e., \\(B\\)) ignoring the effect of juggling (i.e., \\(A\\)). Within this frame of reference we imagine that there are three populations; one for each level of preparation. Note that the sample size is again different for this frame of reference. For each level of preparation, the sample size is 10. Thus, if you were comparing the mean for the low preparation population to the medium population each sample mean would would be based on 10 people. This should be clear when you look at the diagram below. When we want to know the sample size for a \\(B\\) mean we simply multiply the size size (\\(n = 5\\)) by the number of levels of \\(A\\) (i.e., \\(a =2\\)). This is confusing for some people because even though you want to know the sample size of the means for \\(B\\) you use \\(a\\) in the calculation. But if you look the diagram above, it should be clear why this is the case. \\[ \\begin{aligned} \\text{Sample Size B} &amp;= na\\\\ &amp;= 5\\times2\\\\ &amp;= 10\\\\ \\end{aligned} \\] Note when you examine the means for Preparation we refer to this using a few different phrases: Examining the Main effect of \\(B\\) Examining the Margin means for \\(B\\) Examining the Main effect of Preparation Examining the Margin means for Preparation But again remember we are using sample means to make inferences about population means. Never forget the conclusions we make are at the population level - not the sample level. 23.8 ANOVA by another name A rose by another name would smell as sweet ANOVA by another name would be just as useful As noted, some researchers use the world factor to refer to an independent variable. An implication of this is that instead of using the term N-way ANOVA they use the term Completely Randomized Factorial Design or CRF Design. So be aware there are many ways to refer to an ANOVA design. Likewise, note that in some research areas, they use a different word for interaction, namely the synonym moderation. That is, although many research areas would say: “We examined how juggling and preparation interacted to influence calmness scores”. Other research areas might phrase it as 1) “We examined how the effect of juggling on calmness scores was moderated by preparation” or, the other way around, 2) “We examined how the effect of preparation on calmness scores was moderated by juggling”. Mathematically, these three ways of describing the combined effect of the independent variables (or factors) on the dependent variable are identical. Conceptually, the two moderation examples differ, but surprisingly not in the math we use to test them. 23.9 Homogeneity of variance As we saw in previous sections, depending on the effect you are interested in, the relevant populations (i.e., frame of reference) can be quite different. However, regardless of the frame of reference, we assume the variance of people’s calmness scores (\\(\\sigma_{people}^2\\)) is the same across populations. For the cell frame of reference, we assume the variance of calmness scores in all six populations of calmness scores (\\(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2,\\sigma_5^2,\\sigma_6^2\\)) is the same. Using symbols, we indicate that there a single value, \\(\\sigma_{cells}^2\\), for the variance of calmness scores at the population level across the six cells: \\(\\sigma_{cells}^2=\\sigma_1^2=\\sigma_2^2=\\sigma_3^2=\\sigma_4^2=\\sigma_5^2=\\sigma_6^2\\) For the effect of \\(A\\) frame of reference (i.e., juggling), we assume the variance of calmness scores in both juggling populations (\\(\\sigma_{A1}^2,\\sigma_{A2}^2\\)) is the same. Using symbols, we indicate that there a single value, \\(\\sigma_{A}^2\\), for the variance of calmness scores at the population level across the both populations. \\(\\sigma_{A}^2=\\sigma_{A1}^2=\\sigma_{A2}^2\\) For the effect of \\(B\\) frame of reference (i.e., preparation), we assume the variance of calmness scores in all three preparation populations (\\(\\sigma_{B1}^2=\\sigma_{B2}^2=\\sigma_{B3}^2\\)) is the same. Using symbols, we indicate that there a single value, \\(\\sigma_{B}^2\\), for the variance of calmness scores at the population level across the three populations. \\(\\sigma_{B}^2=\\sigma_{B1}^2=\\sigma_{B2}^2=\\sigma_{B3}^2\\) But most importantly we assume the variance of calmness scores is the same across all three frames of reference. That is, there is a single value that represents the variance of population-level calmness scores regardless of the population set. We refer to this as value the variance of people’s calmness score and represent it with the symbol: \\(\\sigma_{people}^2\\) So: \\(\\sigma_{people}^2 =\\sigma_{cells}^2=\\sigma_{A}^2=\\sigma_{B}^2\\) We typically refer to this as the homogeneity of POPULATION variances assumption. With this assumption, in this example, we are referring the variance of participants calmness scores across the populations in the variance frames of reference. 23.10 Estimating population variance Let’s start by considering the cell frame of reference. Within this frame of reference there are six populations of calmness scores and we have a sample (\\(n =5\\)) from each population. We can estimate each of the six population variances using this data. To do so, however, we need the sample mean for each of the six cells. So we calculate those means (\\(\\bar{x}\\)) first. 23.10.1 Sample means We calculate the sample means for each cell using the formula below: \\[ \\bar{x} = \\frac{\\sum x_i}{n} \\] We use this formula in each of the cells: Each of these six cell means (i.e., sample means) is an estimate of the corresponding six population means. 23.10.2 Population variance estimates Now that we have the mean for each cell we can estimate the variance of calmness scores for the six populations. We do this with the usual equation below: \\[ s^2 = \\frac{\\sum (x_i - \\bar{x}^2)}{n-1} \\] We use this formula in each of the cells: As noted above, it’s important to remember that each sample variance (e.g., \\(s_1^2\\)) is an estimate of the corresponding population variance (e.g., \\(\\sigma_1^2\\)). This is illustrated in the figure below. 23.10.3 Averaging variances In the previous section, we started from the premise that there were six populations. The populations variances (\\(\\sigma^2\\)) are unknown but we calculated an estimate of each population variance (\\(s^2\\)) using sample data. These are presented in the table below: Cell Juggling level Preparation level Population variance Sample estimate 1 yes low \\(\\sigma_1^2\\) \\(s_1^2 = 30.3\\) 2 yes medium \\(\\sigma_2^2\\) \\(s_2^2 = 78.8\\) 3 yes high \\(\\sigma_3^2\\) \\(s_3^2 = 66.5\\) 4 no low \\(\\sigma_4^2\\) \\(s_4^2 = 9.8\\) 5 no medium \\(\\sigma_5^2\\) \\(s_5^2 = 58.3\\) 6 no high \\(\\sigma_6^2\\) \\(s_6^2 = 116.3\\) Recall, however, the homogeneity of population variances assumption. We assume \\(\\sigma_{people}^2=\\sigma_1^2=\\sigma_2^2=\\sigma_3^2=\\sigma_4^2=\\sigma_5^2=\\sigma_6^2=\\sigma^2=?\\). Consequently, the sample variance estimates (\\(s_1^2\\), \\(s_2^2\\), \\(s_3^2\\), \\(s_4^2\\), \\(s_5^2\\), \\(s_6^2\\)) are estimates of the same population value of calmness scores, \\(\\sigma_{people}^2\\). Therefore, a better way to display the population variance estimates is with the table below. Population Variance Sample Estimates \\(\\sigma_{people}^2\\) \\(s_1^2 = 30.3\\), \\(s_2^2 = 78.8\\), \\(s_3^2 = 66.5\\) \\(s_4^2 = 9.8\\), \\(s_5^2 = 58.3\\), \\(s_6^2 = 116.3\\) CRITICAL: When we ASSUME the six sample estimates (\\(s_1^2\\), \\(s_2^2\\), \\(s_3^2\\), \\(s_4^2\\), \\(s_5^2\\), \\(s_6^2\\)) are all estimates of the same population value, \\(\\sigma_{people}^2\\), we can combine the sample estimates (i.e., average them) to get a best estimate of the population value (i.e., \\(s_{people}^2\\)). This makes sense because, with the identical population variances assumption, each sample variance estimate (e.g., \\(s_1^2\\)) only differs from the population variance (\\(\\sigma_{people}^2\\)) due to random sampling error. By averaging the sample estimates (\\(s_1^2\\), \\(s_2^2\\), \\(s_3^2\\), \\(s_4^2\\), \\(s_5^2\\), \\(s_6^2\\)) we are “averaging out” the random sampling error. We use the formula below to average the cell variances when the sample sizes for the 6 cells are identical. That is, all \\(n = 5\\).: \\[ \\begin{aligned} s_{people}^2 &amp;=\\frac{s_1^2 + s_2^2 + s_3^2 + s_4^2 + s_5^2 + s_6^2}{6}\\\\ &amp;= \\frac{30.3 + 78.8 +66.5 +9.8 +58.3 +116.3 }{6} \\\\ &amp;= 60 \\end{aligned} \\] Thus, our best sample-based estimate of the variance of calmness scores in the population is \\(s_{people}^2 = 60.00\\). We averaged, or took the mean of, six sample based estimates to obtain \\(s_{people}^2\\). Most people refer to \\(s_{people}^2\\) as Mean Squared Error or MSE. You can think of Mean Squared Error as a synonym for estimated population variance. \\(\\text{Mean Squared Error} = MSE = s_{people}^2 = 60.0\\) CRITICAL: Note that averaging the sample variances estimates ONLY MAKES SENSE if we assume the six population variances are all the same. It only makes sense to average the sample variance estimates if they are all estimates of a single population value. Thus, calculation of Mean Squared Error only makes sense if the homogeneity of population variances assumption is true. Also note that when the cell sizes are unequal you need to created a weighted average of the sample variance estimates - this math is not reviewed in detail for the sake of a simple narrative. We now in the following situation: Notice the degrees of freedom for MSE. Recall that when we calculated MSE (i.e., \\(s_{people}^2\\)) we averaged 6 cell variance estimates (e.g., \\(s_1^2\\)). Each of those six estimates had \\(n-1\\) degrees for freedom. You know it’s \\(n-1\\) degrees of freedom for each cell variance (e.g., \\(s_1^2\\)) because that’s the denominator of the variance formula (\\(s^2 = \\frac{\\sum (x_i - \\bar{x}^2)}{n-1}\\)). So the degrees of freedom for \\(s_{people}^2\\) (i.e., MSE) is \\(6(n-1)\\) because there were six cell estimates we combined. Symbolically, we express the calculation of degrees of freedom as \\((3)(2)(n-1)\\) which is \\(ab(n-1)\\). So the degrees of freedom for \\(s_{people}^2\\) (i.e., MSE) is \\(ab(n-1)\\). It’s important to know this tidbit so that you will understand where all the numbers in a 2-way ANOVA table come from when we examine it later. 23.11 Random Sampling Variance As we learned in the 1-way ANOVA chapters, an important part of ANOVA is determining if the variability in sample means is greater than could be expected due to random sampling error. Consequently, we have to estimate how much random sampling error variance there would be (on average) in sample means for each of the three frames of reference (main effect of \\(A\\) means, main effect of \\(B\\) means, and cell means). To estimate the expected variability in sample means due to sampling error we need an estimate (\\(s_{people}^2\\)) of the population variance (\\(\\sigma_{people}^2\\)). That’s why we spent so much time obtaining \\(s_{people}^2\\) in the previous section. Recall the general form of the formula for the expected variance in sample means, due to sampling error, from previous chapters: \\[ \\begin{aligned} \\text{Sampling error variance for means} &amp;= \\sigma_{\\bar{x}}^2 \\\\ &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ \\end{aligned} \\] The formula above, for the variability due to sampling error alone, is based on the Central Limit Theorem (CLT) and is reviewed extensively in past chapters. 23.11.1 CLT \\(A\\): Juggling In the context of the juggling main effect means, each juggle mean is based on \\(nb=(5)(3)=15\\) people. Therefore, for these juggling main effect means the general formula for the variance due to sampling error is below: \\[ \\begin{aligned} \\text{Sampling error variance for means for A} &amp;= \\sigma_{\\bar{x}}^2 \\\\ &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ &amp;= \\frac{\\sigma_{people}^2}{nb}\\\\ \\end{aligned} \\] We don’t know the population variance (\\(\\sigma_{people}^2\\)) though, but we use an estimate below (\\(s_{people}^2\\)): \\[ \\begin{aligned} \\text{Estimated sampling error variance for means for A} &amp;= s_{\\bar{x}}^2\\\\ &amp;= \\frac{s_{people}^2}{nb}\\\\ &amp;= \\frac{s_{people}^2}{(5)(3)}\\\\ &amp;= \\frac{60.0}{15}\\\\ &amp;= 4.0 \\end{aligned} \\] Thus, when we have two juggling populations from which we obtained samples of 15 people, we would expect the variance of sample means to be 4.0; assuming the population means are the same. 23.11.2 CLT \\(B\\): Preparation In the context of the preparation main effect means, each preparation mean is based on \\(na=(5)(2)=10\\) people. Therefore, for these preparation main effect means the general formula for the variance due to sampling error is below: \\[ \\begin{aligned} \\text{Sampling error variance for means for B} &amp;= \\sigma_{\\bar{x}}^2 \\\\ &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ &amp;= \\frac{\\sigma_{people}^2}{na}\\\\ \\end{aligned} \\] We don’t know the population variance (\\(\\sigma_{people}^2\\)) though, but we use an estimate below (\\(s_{people}^2\\)): \\[ \\begin{aligned} \\text{Estimated sampling error variance for means for B} &amp;= s_{\\bar{x}}^2\\\\ &amp;= \\frac{s_{people}^2}{na}\\\\ &amp;= \\frac{s_{people}^2}{(5)(2)}\\\\ &amp;= \\frac{60.0}{10}\\\\ &amp;= 6.0 \\end{aligned} \\] Thus, when we have three preparation populations from which we obtained samples of 10 people, we would expect the variance of sample means to be 6.0; assuming the population means are the same. 23.11.3 CLT Cells In the context of the cell means, each cell mean is based on \\(n=5\\) people. Therefore, for these cell means the general formula for the variance due to sampling error is below: \\[ \\begin{aligned} \\text{Sampling error variance for cell means} = \\sigma_{\\bar{x}}^2 &amp;= \\frac{\\sigma_{people}^2}{\\text{sample size}}\\\\ &amp;= \\frac{\\sigma_{people}^2}{n}\\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Estimated sampling error variance for cell means} &amp;= s_{\\bar{x}}^2\\\\ &amp;= \\frac{s_{people}^2}{n}\\\\ &amp;= \\frac{60.0}{5}\\\\ &amp;= 12.0 \\end{aligned} \\] I omit the diagram for cells populations and sample mean distributions. This is done because there are so many different ways cells might be examined. There would be too many diagrams - and it would be misleading to present just one diagram. But don’t worry we will return to this issue, with diagrams, when we start talking about interactions in more detail. 23.11.4 CLT Summary You can see in the table above that we have estimated the variability we will see in the sample means for different effects. The estimated sampling error differs for each effect because the sample size differs for each effect. Notice that the degrees of freedom for the sampling error variance is the same for all the effects. This is because the degrees of freedom is based on the \\(s_{people}^2\\) calculation (i.e., MSE calculation). Sampling error variance is just \\(s_{people}^2\\) (i.e., MSE) divided by sample size. Consequently, the degrees for freedom for sampling error variance the same as MSE, \\(ab(n-1)\\). 23.12 Main Effect \\(A\\): Juggling 23.12.1 Total variance \\(A\\) We might wonder if there is an effect of juggling on calmness scores ignoring the effect of preparation. Said another way, we might wonder if the mean of the yes juggling population (\\(\\mu_{A1}\\)) is different from the mean of the no juggling population (\\(\\mu_{A2}\\)). We examine this question by looking the variance in the population means. If we knew the population means we could: Calculate the grand mean (i.e., \\(\\mu_A\\)). That is, the mean of the two juggling population means. Recall because there are two levels of juggling (i.e., two populations), \\(a = 2\\). Note that this is the same as the taking the mean of all participants from both populations (assuming equal population sizes). \\[ \\begin{aligned} \\text{Grand mean} = \\mu_A &amp;= \\frac{\\sum\\mu_{A_i}}{a} \\\\ &amp;= \\frac{\\mu_{A_1}+\\mu_{A_2}}{2}\\\\ &amp;= \\frac{? + ?}{2} \\end{aligned} \\] But, as the question marks clearly illustrate - we don’t know the population means. We don’t have everyone in the population. But if we did have the population means we could proceed as below. Use the grand mean to calculate the variance in the means: \\[ \\begin{aligned} \\sigma_{\\mu_A}^2 &amp;= \\frac{\\sum (\\mu_{A_i}-\\mu_{\\mu_A})^2}{a} \\\\ &amp;= \\frac{(\\mu_{A_1} - \\mu_{\\mu_A})^2 + (\\mu_{A_2} - \\mu_{\\mu_A})^2}{2} \\\\ &amp;= ?\\\\ \\end{aligned} \\] If the variance of the population means is zero than the populations means are the same. If the variance of the populations means is greater than zero than the population means are different. Unfortunately, we don’t have access to population-level means. So we need to rely upon sample-level information to estimate the variability in population means. You can check out the sample-level means in the diagram below. As we discussed previously. For the \\(A\\) independent variable, these means have a sample size of \\(15\\). We ESTIMATE the grand mean using the mean of the sample-level means. We refer to our grand mean estimate using \\(\\bar{bar{bar}}\\). There are two bars about the \\(x\\) to indicate that it is the mean of means. Note that this “grand mean” of participants is the same as taking the mean of all participants (assuming equal cell sizes). \\[ \\begin{aligned} \\text{Grand mean estimate} = \\bar{\\bar{x}} &amp;= \\frac{\\sum \\overline{x_{A_i}}}{a} \\\\ &amp;= \\frac{\\overline{x_{A_1}}+\\overline{x_{A_2}}}{2}\\\\ &amp;= \\frac{77.33333 + 55.8}{2} \\\\ &amp;= 66.56667\\\\ \\end{aligned} \\] Now estimate the total variance in populations means: \\[ \\begin{aligned} s_{\\mu_A}^2 &amp;= \\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1} \\\\ &amp;= \\frac{(\\overline{x_{A_1}} - \\bar{\\bar{x}})^2 + (\\overline{x_{A_2}} - \\bar{\\bar{x}})^2}{a-1} \\\\ &amp;= \\frac{(77.33333 - 66.56667)^2 + (55.8 - 66.56667)^2}{2-1} \\\\ &amp;= 231.8422 \\end{aligned} \\] When you estimate the variance is sample means this way (using the total variance formula) the number you obtain will reflect two things: Variability in sample means due to random sampling: \\(\\frac{\\sigma_{people}^2}{nb}\\) Variability in sample means due differences among the population means \\(\\sigma_{\\mu_A}^2\\) In mathematical terms we say the Expected Mean Square is: \\(\\text{Expected Mean Square} = \\frac{\\sigma_{people}^2}{nb} + \\sigma_{\\mu_A}^2\\) 23.12.2 \\(F\\)-value for \\(A\\) Recall that we now have two estimates for the variability in juggling means (total variance estimate, random sampling variance estimate). In the section immediately above, we obtained a variance \\(\\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1}=231.8422\\). This value reflects random sampling error variance and the variance of the population mean. Previously, we obtained a variance of estimate of \\(\\frac{s_{people}^2}{nb} = 4.0\\). This value reflects random sampling variance only. These two values are summarized in the table below. Juggling Formula Expected value degrees of freedom Estimated total variance \\(\\frac{\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}=231.8422\\) \\(\\frac{\\sigma_{people}^2}{nb} + \\sigma_{A}^2\\) \\((a-1) = 1\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{nb} = 4.0\\) \\(\\frac{\\sigma_{people}^2}{nb}\\) \\(ab(n-1) = 24\\) We calculate an \\(F\\)-value to compare these two values: \\[ \\begin{aligned} F &amp;= \\frac{\\text{estimated total variance}}{\\text{estimated variance due to random sampling}} \\\\ &amp;= \\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}}\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{Ai}}-\\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}}\\\\ &amp;= \\frac{\\frac{(\\overline{x_{A_1}} - \\bar{\\bar{x}})^2 + (\\overline{x_{A_2}} - \\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}}\\\\ &amp;=\\frac{\\frac{(77.33333 - 66.56667)^2 + (55.8 - 66.56667)^2}{2-1}}{\\frac{60}{(5)(3)}}\\\\ &amp;=\\frac{231.8422}{\\frac{60}{(5)(3)}}\\\\ &amp;=\\frac{231.8422}{\\frac{60}{15}}\\\\ &amp;=\\frac{231.8422}{4}\\\\ &amp;= 57.96\\\\ \\end{aligned} \\] 23.12.3 \\(p\\)-value for \\(A\\) This sample \\(F\\)-value tells us that, with these data, the estimated total variance of juggling means is 57.96 times larger than than we would estimate due to random sampling alone. This sounds like a large number - but it may not be. Notice the use of word estimate(d) in the previous sentences. We are looking at estimates that themselves may vary due to random sampling error. So we need to know how likely this is, when the population means are equal, in the context of the number of populations and the sample sizes. Recall Central \\(F\\)-distributions from a previous chapter. To interpret an \\(F\\)-value for the main effect of \\(A\\), you need to think about all the \\(F\\)-values that could have occurred - when the population means are equal (i.e., \\(\\sigma_{A}^2=0\\)). More specifically, we have to imagine all of the \\(F\\)-values with 1 and 24 degrees of freedom in the Central \\(F\\)-distribution. This distribution is illustrated below. It reflects, for this main effect, all the \\(F\\)-values that could occur when the populations means are equal - due to the fact we are using random samples. To assign a \\(p\\)-value to the \\(F\\)-value that we obtained, \\(F = 57.96\\) we determine the proportion of the Central \\(F\\)-distribution (with 1 and 24 degrees of freedom) that is equal to, or more extreme, than the \\(F\\)-value we obtained in our study. If you inspect the Central \\(F\\)-distribution above you can see that the current \\(F\\)-value of 57.96 doesn’t even fit on the graph - because the x-axis only goes to 10. We need a new graph with a wider range of values on the x-axis. This graph is presented below: In this graph you can see the \\(F\\)-value we obtained (57.96) is completely to the right of Central \\(F\\)-distribution. In fact, the proportion of the \\(F\\)-distribution that is equal to, or more extreme, than 57.96 is &lt; .001. So we say the \\(p\\)-value is \\(p &lt; .001\\). Consequently, under the assumption of equal population means, our data (or more extreme data) is very unlikely (\\(p &lt; .001\\)). Therefore, we reject the assumption that the population means (i.e., the population means for yes and no juggling) are equal. We conclude those population means may be different. To assign a \\(p\\)-value to the \\(F\\)-value that we obtained, \\(F = 57.96\\) we determine the proportion of the Central \\(F\\)-distribution (with 1 and 24 degrees of freedom) that is equal to, or more extreme, than the \\(F\\)-value we obtained in our study. If you inspect the Central \\(F\\)-distribution above you can see that the current \\(F\\)-value of 57.96 doesn’t even fit on the graph - because the x-axis only goes to 10. We need a new graph with a wider range of values on the x-axis. This graph is presented below: Remember the \\(p\\)-value is the probability of our data, or more extreme data, assuming the null hypothesis is true. WARNING: A small \\(p\\)-value (\\(p&lt;.001\\)) does NOT indicate that an effect is a large effect. If you want to know the size of the effect you need to look at an effect size (e.g., partial \\(\\eta^2\\) or partial \\(\\omega^2\\).) 23.12.4 \\(A\\) conceptual recap (juggling) A recap of the conceptual basis for the calculations we’ve done for juggling is provided below. In the next section we map these conceptual calculations onto the typical way ANOVA information is presented – which obscures the conceptual meaning. Juggling Formula Expected value degrees of freedom Estimated total variance \\(\\frac{\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}=231.8422\\) \\(\\frac{\\sigma_{people}^2}{nb} + \\sigma_{A}^2\\) \\((a-1) = 1\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{nb}=\\frac{60}{15} = 4.0\\) \\(\\frac{\\sigma_{people}^2}{nb}\\) \\(ab(n-1) = 24\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}}{ \\frac{s_{people}^2}{nb}}= \\frac{231.8422}{4.0}= 57.96\\) \\(\\frac{\\frac{\\sigma_{people}^2}{nb} + \\sigma_{A}^2}{ \\frac{\\sigma_{people}^2}{nb}}\\) \\([(a-1),ab(n-1)]=[1,24]\\) 23.12.5 \\(A\\) typical (juggling) The conceptual formula are algebraically rearranged in typical ANOVA output - due to the fact in the “old days” it made hand calculations somewhat easier. This typical rearrangement obscures the conceptual meaning of the calculations though. But it is relatively simply algebra (see below). Note, the \\(F\\)-value does not change. \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}}\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}} \\times 1\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1}}{\\frac{s_{people}^2}{nb}} \\times \\frac{nb}{nb}\\\\ &amp;= \\frac{nb\\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1}}{s_{people}^2}\\\\ &amp;= \\frac{(5)(3)(231.8422)}{60}\\\\ &amp;= \\frac{(15)(231.8422)}{60}\\\\ &amp;= \\frac{3477.633}{60}\\\\ &amp;= 57.96\\\\ \\end{aligned} \\] From this rearrangement we use the labels below. Notice the inclusion of \\(nb\\) in the \\(MS_{juggling}\\) this is hard to explain unless you first understood the conceptual version of the equations above. Indeed, people who tend to explain ANOVA using a “between vs within” explanation usually need to skip over or “brush away” the \\(nb\\) term in their explanations. \\(MS_{juggling} = nb\\frac{\\sum (\\overline{x_{A_i}}-\\bar{\\bar{x}})^2}{a-1} = 3477.633\\) and \\(MSE = s_{people}^2 = 60.0\\) Summarized below: Juggling Formula Expected value degrees of freedom Sample size multiplied by estimated total variance (i.e., \\(MS_{juggling}\\)) \\(\\frac{nb\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}=3477.633\\) \\(\\sigma_{people}^2 + nb \\sigma_{A}^2\\) \\((a-1)\\) Estimated population variance (i.e., \\(MSE\\)) \\(s_{people}^2=60.0\\) \\(\\sigma_{people}^2\\) \\(ab(n-1)\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{nb\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{a-1}}{s_{people}^2}=\\frac{3477.633}{60}=57.96\\) \\(\\frac{\\sigma_{people}^2 + nb \\sigma_{A}^2}{\\sigma_{people}^2}\\) \\([(a-1),ab(n-1)]\\) 23.13 Main Effect \\(B\\): Preparation 23.13.1 Total variance \\(B\\) Let’s look at the other independent variable: preparation. As before, we might wonder if there is an effect of preparation on calmness scores ignoring the effect of juggling. Said another way, we might wonder if the means of the three preparation populations (low [\\(\\mu_{B1}\\)], medium [\\(\\mu_{B2}\\)], high [\\(\\mu_{B3}\\)]) are different from each other. We examine this question by looking the variance in the population means. If we knew the population means we could: Calculate the grand mean (i.e., \\(\\mu_B\\)). That is, the mean of the three preparation population means. Recall because there are three levels of preparation (i.e., three populations), \\(b = 3\\). Note that this is the same as the taking the mean of all participants from the three populations (assuming equal population sizes). \\[ \\begin{aligned} \\text{Grand mean} = \\mu_B &amp;= \\frac{\\sum\\mu_{B_i}}{b} \\\\ &amp;= \\frac{\\mu_{B_1}+\\mu_{B_2}+\\mu_{B_3}}{3}\\\\ &amp;= \\frac{? + ? + ?}{3} \\end{aligned} \\] But, as the question marks clearly illustrate - we don’t know the population means. We don’t have everyone in the population. But if we did have the population means we could proceed as below. Use the grand mean to calculate the variance in the means: \\[ \\begin{aligned} \\sigma_{\\mu_B}^2 &amp;= \\frac{\\sum (\\mu_{B_i}-\\mu_{\\mu_B})^2}{b} \\\\ &amp;= \\frac{(\\mu_{B_1} - \\mu_{\\mu_B})^2 + (\\mu_{B_2} - \\mu_{\\mu_B})^2 (\\mu_{B_3} - \\mu_{\\mu_B})^2}{3} \\\\ &amp;= ?\\\\ \\end{aligned} \\] If the variance of the population means is zero than the populations means are the same. If the variance of the populations means is greater than zero than the population means are different. Unfortunately, we don’t have access to population-level means. So we need to rely upon sample-level information to estimate the variability in population means. You can check out the sample-level means in the diagram below. As we discussed previously. For the \\(B\\) independent variable, these means have a sample size of \\(10\\). We ESTIMATE the grand mean using the mean of the sample-level means. We refer to our grand mean estimate using \\(\\bar{bar{bar}}\\). There are two bars about the \\(x\\) to indicate that it is the mean of means. Note that this “grand mean” of participants is the same as taking the mean of all participants (assuming equal cell sizes). \\[ \\begin{aligned} \\text{Grand mean estimate} = \\bar{\\bar{x}} &amp;= \\frac{\\sum \\bar{x}_{B_i}}{b} \\\\ &amp;= \\frac{\\overline{x_{B_1}}+\\overline{x_{B_2}} +\\overline{x_{B_3}} }{3}\\\\ &amp;= \\frac{65.4 + 62.6 + 71.7}{3} \\\\ &amp;= 66.56667\\\\ \\end{aligned} \\] Now estimate the total variance in populations means: \\[ \\begin{aligned} s_{\\mu_B}^2 &amp;= \\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1} \\\\ &amp;= \\frac{(\\overline{x_{B_1}} - \\bar{\\bar{x}})^2 + (\\overline{x_{B_2}} - \\bar{\\bar{x}})^2+ (\\overline{x_{B_3}} - \\bar{\\bar{x}})^2}{b-1} \\\\ &amp;= \\frac{(65.4 - 66.56667)^2 + (62.6 - 66.56667)^2 + (71.7 - 66.56667)^2}{3-1} \\\\ &amp;= 21.72333 \\end{aligned} \\] When you estimate the variance is sample means this way (using the total variance formula) the number you obtain will reflect two things: Variability in sample means due to random sampling: \\(\\frac{\\sigma_{people}^2}{na}\\) Variability in sample means due differences among the population means \\(\\sigma_{\\mu_B}^2\\) In mathematical terms we say the Expected Mean Square is: \\(\\text{Expected Mean Square} = \\frac{\\sigma_{people}^2}{na} + \\sigma_{\\mu_B}^2\\) 23.13.2 \\(F\\)-value for \\(B\\) Recall that we now have two estimates for the variability in juggling means (total variance estimate, random sampling variance estimate). In the section immediately above, we obtained a variance \\(\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1}=21.72333\\). This value reflects random sampling error variance and the variance of the population mean. Previously, we obtained a variance of estimate of \\(\\frac{s_{people}^2}{na} = 6.0\\). This value reflects random sampling variance only. These two values are summarized in the table below. Juggling Formula Expected value degrees of freedom Estimated total variance \\(\\frac{\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}=21.72333\\) \\(\\frac{\\sigma_{people}^2}{na} + \\sigma_{B}^2\\) \\((b-1) = 2\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{na} = 6.0\\) \\(\\frac{\\sigma_{people}^2}{na}\\) \\(ab(n-1) = 24\\) We calculate an \\(F\\)-value to compare these two values: \\[ \\begin{aligned} F &amp;= \\frac{\\text{estimated total variance}}{\\text{estimated variance due to random sampling}} \\\\ &amp;= \\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}}\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{Bi}}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}}\\\\ &amp;= \\frac{\\frac{(\\overline{x_{B_1}} - \\bar{\\bar{x}})^2 + (\\overline{x_{B_2}} - \\bar{\\bar{x}})^2 + (\\overline{x_{B_3}} - \\bar{\\bar{x}})^2 }{b-1}}{\\frac{s_{people}^2}{na}}\\\\ &amp;=\\frac{\\frac{(65.4 - 66.56667)^2 + (62.6 - 66.56667)^2 + (71.7 - 66.56667)^2}{3-1}}{\\frac{60}{(5)(2)}}\\\\ &amp;=\\frac{21.72333}{\\frac{60}{(5)(2)}}\\\\ &amp;=\\frac{21.72333}{\\frac{60}{10}}\\\\ &amp;=\\frac{21.72333}{6}\\\\ &amp;= 3.62\\\\ \\end{aligned} \\] 23.13.3 \\(p\\)-value for \\(B\\) This sample \\(F\\)-value tells us that, with these data, the estimated total variance of juggling means is 3.62 times larger than than we would estimate due to random sampling alone. Is this large or small? It’s hard to tell. As well, notice the use of word estimate(d) in the previous sentences. We are looking at estimates that themselves may vary due to random sampling error. So we need to know how likely this is, when the population means are equal, in the context of the number of populations and the sample sizes. Recall Central \\(F\\)-distributions from a previous chapter. To interpret an \\(F\\)-value for the main effect of \\(B\\), you need to think about all the \\(F\\)-values that could have occurred - when the population means are equal (i.e., \\(\\sigma_{B}^2=0\\)). More specifically, we have to imagine all of the \\(F\\)-values with 2 and 24 degrees of freedom in the Central \\(F\\)-distribution. This distribution is illustrated below. It reflects, for this main effect, all the \\(F\\)-values that could occur when the populations means are equal - due to the fact we are using random samples. To assign a \\(p\\)-value to the \\(F\\)-value that we obtained, \\(F = 3.62\\) we determine the proportion of the Central \\(F\\)-distribution (with 2 and 24 degrees of freedom) that is equal to, or more extreme, than the \\(F\\)-value we obtained in our study. The \\(F\\)-value we obtained is position in Central \\(F\\)-distribution below. In the graph above you can see the \\(F\\)-value we obtained (3.62) is somewhat to the right side of the Central \\(F\\)-distribution. In fact, the proportion of the \\(F\\)-distribution that is equal to, or more extreme, than 3.62 is .042 (i.e. 4.2%). So we say the \\(p\\)-value is \\(p = .042\\). Consequently, under the assumption of equal population means, our data (or more extreme data) is very unlikely (\\(p = .042\\)). Therefore, we reject the assumption that the population means (i.e., the population means for low, medium, and high preparation) are equal. We conclude those population means may be different. Remember the \\(p\\)-value is the probability of our data, or more extreme data, assuming the null hypothesis is true. 23.13.4 \\(B\\) conceptual recap (preparation) A recap of the conceptual basis for the calculations we’ve done for preparation is provided below. In the next section we map these conceptual calculations onto the typical way ANOVA information is presented – which obscures the conceptual meaning. Preparation Formula Expected value degrees of freedom Estimated total variance \\(\\frac{\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}=21.72333\\) \\(\\frac{\\sigma_{people}^2}{na} + \\sigma_{B}^2\\) \\((b-1)\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{na}= \\frac{60}{10}=6.0\\) \\(\\frac{\\sigma_{people}^2}{na}\\) \\(ab(n-1)\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}}{ \\frac{s_{people}^2}{na}} = \\frac{21.72333}{6.0}=3.62\\) \\(\\frac{\\frac{\\sigma_{people}^2}{na} + \\sigma_{B}^2}{ \\frac{\\sigma_{people}^2}{na}}\\) \\([(b-1),ab(n-1)\\) 23.13.5 \\(B\\) typical (preparation) The conceptual formula are algebraically rearranged in typical ANOVA output - due to the fact in the “old days” it made hand calculations somewhat easier. This typical rearrangement obscures the conceptual meaning of the calculations though. But it is relatively simply algebra (see below). Note, the \\(F\\)-value does not change. \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\sum (\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}}\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}}\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}} \\times 1\\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1}}{\\frac{s_{people}^2}{na}} \\times \\frac{na}{na}\\\\ &amp;= \\frac{na\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1}}{s_{people}^2}\\\\ &amp;= \\frac{(5)(2)(21.72333)}{60}\\\\ &amp;= \\frac{(10)(21.72333)}{60}\\\\ &amp;= \\frac{217.2333}{60}\\\\ &amp;= 3.62\\\\ \\end{aligned} \\] From this rearrangement we use the labels below. Notice the inclusion of \\(na\\) in the \\(MS_{preparation}\\) this is hard to explain unless you first understood the conceptual version of the equations above. Indeed, people who tend to explain ANOVA using a “between vs within” explanation usually need to skip over or “brush away” the \\(na\\) term in their explanations. \\(MS_{preparation} = na\\frac{\\sum (\\overline{x_{B_i}}-\\bar{\\bar{x}})^2}{b-1} = 217.2333\\) and \\(MSE = s_{people}^2 = 60.0\\) Summarized below: Preparation Formula Expected value degrees of freedom Sample size multiplied by estimated total variance (i.e., \\(MS_{preparation}\\)) \\(\\frac{na\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}= 217.2333\\) \\(\\sigma_{people}^2 + na \\sigma_{B}^2\\) \\((b-1)\\) Estimated population variance (i.e., \\(MSE\\)) \\(s_{people}^2=60.0\\) \\(\\sigma_{people}^2\\) \\(ab(n-1)\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{na\\sum(\\bar{x_i}-\\bar{\\bar{x}})^2}{b-1}}{s_{people}^2} = \\frac{217.2333}{60}=3.62\\) \\(\\frac{\\sigma_{people}^2 + na \\sigma_{B}^2}{\\sigma_{people}^2}\\) \\([(b-1),ab(n-1)]\\) 23.14 Interaction Preparation Interaction calculations are similar to main effect calculations. However, there is a substantial amount of work we need to do before being able to do those calculations. When we say two variable interact to predict the dependent variable we are saying there is more going on than the two main effects. So when we seek to determine if there is an interaction we work with residual cell means. That is, cell means were the main effects have been removed. That’s the work we need to do before proceeding with the calculations used above. In the section below we create the residual means. In the subsequent section we return to the familiar calculations we used for main effects. 23.14.1 Obtaining residualized means We begin with the cell means: 23.14.1.1 Subtract \\(A\\) marginal means The cell means are illustrated below. Recall the overall marginal mean for yes_juggling is 77.33 and the overall marginal mean for no_juggling is 55.8. Be begin by removing these main effect means for \\(A\\) from the marginal means. Effect low_prep medium_prep high_prep yes_juggle \\(71.4 - 77.33 = -5.93\\) \\(72.6 - 77.33 = -4.73\\) \\(88 - 77.33 = 10.67\\) no_juggle \\(59.4 - 55.8 = 3.6\\) \\(52.6 - 55.8 = -3.2\\) \\(55.4 - 55.8 = -0.4\\) 23.14.1.2 Subtract \\(B\\) marginal means Next, we take the residual. means from the previous step and remove the main effect of \\(B\\). Recall the marginal means for low, medium, and high preparation were 55.8, 62.6, and 71.7, respectively. We subtract these main effect means from the residual. means from the previous step. Effect low_prep medium_prep high_prep yes_juggle \\(-5.93 - 65.4 = -71.33\\) \\(-4.73 - 62.6 = -67.33\\) \\(10.67 - 71.7 = -61.03\\) no_juggle \\(3.6 - 65.4 = -61.8\\) \\(-3.2 - 62.6 = -65.8\\) \\(-0.4 - 71.7 = -72.1\\) 23.14.1.3 Add grand mean In the previous steps, we removed the main effect means for \\(A\\) and then we removed the main effect means for \\(B\\). In effect, though each time we did this we removed the “grand mean” twice. We only want to remove the grand mean once though - because this makes so later calculations easier. So now we add the grand mean back in: Effect low_prep medium_prep high_prep yes_juggle \\(-71.33 + 66.57 = -4.77\\) \\(-67.33 + 66.57 = -0.77\\) \\(-61.03 + 66.57 = 5.53\\) no_juggle \\(-61.8 + 66.57 = 4.77\\) \\(-65.8 + 66.57 = 0.77\\) \\(-72.1 + 66.57 = -5.53\\) 23.14.1.4 Final residualized means The final residual cell means we use for the interaction calculation are below: Effect low_prep medium_prep high_prep yes_juggle \\(-4.77\\) \\(-0.77\\) \\(5.53\\) no_juggle \\(4.77\\) \\(0.77\\) \\(-5.53\\) 23.15 Interaction (\\(AB\\)) 23.15.1 Total variance \\(AB\\) The residual cell means calculated in the previous section are presented below – as a starting point for the interaction calculations. It’s important to remember that the above table of residual cell means is a table of sample-level residual means. That means, the values in the cells are influenced by both the population means (for the cell and the main effects) as well as sampling error. Consider two situations, at the population-level, where: 1) There is no interaction, only main effects 2) There is an interaction Imagine the population-level scenario where there is no interaction and only main effects. In this scenario, the values in the population-level cells are entirely the result of the main effects. Further imagine that we we subtracted out the main effects – and created population-level residual means (as we did above, but at the population-level). In this scenario, the population-level residual cells means would all have a value of exactly zero. That value is zero, because the only thing influence scores is the main effects and when the main effects are removed there’s nothing left. Now consider the population-level scenario where there is an interaction. In this case, when we create residual population-level cell means, they would not all be equal to zero. That’s because there is more going on than the main effects. The interpretation challenge occurs when we move to the sample level because the sample-level residuals may differ, due to random sampling, from the population-residuals. To help distinguish between these two situations we calculate a \\(p\\)-value. The \\(p\\)-value tells us how likely we are to have sample-level cell means this large (or larger) if the population-level residual cell means are zero. 23.15.1.1 Degrees of freedom for \\(AB\\) total variance Recall how we calculated the total number of cells. There were two levels of juggling so \\(a = 2\\). There were three levels of preparation so \\(b = 3\\). \\(\\text{Total number of cells} = ab = (2)(3) = 6\\) We use the same logic when determining the degrees of freedom for the interaction. The degrees of freedom for juggling is \\(a-1\\) where as the degrees of freedom for preparation are \\(b-1\\). We use the same general approach to the total degrees of freedom: \\(\\text{Degrees of freedom for total variance} = (a-1)(b-1)=(2-1)(3-1)=(1)(2)=2\\) We use this degrees of freedom in the total variance formula. This is the same total variance formula we’ve used before but adjusted to use the residual cell means. \\[ s_{residual}^2 = \\frac{\\sum (\\overline{x_{residual_i}} - \\overline{\\overline{x_{residual}}})^2}{(a-1)(b-1)} \\] We apply the formula above the means in the residual mean table. These are the cell means with the main effects removed. The estimated population variance is: \\[ \\begin{aligned} s_{residual}^2 &amp;= \\frac{\\sum (\\overline{x_{residual_i}} - \\overline{\\overline{x_{residual}}})^2}{(a-1)(b-1)}\\\\ &amp;= \\frac{(-4.77 - 0)^2 + (-0.77 - 0)^2 + (5.53 - 0)^2 + (4.77 - 0)^2 + (0.77 - 0)^2 + (-5.53 - 0)^2}{(2-1)(3-1)} \\\\ &amp;= \\frac{(-4.77)^2 + (-0.77)^2 + (5.53)^2 + (4.77)^2 + (0.77)^2 + (-5.53)^2}{(2-1)(3-1)} \\\\ &amp;= \\frac{107.85}{2} &amp;= 53.92667 \\end{aligned} \\] Each residual mean is based on \\(n=5\\). Therefore, the estimated variance due to sampling error is: \\[ \\frac{s_{people}^2}{n} \\] 23.15.2 \\(F\\)-value for \\(AB\\) So F-ratio comparing these two estimates is: \\[ \\begin{aligned} F &amp;= \\frac{\\text{estimated total variance}}{\\text{estimated variance due to random sampling}} \\\\ &amp;= \\frac{\\frac{\\sum (\\overline{x_{residual_i}} - \\overline{\\overline{x_{residual}}})^2}{(a-1)(b-1)}}{\\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{53.92667}{\\frac{60}{5}}\\\\ &amp;= 4.49\\\\ \\end{aligned} \\] 23.15.3 \\(p\\)-value for \\(AB\\) This sample \\(F\\)-value tells us that, with these data, the estimated total variance of juggling means is 4.49 times larger than than we would estimate due to random sampling alone. Is this large or small? It’s hard to tell. As well, notice the use of word estimate(d) in the previous sentences. We are looking at estimates that themselves may vary due to random sampling error. So we need to know how likely this is, when the population means are equal, in the context of the number of populations and the sample sizes. Recall Central \\(F\\)-distributions from a previous chapter. To interpret an \\(F\\)-value for the main effect of \\(AB\\), you need to think about all the \\(F\\)-values that could have occurred - when the population means are equal (i.e., \\(\\sigma_{AB}^2=0\\)). More specifically, we have to imagine all of the \\(F\\)-values with 2 and 24 degrees of freedom in the Central \\(F\\)-distribution. This distribution is illustrated below: To assign a \\(p\\)-value to the \\(F\\)-value that we obtained, \\(F = 4.49\\) we determine the proportion of the Central \\(F\\)-distribution (with 2 and 24 degrees of freedom) that is equal to, or more extreme, than the \\(F\\)-value we obtained in our study. The \\(F\\)-value we obtained is positioned in Central \\(F\\)-distribution below. In the graph above you can see the \\(F\\)-value we obtained (4.49) is somewhat to the right side of the Central \\(F\\)-distribution. In fact, the proportion of the \\(F\\)-distribution that is equal to, or more extreme, than 4.49 is .022 (i.e. 2.2%). So we say the \\(p\\)-value is \\(p = .022\\). Consequently, under the assumption of equal residual population means, our data (or more extreme data) is very unlikely (\\(p = .022\\)). Therefore, we reject the assumption that the population residual means are equal. We conclude those population means may be different. Which implies there is an interaction. When there is an interaction you are concluding the two independent variables combine to influence the dependent variable. In our example study, you could say the impact of juggling on calmness before the exam depends on how much students prepared. Or you also say the impact of preparation on calmness before the exam depends on whether students juggled or not during breaks. Remember the \\(p\\)-value is the probability of our data, or more extreme data, assuming the null hypothesis is true. 23.15.4 \\(AB\\) conceptual (interaction) Preparation Formula Expected value degrees of freedom Estimated total variance \\(\\frac{\\sum (\\overline{x_{residual_i}})^2}{(a-1)(b-1)} = 53.92667\\) \\(\\frac{\\sigma_{people}^2}{n} + \\sigma_{AB}^2\\) \\((a-1)(b-1)\\) Estimated variance sampling error \\(\\frac{s_{people}^2}{n}=\\frac{60}{5}=12\\) \\(\\frac{\\sigma_{people}^2}{n}\\) \\(ab(n-1)\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{\\sum (\\overline{x_{residual_i}} )^2}{(a-1)(b-1)}}{ \\frac{s_{people}^2}{n}}=\\frac{53.92667}{12}=4.49\\) \\(\\frac{\\frac{\\sigma_{people}^2}{n} + \\sigma_{AB}^2}{ \\frac{\\sigma_{people}^2}{n}}\\) \\([(a-1)(b-1),ab(n-1)]\\) 23.15.5 \\(AB\\) typical (interaction) Juggling Formula Expected value degrees of freedom Sample size multiplied by estimated total variance \\(\\frac{n\\sum (\\overline{x_{residual_i}})^2}{(a-1)(b-1)}\\) \\(\\sigma_{people}^2 + n \\sigma_{AB}^2\\) \\((a-1)(b-1)\\) Estimated population variance \\(s_{people}^2\\) \\(\\sigma_{people}^2\\) \\(ab(n-1)\\) Comparison using \\(F\\)-ratio \\(\\frac{\\frac{\\sum (\\overline{x_{residual_i}})^2}{(a-1)(b-1)}}{s_{people}^2}\\) \\(\\frac{\\sigma_{people}^2 + n \\sigma_{AB}^2}{\\sigma_{people}^2}\\) \\([(a-1)(b-1),ab(n-1)]\\) "],["n-way-cell-comparisons.html", "Chapter 24 N-way Cell Comparisons 24.1 Recap and next directions 24.2 Confirmatory Comparisons", " Chapter 24 N-way Cell Comparisons 24.1 Recap and next directions In the last chapter we focused on the ANOVA itself. In this chapter we focus on comparing the means of cells within the ANOVA. These comparisons could be: planned comparisons based on hypotheses (i.e., confirmatory or a priori comparisons) or exploratory comparisons to investigate an interaction. Previously we conducted an N-way ANOVA, where juggling (yes/no) and preparation (low/medium/high) were used to predict calmness scores before an exam. The analysis we conducted is repeated below: library(tidyverse) library(apaTables) nway_data = read_csv(&quot;data_crf.csv&quot;) # Turn character columns into factors nway_data &lt;- nway_data %&gt;% mutate_if(is.character, as.factor) Next we conduct the analysis: # set contrasts to values match SPSS options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # Conduct analysis lm_nway &lt;- lm(calm ~ preparation*juggling, data = nway_data) Then we create the output using: apa.aov.table(lm_nway, filename = &quot;my_table.doc&quot;) 24.2 Confirmatory Comparisons 24.2.1 \\(t\\)-test If you have a comparison (with 2 means) that was planned prior to collecting data, one option to test the comparison is the \\(t\\)-test. This is perhaps the most frequently used approach in some research areas. Consider a scenario where you have a directional hypothesis (i.e., you believe on cell mean is higher than an other). This is also known as a one-sided test. In this particular scenario, your planned directional comparison involves comparing cell 1 (yes juggling/low preparation) with cell 3 (yes juggling / high preparation). You believe the mean for cell 3 will be higher than cell 1. That is, you have a directional hypothesis (i.e., one-tailed or one-sided hypothesis). This belief is based on theory and you registered your hypothesis at osf.io prior to collecting data. 24.2.1.1 \\(t\\)-test statistic You could proceed as follows. Obtain the data for these two cells: juggling_high_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;high&quot;)%&gt;% pull(calm) juggling_low_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;low&quot;)%&gt;% pull(calm) Conduct a two-sided \\(t\\)-test with the formula (equal variances assumed): \\[ t = \\frac{\\bar{x_3} - \\bar{x_1}}{\\sqrt{\\frac{s_{people}^2}{n_1}+\\frac{s_{people}^2}{n_2}}} \\] Note that when we conduct a \\(t\\)-test the estimate of the population variance of calmness scores ($ s_{people}^2$) is based on only the two cells involved in the comparison (see below). Recall though that in the ANOVA this estimate was based on all six cells. \\[ s_{people}^2 = \\frac{s_1^2 + s_2^2}{2} \\] Using the code below we make it a two-sided test to obtain the CI in format typically used in psychology. We convert it to a one-sided \\(t\\)-test below. tout&lt;- t.test(juggling_high_preparation, juggling_low_preparation, var.equal = TRUE, alternative = &quot;two.sided&quot;) print(tout) ## ## Two Sample t-test ## ## data: juggling_high_preparation and juggling_low_preparation ## t = 3.8, df = 8, p-value = 0.005 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 6.454 26.746 ## sample estimates: ## mean of x mean of y ## 88.0 71.4 24.2.1.2 \\(t\\)-test p-value Divide the two-sided \\(p\\)-value by 2 to obtain the one-sided or directional \\(p\\)-value. \\(0.005444/2=0.002722\\) Your output text now stands as: Among jugglers, high preparation students (\\(M = 88.00\\), \\(SD = 8.15\\)) were calmer, on average, (\\(M_{\\text{diff}} = 16.6\\), 95% \\(CI[6.45, 26.75]\\)) than low preparation students,(\\(M = 71.40\\), \\(SD = 5.50\\)), \\(t(8)=3.77\\), \\(p = .003\\). Notably, we are missing a standardized effect size, \\(d\\)-value with confidence interval, for the above. 24.2.1.3 \\(t\\)-test effect size We can calculate the \\(d\\)-value with a CI for this comparison using the formula below. \\[ d = \\frac{\\bar{x_3} - \\bar{x_1}}{s_{people}} \\] Not that in this formula, \\(s_{people}\\), was based on the two cells only. To obtain this effect size in R, we use t-test value as the non-centralty parameter in this command: library(MBESS) ci.smd(ncp = tout$statistic, n.1 = 5, n.2 = 5) ## $Lower.Conf.Limit.smd ## [1] 0.6578 ## ## $smd ## t ## 2.386 ## ## $Upper.Conf.Limit.smd ## [1] 4.038 # this is the came as typing: # ci.smd(ncp = 3.77, n.1 = 5, n.2 = 5) 24.2.1.4 \\(t\\)-test text Among jugglers, high preparation students (\\(M = 88.00\\), \\(SD = 8.15\\)) were calmer, on average, (\\(M_{\\text{diff}} = 16.6\\), 95% \\(CI[6.45, 26.75]\\), \\(d = 2.39\\), 95% \\(CI[0.66, 4.04\\)]) than low preparation students,(\\(M = 71.40\\), \\(SD = 5.50\\)), \\(t(8)=3.77\\), \\(p = .003\\). Approach pooled variance test effect size \\(t\\)-test (equal variances assumed) \\(s_{people}^2 = \\frac{s_1^2 + s_2^2}{2}\\) \\(t = \\frac{\\bar{x_3} - \\bar{x_1}}{\\sqrt{\\frac{s_{people}^2}{n_1}+\\frac{s_{people}^2}{n_2}}}\\) \\(d = \\frac{\\bar{x_3} - \\bar{x_1}}{s_{people}}\\) 24.2.2 contrast (2 means) As we did with the t-test, consider a scenario where you planned a directional comparison involves comparing cell 1 (yes juggling/low preparation) with cell 3 (yes juggling / high preparation). You believe the mean for cell 3 will be higher than cell 1. That is, you have a directional hypothesis (i.e., one-tailed or one-sided hypothesis). This belief is based on theory and you registered your hypothesis at osf.io prior to collecting data. Although the t-test is a popular approach for comparing two cell means in an ANOVA, a contrast is actually more consistent with the underlying ANOVA framework. The big difference between a t-test and a contrast is how they calculate the pooled variance term. In fact, that’s the only difference. With a t-test the population variance of people’s calmness scores is estimated using only the two cells involved in the comparison. The contrast, the contrast-test, takes a different approach. Even though only two cell means are being compared in a contrast, the population variance of people’s calmness scores is estimated using all the cells in the ANOVA. Let’s walk thought the calculation process below. It initially seems more different than it is because it’s an F-test. But we’ll see how this is a superficial differences as we go through the process. We begin by obtaining the sample data for these cells juggling_high_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;high&quot;)%&gt;% pull(calm) juggling_low_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;low&quot;)%&gt;% pull(calm) 24.2.2.1 contrast statistic As noted, a contrast differs from a t-test primarily because of how the pooled variance term is calculated. A t-test only uses the two cells involved in the comparison to estimate \\(s_{people}^2\\). A contrast uses a different approach. It uses all 6 cells from the ANOVA to estimate \\(s_{people}^2\\). \\[ s_{people}^2 = \\frac{s_1^2 + s_2^2 +s_3^2+s_4^2+s_5^2+s_6^2}{6} = MSE = 60 \\] Notice how, below, the calculation of this \\(F\\)-value below follows the general logic we have used with \\(F\\)-values all along. The estimate of the total variance is divided by the estimate of the random sampling variance. We begin by referring to the means we will use in the contrast as contrast means. That is, instead of using the original lables (\\(\\bar{x_1}\\) and \\(\\bar{x_3}\\)}) we use new labels with the indices 1 and 2 to refer to the first and second means involved in the contrast. The relabelling is described below: We use \\(\\bar{c_1}\\) to refer to the the mean in cell 1 (juggling yes, low preparation). \\[ \\bar{c_1} = \\bar{x_1} = 71.4 \\] We use \\(\\bar{c_2}\\) to refer to the the mean in cell 3 (juggling yes, high preparation). \\[ \\bar{c_2} = \\bar{x_3} = 88 \\] We imagine there are only these two populations ([juggling yes, low preparation],[juggling yes, high preparation]) and there there is a population-level grand mean create by averaging the two population means. We estimate that population-level grand mean using sample-level data: \\[ \\begin{aligned} \\bar{\\bar{c}} &amp;= \\frac{\\sum \\bar{c_i}}{2} \\\\ &amp;= \\frac{\\bar{c_1}+\\bar{c_2}}{2}\\\\ &amp;= \\frac{71.4 + 88}{2} \\\\ &amp;= 79.7\\\\ \\end{aligned} \\] We estimate the total variance (as we did with the ANOVA): \\[ \\begin{aligned} \\text{total variance} &amp;= \\frac{\\sum (\\bar{c_{i}}-\\bar{\\bar{c}})^2}{2-1} \\\\ &amp;= \\frac{(\\bar{c_{1}} - \\bar{\\bar{c}})^2 + (\\bar{c_{2}} - \\bar{\\bar{c}})^2}{2-1} \\\\ &amp;= \\frac{(71.4- 79.7)^2 + (88 - 79.7)^2}{2-1} \\\\ &amp;= 137.78 \\end{aligned} \\] We estimate variance due to random sampling: \\[ \\frac{s_{people}^2}{n} = \\frac{MSE}{n} = \\frac{60}{5} = 12 \\] And then we compare these two estimates with the F-ratio: \\[ F = \\frac{\\frac{\\sum (\\bar{x_i} - \\bar{\\bar{x}})^2}{2-1}}{\\frac{s_{people}^2}{n}} \\] This \\(F\\)-value formula looks very different from the \\(t\\)-test formula we used previously. But we can rearrange the formula above to make it look like a t-test. \\[ F = \\left[ \\frac{\\bar{x_3} - \\bar{x_1}}{\\sqrt{\\frac{s_{people}^2}{n_1}+\\frac{s_{people}^2}{n_2}}} \\right] ^2 \\] When it is rearranged, you can see the \\(F = t^2\\). This rearrangement only works when there are two means. The important point is though that you can see that really the \\(t\\)-test and the contrast are the same except for the way the pooled variance term is calculated. The t-test used a pooled variance estimate based on the two cells involved in the comparison whereas the contrast used a pooled variance estimate based on all the cells in the ANOVA. In R, we can calculate the contrast with the code below. mean_cell1 = mean(juggling_low_preparation, na.rm = TRUE) mean_cell3 = mean(juggling_high_preparation, na.rm = TRUE) means_to_compare = c(mean_cell3, mean_cell1) mse = (summary(lm_nway)$sigma)^2 # or just 60 for MSE df1 = 2 -1 # two means - 1 df2 = 24 # degrees of freedom for MSE n = 5 # 5 per cell Fvalue = var(means_to_compare)/(mse/n) print(Fvalue) ## [1] 11.48 24.2.2.2 contrast p-value pvalue_nondirectional = pf(Fvalue, df1 = df1, df2 = df2, lower.tail = FALSE) pvalue_directional = pvalue_nondirectional/2 print(pvalue_directional) ## [1] 0.001213 Your output text now stands as: Among jugglers, high preparation students (\\(M = 88.00\\), \\(SD = 8.15\\)) were calmer (\\(M_{\\text{diff}} = 16.60\\)) than low preparation students,(\\(M = 71.40\\), \\(SD = 5.50\\)), \\(F(1, 24)= 11.482\\), \\(p = .001\\). Notably, we are missing a standardized effect size, with confidence interval, for the above. 24.2.2.3 contrast effect size We can calculate an effect size with a CI for this comparison using the formula below. \\[ sc = \\frac{\\bar{c_2} - \\bar{c_1}}{s_{people}} \\] Again, notice the only difference from a d-value is that we calculated \\(s_{people}\\) based on 6 cells instead of 2. Consequently, some people call this value a \\(d\\)-value. So in this calculation, the denominator is determined: By calculating \\(s_{people}^2\\): \\[ s_{people}^2 = \\frac{s_1^2 + s_2^2 +s_3^2+s_4^2+s_5^2+s_6^2}{6} = MSE = 60 \\] Turning it into \\(s_{people}\\): \\[ s_{people}=\\sqrt{s_{people}^2}=\\sqrt{MSE}=\\sqrt{60} \\] Then using the resulting value in the calculation \\[ sc = \\frac{88 - 71.4}{\\sqrt{60}} = 2.14 \\] In R, we use the code below to obtain the effect size: n_per_cell = 5 mse_n = 2*3*n_per_cell mean_cell1 = mean(juggling_low_preparation, na.rm = TRUE) mean_cell3 = mean(juggling_high_preparation, na.rm = TRUE) mean_c1 = mean_cell1 mean_c2 = mean_cell3 means_to_compare = c(mean_c2, mean_c1) n_for_means = c(n_per_cell, n_per_cell) comparison_weights = c(1, -1) # Approach 1 to s.anova mse = 60 #s2_people s_people = sqrt(mse) # Approach 2 to s.anova # re-use lm_nway from initial ANOVA s_people = summary(lm_nway)$sigma # Calculate standardizec contrast with CI ci.sc(means = means_to_compare, c.weights = comparison_weights, n = rep(n_per_cell, 2), s.anova = s_people, N = mse_n) ## $Lower.Conf.Limit.Standardized.Contrast ## [1] 0.7674 ## ## $Standardized.contrast ## [1] 2.143 ## ## $Upper.Conf.Limit.Standardized.Contrast ## [1] 3.487 24.2.2.4 contrast text Among jugglers, high preparation students (\\(M = 88.00\\), \\(SD = 8.15\\)) were calmer (\\(M_{\\text{diff}} = 16.60\\), \\(sc = 2.14\\), 95% \\(CI[0.77, 3.49\\)]) than low preparation students,(\\(M = 71.40\\), \\(SD = 5.50\\)), \\(F(1, 24)= 11.482\\), \\(p = .001\\). But, as noted, some people might just write this up calling the effect size a \\(d\\)-value: Among jugglers, high preparation students (\\(M = 88.00\\), \\(SD = 8.15\\)) were calmer (\\(M_{\\text{diff}} = 16.60\\), \\(d = 2.14\\), 95% \\(CI[0.77, 3.49\\)]) than low preparation students,(\\(M = 71.40\\), \\(SD = 5.50\\)), \\(F(1, 24)= 11.482\\), \\(p = .001\\). The fact that you could calculate a \\(d\\)-value in two different ways (using difference variance calculations) illustrates why it is extremely important that you indicate in your paper (even in a footnote) the formula you used to calculate the \\(d\\)-value and pooled variance terms. 24.2.2.5 Comparison with \\(t\\)-test Approach pooled variance test effect size contrast (equal variances assumed) \\(s_{people}^2 = \\frac{s_1^2 + s_2^2 +s_3^2+s_4^2+s_5^2+s_6^2}{6}\\) \\(F = \\frac{\\frac{\\sum (\\bar{x_i} - \\bar{\\bar{x}})^2}{2-1}}{\\frac{s_{people}^2}{n}}\\) \\(sc = \\frac{\\bar{x_3} - \\bar{x_1}}{s_{people}}\\) \\(t\\)-test (equal variances assumed) \\(s_{people}^2 = \\frac{s_1^2 + s_2^2}{2}\\) \\(t = \\frac{\\bar{x_3} \\bar{x_1}}{\\sqrt{\\frac{s_{people}^2}{n_1}+\\frac{s_{people}^2}{n_2}}}\\) \\(d = \\frac{\\bar{x_3} - \\bar{x_1}}{s_{people}}\\) You can see from the above table that when you are comparing two means the contrast and \\(t\\)-test approaches are very similar. The only difference between the two approaches is how the population variance estimate (\\(s_{people}^2\\)) is calculated. 24.2.2.6 \\(F = t^2\\) In case you wanted to understand how and F-value and t-value can be the same with two groups here is the proof: \\[ \\begin{align} F &amp;= \\frac{\\frac{\\sum (\\bar{x_i} - \\bar{\\bar{x}})^2}{2-1}}{\\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{\\frac{(\\bar{x_1} - \\bar{\\bar{x}})^2 + (\\bar{x_2} - \\bar{\\bar{x}})^2}{1}}{\\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{(\\bar{x_1} - \\bar{\\bar{x}})^2 + (\\bar{x_2} - \\bar{\\bar{x}})^2}{\\frac{s_{people}^2}{n}}; \\text{With only two means they are equidistant from the grand mean}\\\\ &amp;= \\frac{\\frac{(\\bar{x_1} - \\bar{x_2})^2}{2}}{\\frac{s_{people}^2}{n}}; \\text{So we can change the numerator because:} (\\bar{x_1} - \\bar{\\bar{x}})^2 + (\\bar{x_2} - \\bar{\\bar{x}})^2 = \\frac{(\\bar{x_1} - \\bar{x_2})^2}{2}\\\\ &amp;= \\frac{\\frac{(\\bar{x_1} - \\bar{x_2})^2}{2}}{\\frac{s_{people}^2}{n}}\\times 1\\\\ &amp;= \\frac{\\frac{(\\bar{x_1} - \\bar{x_2})^2}{2}}{\\frac{s_{people}^2}{n}}\\times \\frac{2}{2}\\\\ &amp;= \\frac{(\\bar{x_1} - \\bar{x_2})^2}{2 \\times \\frac{s_{people}^2}{n}}\\\\ &amp;= \\frac{(\\bar{x_1} - \\bar{x_2})^2}{ \\frac{s_{people}^2}{n} + \\frac{s_{people}^2}{n}}\\\\ &amp;= t^2 \\end{align} \\] 24.2.3 contrast (multiple means) Sometimes we might want to combine cells for a comparison. For example, imagine we hypothesized the mean of cells 2 and 3 was higher than the mean of cell 1. Illustrated below: \\[ \\frac{\\bar{x_2} + \\bar{x_3}}{2} \\text{ vs }\\bar{x_1} \\] How do we go about conducting this comparison? 24.2.3.1 contrast (mult) statistic The two contrast means (\\(\\bar{c_1}\\), \\(\\bar{c_2}\\)}) are calculated as below” For the first mean, it’s just a cell mean, where \\(n = 5\\). \\[ \\bar{c_1} = \\bar{x_1} = 71.4 \\] For the second mean, it’s the average of two means. Note for this combined mean \\(n = 10\\); because it was based on two cell means, each \\(n=5\\). \\[ \\bar{c_2} = \\frac{\\bar{x_2} + \\bar{x_3}}{2} = \\frac{ 144}{2} = 80.3 \\] But when we look at these two contrast means, the estimate of the “grand mean” is: \\[ \\begin{aligned} \\bar{\\bar{c}} &amp;= \\frac{\\sum \\bar{c_i}}{2} \\\\ &amp;= \\frac{\\bar{c_1}+\\bar{c_2}}{2}\\\\ &amp;= \\frac{71.4 + 80.3}{2} \\\\ &amp;= 75.85\\\\ \\end{aligned} \\] We estimate the total variance: \\[ \\begin{aligned} \\text{total variance} &amp;= \\frac{\\sum (\\bar{c_{i}}-\\bar{\\bar{c}})^2}{2-1} \\\\ &amp;= \\frac{(\\bar{c_{1}} - \\bar{\\bar{c}})^2 + (\\bar{c_{2}} - \\bar{\\bar{c}})^2}{2-1} \\\\ &amp;= \\frac{(71.4- 75.85)^2 + (80.3 - 75.85)^2}{2-1} \\\\ &amp;= 39.605 \\end{aligned} \\] We estimate the variance due to random sampling: The calculation is bit more complicated for sampling error because the sample sizes are different for the two means. For first contrast mean, \\(c_1\\), is \\(n = 5\\). Therefore sampling error is: \\[ \\frac{s_{people}^2}{n} = \\frac{MSE}{n} = \\frac{60}{5} = 12 \\] For second contrast mean, \\(c_2\\), is \\(n = 10\\). Therefore sampling error is: \\[ \\frac{s_{people}^2}{n + n} = \\frac{MSE}{n + n} = \\frac{60}{5 + 5} = \\frac{60}{10} = 6 \\] We use the average of these two sampling error calculations \\[ \\text{random sampling variance} = \\frac{\\frac{s_{people}^2}{5} + \\frac{s_{people}^2}{10}}{2} = 9 \\] The \\(F\\)-value is: \\[ \\begin{align} F &amp;= \\frac{\\text{total variance}}{\\text{random sampling variance}}\\\\ &amp;= \\frac{\\frac{\\sum (\\bar{c_{i}}-\\bar{\\bar{c}})^2}{2-1}}{\\left[ \\frac{\\frac{s_{people}^2}{5} + \\frac{s_{people}^2}{10}}{2} \\right]}\\\\ &amp;= \\frac{\\frac{(71.4- 75.85)^2 + (80.3 - 75.85)^2}{2-1}}{\\left[ \\frac{12 + 6}{2} \\right]}\\\\ &amp;= \\frac{39.605}{\\left[ 9 \\right]}\\\\ &amp;= 4.40 \\end{align} \\] In R, it’s easy to get these numbers. We begin by getting the cell data: juggling_high_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;high&quot;)%&gt;% pull(calm) juggling_medium_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;medium&quot;)%&gt;% pull(calm) juggling_low_preparation &lt;- nway_data %&gt;% filter(juggling == &quot;yes&quot;)%&gt;% filter(preparation == &quot;low&quot;)%&gt;% pull(calm) Then calculate the cell means: mean_cell1 &lt;- mean(juggling_low_preparation, na.rm = TRUE) mean_cell2 &lt;- mean(juggling_medium_preparation, na.rm = TRUE) mean_cell3 &lt;- mean(juggling_high_preparation, na.rm = TRUE) # estimate total variance mean_c1 &lt;- mean_cell1 mean_c2 &lt;- mean( c(mean_cell2, mean_cell3) ) contrast_means = c(mean_c1, mean_c2) total_var = var(contrast_means) # estimate variance due to sampling error n_per_cell = 5 n = n_per_cell sampling_var_c1 &lt;- mse/n sampling_var_c2 &lt;- mse/(n + n) sampling_var = mean( c(sampling_var_c1, sampling_var_c2) ) # compare the estimates with a ratio Fvalue = total_var / sampling_var print(Fvalue) ## [1] 4.401 24.2.3.2 contrast (mult) p-value df1 = 1 # two means being compared so: 2 - 1 df df2 = 24 #recall 24 df for MSE: ab(n-1) pvalue_nondirectional = pf(Fvalue, df1 = df1, df2 = df2, lower.tail = FALSE) pvalue_directional = pvalue_nondirectional/2 print(pvalue_directional) ## [1] 0.02332 Among jugglers, the combined mean of calmness for high and medium preparation students (\\(M = 80.30\\)) was higher than the mean (\\(M_{\\text{diff}} = 8.90\\)) for low preparation students,(\\(M = 71.40\\)), \\(F(1,24) = 4.40, p = .023\\). But we would still like to put an effect size in this sentence. 24.2.3.3 contrast (mult) effect size Now the effect size. To calculate the effect size we need to use a rather odd notation. But this odd notation is commonly used with contrasts. We assign weights to the contrasts using a few rules. All of the cell means involved in one contrast mean (e.g., \\(\\bar{c_1}\\)) receive a negative number for the weight. All of the cell means involved in the other contrast mean (e.g., \\(\\bar{c_2}\\)) receive a positive number for the weight. All of the negative weights must add up to negative one (i.e., -1.0) All of the positive weights must add up to positive one (i.e., 1.0) So if we think of the cell means in order: \\[ \\bar{x_1}, \\bar{x_2}, \\bar{x_3} \\] If we want \\(x_1\\) compared to the average of cells two (\\(x_2\\) and \\(x_3\\)) then we would use the weights \\[ -1, 0.5, 0.5 \\] This results in the following comparison: \\[ \\frac{\\bar{x_2} + \\bar{x_3}}{2} - \\bar{x_1} \\] n_per_cell = 5 mse_n = 2*3*n_per_cell means_to_compare = c(mean_cell1, mean_cell2, mean_cell3) n_for_means = c(n_per_cell, n_per_cell, n_per_cell) comparison_weights = c(-1, .5, .5) # re-use lm_nway from initial ANOVA s_people = summary(lm_nway)$sigma # Calculate standardizec contrast with CI ci.sc(means = means_to_compare, c.weights = comparison_weights, n = n_for_means, s.anova = s_people, N = mse_n) ## $Lower.Conf.Limit.Standardized.Contrast ## [1] 0.02316 ## ## $Standardized.contrast ## [1] 1.149 ## ## $Upper.Conf.Limit.Standardized.Contrast ## [1] 2.255 24.2.3.4 contrast (mult) text Among jugglers, the combined mean of calmness for high and medium preparation students (\\(M = 80.30\\)) was higher than the mean (\\(M_{\\text{diff}} = 8.90\\), \\(sc = 1.15\\), 95% \\(CI[.02, 2.26]\\)) for low preparation students,(\\(M = 71.40\\)), \\(F(1,24) = 4.40, p = .023\\). 24.2.3.5 contrasts in practice The calculations presented here are designed to present the conceptual basis for contrasts. In particular, I aimed to emphasize how the logic used is the same as that used in ANOVA. I strongly encourage you to check out the excellent computational treatment of this topic provided in the book below: Maxwell, S. E., Delaney, H. D., &amp; Kelley, K. (2017). Designing experiments and analyzing data: A model comparison perspective. Routledge. Chicago "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
